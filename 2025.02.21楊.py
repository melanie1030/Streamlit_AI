import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
import base64
from io import BytesIO
from openai import OpenAI
from PIL import Image
import google.generativeai as genai  # æ–°å¢Geminiä¾èµ–
from streamlit_ace import st_ace
import time

# --- åˆå§‹åŒ–è®¾ç½® ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
LLM_MODELS = [  # ä¿®æ”¹åçš„æ¨¡å‹åˆ—è¡¨
    "gpt-4-turbo",
    "gpt-3.5-turbo-16k",
    "gemini-1.5-flash",
    "gemini-1.5-pro",
    "models/gemini-2.0-flash"
]

MAX_MESSAGES = 10  # Limit message history

def initialize_client(api_key):
    return OpenAI(api_key=api_key) if api_key else None

def debug_log(msg):
    if st.session_state.get("debug_mode", False):
        st.session_state.debug_logs.append(f"**DEBUG LOG:** {msg}")
        st.write(msg)
        print(msg)

def debug_error(msg):
    if st.session_state.get("debug_mode", False):
        st.session_state.debug_errors.append(f"**DEBUG ERROR:** {msg}")
        print(msg)

def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)

    debug_log(f"Saving file to {file_path}")
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    debug_log(f"Files in {UPLOAD_DIR}: {os.listdir(UPLOAD_DIR)}")
    return file_path

def load_image_base64(image):
    """Convert image to Base64 encoding."""
    try:
        buffer = BytesIO()
        image.save(buffer, format="PNG")  # Use PNG for consistency
        return base64.b64encode(buffer.getvalue()).decode('utf-8')
    except Exception as e:
        debug_error(f"Error converting image to base64: {e}")
        return ""

def append_message(role, content):
    """Append a message and ensure the total number of messages does not exceed MAX_MESSAGES."""
    st.session_state.messages.append({"role": role, "content": content})
    if len(st.session_state.messages) > MAX_MESSAGES:
        # Remove the oldest messages except the system prompt
        st.session_state.messages = [st.session_state.messages[0]] + st.session_state.messages[-(MAX_MESSAGES - 1):]
        debug_log("Message history trimmed to maintain token limits.")

def add_user_image(uploaded_file):
    """æ·»åŠ ç”¨æˆ¶åœ–ç‰‡æ¶ˆæ¯åˆ°session state"""
    try:
        # # 1. æª¢æŸ¥æ˜¯å¦å·²ç¶“è™•ç†éç›¸åŒåç¨±çš„æª”æ¡ˆ
        # #    å¦‚æœ "last_uploaded_filename" å·²ç¶“æ˜¯é€™å€‹æª”åï¼Œå°±è·³éã€‚
        # if st.session_state.get("last_uploaded_filename") == uploaded_file.name:
        #     debug_log("å·²ä¸Šå‚³éç›¸åŒæª”æ¡ˆï¼Œè·³éè™•ç†ã€‚")
        #     return
        
        # è‹¥æ˜¯ç¬¬ä¸€æ¬¡çœ‹åˆ°é€™å€‹æª”åï¼Œå°±è¨˜éŒ„ä¸‹ä¾†
        st.session_state["last_uploaded_filename"] = uploaded_file.name

        # 2. ç²å–ç•¶å‰é¸æ“‡çš„æ¨¡å‹
        current_model = st.session_state.get("selected_model", "").lower()
        
        # 3. åˆ¤æ–·æ˜¯å¦éœ€è¦Base64ç·¨ç¢¼ï¼ˆç•¶æ¨¡å‹åç¨±åŒ…å«gptæ™‚å•Ÿç”¨ï¼‰
        use_base64 = "gpt" in current_model  
        
        # 4. ä¿å­˜ä¸Šå‚³æ–‡ä»¶ä¸¦ç²å–è·¯å¾‘
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.uploaded_image_path = file_path
        
        # 5. æ ¹æ“šæ¨¡å‹é¡å‹æ§‹å»ºåœ–ç‰‡URL
        if use_base64:
            # ç‚ºOpenAIæ¨¡å‹ç”ŸæˆBase64 URL
            image_base64 = load_image_base64(file_path)
            image_url = f"data:image/{file_path.split('.')[-1]};base64,{image_base64}"
        else:
            # ç‚ºGeminiä½¿ç”¨ç›´æ¥æ–‡ä»¶è·¯å¾‘
            image_url = file_path
        
        # 6. æ§‹å»ºæ¶ˆæ¯çµæ§‹
        image_msg = {
            "type": "image_url",
            "image_url": {
                "url": image_url,
                "detail": "auto"
            }
        }
        
        # 7. æ·»åŠ æ¶ˆæ¯åˆ°æ­·å²è¨˜éŒ„
        if use_base64:
            append_message("user", [image_msg])
        else:
            return image_url
        debug_log(f"åœ–ç‰‡æ¶ˆæ¯å·²æ·»åŠ ï¼š{image_url[:50]}...")
        
        # è‹¥éœ€è¦Base64å°±å­˜åˆ° session_stateï¼Œå¦å‰‡å°±å­˜ None
        st.session_state.image_base64 = image_base64 if use_base64 else None
        
        # 8. åˆ·æ–°ç•Œé¢ï¼ˆoptionalï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
        st.rerun()
        
    except Exception as e:
        st.write(f"æ·»åŠ åœ–ç‰‡æ¶ˆæ¯å¤±æ•—ï¼š{str(e)}")
        st.error("åœ–ç‰‡è™•ç†ç•°å¸¸ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")

def reset_session_messages():
    """Clear conversation history from the session."""
    if "messages" in st.session_state:
        st.session_state.pop("messages")
        st.success("Memory cleared!")
        debug_log("Conversation history cleared.")

def execute_code(code, global_vars=None):
    try:
        exec_globals = global_vars if global_vars else {}
        debug_log("Ready to execute the following code:")
        if st.session_state.get("debug_mode", False):
            st.session_state.debug_logs.append(f"```python\n{code}\n```")

        debug_log(f"Executing code with global_vars: {list(exec_globals.keys())}")
        exec(code, exec_globals)
        output = exec_globals.get("output", "(No output returned)")
        debug_log(f"Execution output: {output}")
        return f"Code executed successfully. Output: {output}"
    except Exception as e:
        error_msg = f"Error executing code:\n{traceback.format_exc()}"
        debug_log(f"Execution error: {error_msg}")
        if st.session_state.get("debug_mode", False):
            return error_msg
        else:
            return "Error executing code (hidden in non-debug mode)."

def extract_json_block(response: str) -> str:
    pattern = r'```(?:json)?\s*(.*?)\s*```'
    match = re.search(pattern, response, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
        debug_log(f"Extracted JSON block: {json_str}")
        return json_str
    else:
        debug_log("No JSON block found in response.")
        return response.strip()
        
# =======================================================================================================
#       ä»¥ä¸‹ç‚ºèˆŠç‰ˆæœ¬çš„ get_llm_response å‡½æ•¸
# def get_llm_response(client, model_params, max_retries=3):
#     """Get response from the LLM model synchronously with retry logic."""
#     retries = 0
#     wait_time = 5  # Start with 5 seconds

#     while retries < max_retries:
#         try:
#             response = client.chat.completions.create(
#                 model=model_params.get("model", "gpt-4-turbo"),
#                 messages=st.session_state.messages,
#                 temperature=model_params.get("temperature", 0.3),
#                 max_tokens=model_params.get("max_tokens", 4096),
#                 stream=False  # Disable streaming
#             )
#             # Extract the full response content
#             response_content = response.choices[0].message.content.strip()
#             debug_log(f"Full assistant response: {response_content}")
#             return response_content

#         except Exception as e:
#             if 'rate_limit_exceeded' in str(e).lower() or '429' in str(e):
#                 debug_error(f"Rate limit exceeded. Retrying in {wait_time} seconds...")
#                 st.warning(f"Rate limit exceeded. Retrying in {wait_time} seconds...")
#                 time.sleep(wait_time)
#                 retries += 1
#                 wait_time *= 2  # Exponential backoff
#             else:
#                 debug_error(f"Error getting response: {e}")
#                 st.error(f"An error occurred while getting the response: {e}")
#                 return ""

#     st.error("Max retries exceeded. Please try again later.")
#     return ""
# =======================================================================================================


# ä»¥ä¸‹ç‚ºæ–°ç‰ˆæœ¬çš„ get_llm_response å‡½æ•¸
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
def get_gemini_response(model_params, max_retries=3):
    """
    æ•´åˆæ–°ç‰ˆ Gemini è«‹æ±‚æ–¹æ³•ï¼Œæ”¯æ´å…ˆè®€å–åœ–ç‰‡ (generate_content) å†é€²è¡Œå®Œæ•´å°è©± (send_message)ã€‚
    æµç¨‹ï¼š
      1) åµæ¸¬æ˜¯å¦æœ‰åœ–ç‰‡ (æœ€å¾Œä¸€å‰‡ user è¨Šæ¯)
      2) å¦‚æœæœ‰ï¼Œå…ˆç”¨ generate_content() å–å¾—ä¸€æ®µå›è¦†ä¸¦åŠ åˆ° messages
      3) æœ€å¾Œç”¨ send_message() æŠŠæ•´å€‹ messages ç™¼é€çµ¦ Geminiï¼Œå–å¾—æœ€çµ‚å›è¦†
    """

    # --- 1) æª¢æŸ¥é‡‘é‘° ---
    api_key = st.session_state.get("gemini_api_key_input", "")
    debug_log(f"gemini api key: {api_key}")
    if not api_key:
        st.error("æœªè¨­å®š Gemini API é‡‘é‘°")
        return ""

    genai.configure(api_key=api_key)
    model_name = model_params.get("model", "gemini-1.5-flash")
    debug_log(f"gemini model: {model_name}")

    # --- 2) åˆå§‹åŒ– Chat ç‰©ä»¶ ---
    model = genai.GenerativeModel(model_name)
    st.session_state.gemini_chat = model.start_chat(history=[])
    debug_log("Gemini chat session created.")

    # --- 3) æª¢æŸ¥æ˜¯å¦å­˜åœ¨ã€Œæœ€å¾Œä¸€å‰‡ã€åŒ…å«åœ–ç‰‡çš„ user è¨Šæ¯ ---
    if st.session_state.uploaded_image_path:
        debug_log("Detected user message with image, using generate_content() first...")
        # æ”¶é›†æ–‡æœ¬ & åœ–ç‰‡è³‡è¨Š
        gen_content = True
    else:
        gen_content = False
    last_user_msg_with_image = None
    for msg in reversed(st.session_state.messages):
        if msg["role"] == "user" and isinstance(msg["content"], list):
            # content ä»¥ list è¡¨ç¤ºå¤šæ¨¡æ…‹ï¼Œè£¡é¢å¯èƒ½å«æœ‰ type="image_url"
            for item in msg["content"]:
                if isinstance(item, dict) and item.get("type") == "image_url":
                    last_user_msg_with_image = msg
                    break
        if last_user_msg_with_image:
            break

    # å¦‚æœæ‰¾åˆ°ã€Œæœ€å¾Œä¸€å‰‡åŒ…å«åœ–ç‰‡ã€çš„è¨Šæ¯ï¼Œå…ˆé€é generate_content() å–®ç¨åšä¸€æ¬¡å›è¦†
    if gen_content:
        debug_log("Detected user message with image, using generate_content() first... by gen_content")

        # æ”¶é›†æ–‡æœ¬ & åœ–ç‰‡è³‡è¨Š
        text_parts = []
        image_data = st.session_state.uploaded_image_path

        # åŸ·è¡Œ generate_content
        # æ³¨æ„ï¼šgenerate_content() é è¨­åªèƒ½è™•ç†å–®ä¸€æ®µè½çš„ roleï¼Œä¸æ”¯æ´ä¸€æ¬¡æ”¾æ•´å€‹å°è©±ã€‚
        #      é€™é‚Šåªè®“å®ƒè®€å–ã€Œæœ€å¾Œä¸€å‰‡ user çš„åœ–ç‰‡ + æ–‡å­—ã€ä»¥å–å¾— AI çš„åˆæ­¥å›è¦†ã€‚
        debug_log("Using generate_content() first... by gen_content")
        try:
            debug_log("entering generate_content()")
            retries = 0
            while retries < max_retries:
                try:
                    debug_log("entering generate_content() try block")
                    # èª¿ç”¨ generate_content()ï¼Œå¸¶å…¥æ–‡å­—èˆ‡åœ–ç‰‡
                    imageee = genai.upload_file(path=image_data, display_name="Image")
                    debug_log(f"imageee: {imageee}")
                    response_gc = model.generate_content(["è«‹ä½ ç¹é«”ä¸­æ–‡è§£è®€åœ–ç‰‡",imageee])  # å–®å¼µåœ–
                    debug_log(f"response_gc: {response_gc.text}")
                    
                    # æ‹¿åˆ°å›è¦†ä¹‹å¾Œï¼Œå…ˆå°‡å…¶æ–°å¢è‡³å°è©±
                    generate_content_reply = response_gc.text
                    debug_log(f"Gemini generate_content reply: {generate_content_reply}")
                    append_message("assistant", generate_content_reply)
                    break
                except genai.GenerationError as e:
                    debug_error(f"generate_content() å¤±æ•—: {e}")
                    retries += 1
                    time.sleep(5 * retries)
                except Exception as e:
                    debug_error(f"generate_content() å…¶ä»–éŒ¯èª¤: {e}")
                    return "generate_content Error"
        except Exception as e:
            debug_error(f"generate_content() å…¶ä»–éŒ¯èª¤: {e}")

    # --- 4) æœ€å¾Œï¼šå°‡æœ€çµ‚æ•´ä¸²å°è©±(å«å‰›å‰› generate_content çš„å›è¦†) ç”¨ send_message() æ‹¿åˆ°æœ€çµ‚å›ç­” ---

    # è½‰æ›å°è©±æ ¼å¼
    converted_history = []
    for msg in st.session_state.messages:
        role = msg.get("role")
        parts = []

        # è‹¥ content æ˜¯ listï¼ˆå¤šæ¨¡æ…‹ï¼‰ï¼Œé€é …åˆ¤æ–·æ˜¯åœ–ç‰‡é‚„æ˜¯ç´”æ–‡å­—
        if isinstance(msg["content"], list):
            for item in msg["content"]:
                if isinstance(item, dict) and item.get("type") == "image_url":
                    # send_message() ç†è«–ä¸Šä¸æ”¯æ´ç›´æ¥å¸¶åœ–ç‰‡ï¼Œæ‰€ä»¥æˆ‘å€‘æœƒæŠŠå®ƒè½‰æˆæ–‡å­—æè¿°/æç¤º
                    parts.append(f"[Image included, base64 size={len(item['image_url']['url'])} chars]")
                    
                else:
                    # ç´”æ–‡å­—
                    parts.append(str(item))
        else:
            # ç›´æ¥è½‰æˆæ–‡å­—
            parts.append(str(msg["content"]))

        converted_history.append({"role": role, "parts": parts})

    converted_history_json = json.dumps(converted_history, ensure_ascii=False)
    debug_log(f"converted history (json) => {converted_history_json}")

    # --- 5) send_message æ‹¿æœ€çµ‚å›è¦† ---
    retries = 0
    wait_time = 5
    while retries < max_retries:
        try:
            debug_log("Calling send_message with entire conversation...")
            response_sm = st.session_state.gemini_chat.send_message(converted_history_json)
            final_reply = response_sm.text.strip()
            debug_log(f"Gemini send_message final reply => {final_reply}")
            return final_reply
        except genai.GenerationError as e:
            debug_error(f"send_message() ç”ŸæˆéŒ¯èª¤: {e}")
            st.warning(f"Gemini ç”ŸæˆéŒ¯èª¤ï¼Œ{wait_time}ç§’å¾Œé‡è©¦...")
            time.sleep(wait_time)
            retries += 1
            wait_time *= 2
        except Exception as e:
            debug_error(f"send_message() APIè«‹æ±‚ç•°å¸¸: {e}")
            st.error(f"Gemini APIè«‹æ±‚ç•°å¸¸: {e}")
            return ""

    return "è«‹æ±‚å¤±æ•—æ¬¡æ•¸éå¤šï¼Œè«‹ç¨å¾Œé‡è©¦"


def get_openai_response(client, model_params, max_retries=3):
    """å¤„ç†OpenAI APIè¯·æ±‚"""
    retries = 0
    wait_time = 5  # åˆå§‹ç­‰å¾…æ—¶é—´5ç§’
    model_name = model_params.get("model", "gpt-4-turbo")
    
    while retries < max_retries:
        try:
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": model_name,
                "messages": st.session_state.messages,
                "temperature": model_params.get("temperature", 0.3),
                "max_tokens": model_params.get("max_tokens", 4096),
                "stream": False
            }
            
            # æ·»åŠ å›¾åƒå¤„ç†é€»è¾‘ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if any(msg.get("content") and isinstance(msg["content"], list) for msg in st.session_state.messages):
                request_params["max_tokens"] = 4096  # å¢åŠ tokené™åˆ¶
                debug_log("Detected multimodal input, adjusting max_tokens")
            
            # å‘é€è¯·æ±‚
            response = client.chat.completions.create(**request_params)
            
            # æå–å¹¶æ¸…ç†å“åº”å†…å®¹
            response_content = response.choices[0].message.content.strip()
            debug_log(f"OpenAIåŸå§‹å“åº”ï¼š\n{response_content}")
            return response_content
            
        except Exception as e:
            # å¤„ç†é€Ÿç‡é™åˆ¶é”™è¯¯
            if 'rate limit' in str(e).lower() or '429' in str(e):
                debug_error(f"é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆå°è¯• {retries+1}/{max_retries}ï¼‰ï¼š{e}")
                st.warning(f"è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œ{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
                retries += 1
                wait_time *= 2  # æŒ‡æ•°é€€é¿
                
            # å¤„ç†è®¤è¯é”™è¯¯
            elif 'invalid api key' in str(e).lower():
                debug_error(f"APIå¯†é’¥æ— æ•ˆï¼š{e}")
                st.error("OpenAI APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥åé‡è¯•")
                return ""
                
            # å…¶ä»–é”™è¯¯å¤„ç†
            else:
                debug_error(f"OpenAIè¯·æ±‚å¼‚å¸¸ï¼š{str(e)}")
                st.error(f"è¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
                return ""
    
    # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
    debug_error(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ{max_retries}æ¬¡ï¼‰")
    st.error("è¯·æ±‚å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œè¯·ç¨åå†è¯•")
    return ""

def get_llm_response(client, model_params, max_retries=3):
    """ç²å–LLMæ¨¡å‹å›è¦†ï¼ˆæ”¯æŒOpenAIå’ŒGeminiï¼‰"""
    model_name = model_params.get("model", "gpt-4-turbo")
    debug_log(f"starting to get llm response...{model_name}")
    
    if "gpt" in model_name:
        debug_log("GPT")
        return get_openai_response(client, model_params, max_retries)
    elif "gemini" in model_name:
        debug_log("Gemini")
        return get_gemini_response(model_params=model_params, max_retries=max_retries)
    else:
        st.error(f"ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹: {model_name}")
        return ""
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

def main():
    st.set_page_config(page_title="Chatbot + Data Analysis", page_icon="ğŸ¤–", layout="wide")
    st.title("ğŸ¤– Chatbot + ğŸ“Š Data Analysis + ğŸ§  Memory + ğŸ–‹ï¸ Canvas (With Debug & Deep Analysis)")

    # Initialize session state variables
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "ace_code" not in st.session_state:
        st.session_state.ace_code = ""
    if "editor_location" not in st.session_state:
        st.session_state.editor_location = "Main"
    if "uploaded_file_path" not in st.session_state:
        st.session_state.uploaded_file_path = None
    if "uploaded_image_path" not in st.session_state:
        st.session_state.uploaded_image_path = None
    if "image_base64" not in st.session_state:
        st.session_state.image_base64 = None
    if "debug_mode" not in st.session_state:
        st.session_state.debug_mode = False
    if "deep_analysis_mode" not in st.session_state:
        st.session_state.deep_analysis_mode = False
    if "second_response" not in st.session_state:
        st.session_state.second_response = ""
    if "third_response" not in st.session_state:
        st.session_state.third_response = ""
    if "deep_analysis_image" not in st.session_state:
        st.session_state.deep_analysis_image = None
    if "debug_logs" not in st.session_state:
        st.session_state.debug_logs = []
    if "debug_errors" not in st.session_state:
        st.session_state.debug_errors = []
    if "thinking_protocol" not in st.session_state:
        st.session_state.thinking_protocol = None  # Initialize thinking_protocol
    if "gemini_ai_chat" not in st.session_state:
        st.session_state.gemini_ai_chat = None  # Initialize gemini_ai_chat
    if "gemini_ai_history" not in st.session_state: 
        st.session_state.gemini_ai_history = []  # Initialize gemini_ai_history

    with st.sidebar:
        st.subheader("ğŸ”‘ API Key Settings")
        # åŸæœ‰çš„OpenAIé‡‘é‘°è¨­å®š
        default_openai_key = os.getenv("OPENAI_API_KEY", "")
        openai_api_key = st.text_input("OpenAI API Key", value=default_openai_key, type="password")
        
        # æ–°å¢Geminié‡‘é‘°è¼¸å…¥
        default_gemini_key = os.getenv("GEMINI_API_KEY", "")
        gemini_api_key = st.text_input("Gemini API Key", 
                                     value=default_gemini_key, 
                                     type="password",
                                     key="gemini_api_key")
        
        # æ›´æ–°ç’°å¢ƒè®Šæ•¸
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        if gemini_api_key:
            st.session_state["gemini_api_key_input"] = gemini_api_key 

        selected_model = st.selectbox(
            "é¸æ“‡æ¨¡å‹", 
            LLM_MODELS, 
            index=0, 
            key="selected_model"  # æ–°å¢keyç”¨æ–¼session stateç¶å®š
        )
        
        # æ­¥é©Ÿ3ï¼šAPIé‡‘é‘°ç‹€æ…‹æª¢æŸ¥ (æ”¾åœ¨æ¨¡å‹é¸æ“‡ä¹‹å¾Œ)
        if "selected_model" in st.session_state:
            current_model = st.session_state.selected_model.lower()
            
            if "gemini" in current_model:
                # æª¢æŸ¥Geminié‡‘é‘° (ç’°å¢ƒè®Šæ•¸æˆ–æ‰‹å‹•è¼¸å…¥)
                gemini_key = os.getenv("GEMINI_API_KEY") or st.session_state.get("gemini_api_key")
                if not gemini_key:
                    st.error("ä½¿ç”¨Geminiæ¨¡å‹éœ€åœ¨ä¸‹æ–¹è¼¸å…¥APIé‡‘é‘° ğŸ”‘")
                    st.stop()  # é˜»æ­¢å¾ŒçºŒä»£ç¢¼åŸ·è¡Œ
                    
            elif "gpt" in current_model:
                # æª¢æŸ¥OpenAIé‡‘é‘°
                openai_key = os.getenv("OPENAI_API_KEY") or st.session_state.get("openai_api_key")
                if not openai_key:
                    st.error("ä½¿ç”¨OpenAIæ¨¡å‹éœ€åœ¨ä¸‹æ–¹è¼¸å…¥APIé‡‘é‘° ğŸ”‘")
                    st.stop()

        st.session_state.debug_mode = st.checkbox("Debug Mode", value=False)
        st.session_state.deep_analysis_mode = st.checkbox("Deep Analysis Mode", value=False)

        if "memory" not in st.session_state:
            st.session_state.memory = []

        if "conversation_initialized" not in st.session_state:
            openai_api_key = os.getenv("OPENAI_API_KEY") or st.session_state.get("openai_api_key_input")
            if openai_api_key or gemini_api_key:
                client = initialize_client(openai_api_key)
                st.session_state.conversation_initialized = True
                st.session_state.messages = []
                debug_log("Conversation initialized with OpenAI client.")
            else:
                st.warning("â¬…ï¸ è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥OpenAI APIé‡‘é‘°ä»¥åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äºº")

        if st.session_state.debug_mode:
            debug_log(f"Currently using model => {selected_model}")

        if st.button("ğŸ—‘ï¸ Clear Memory"):
            st.session_state.memory = []
            st.session_state.messages = []
            st.session_state.ace_code = ""
            st.session_state.uploaded_file_path = None
            st.session_state.uploaded_image_path = None
            st.session_state.image_base64 = None
            st.session_state.deep_analysis_mode = False
            st.session_state.second_response = ""
            st.session_state.third_response = ""
            st.session_state.deep_analysis_image = None
            st.session_state.debug_logs = []
            st.session_state.debug_errors = []
            st.session_state.thinking_protocol = None  # Clear thinking_protocol
            st.success("Memory cleared!")
            debug_log("Memory has been cleared.")

        st.subheader("ğŸ§  Memory State")
        if st.session_state.messages:
            memory_content = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.messages])
            st.text_area("Current Memory", value=memory_content, height=200)
            debug_log(f"Current memory content: {memory_content}")
        else:
            st.text_area("Current Memory", value="No messages yet.", height=200)
            debug_log("No messages in memory.")

        # --- CSV Upload ---
        st.subheader("ğŸ“‚ Upload a CSV File")
        uploaded_file = st.file_uploader("Choose a CSV file:", type=["csv"])
        csv_data = None
        if uploaded_file:
            st.session_state.uploaded_file_path = save_uploaded_file(uploaded_file)
            debug_log(f"Uploaded file path: {st.session_state.uploaded_file_path}")
            try:
                csv_data = pd.read_csv(st.session_state.uploaded_file_path)
                st.write("### Data Preview")
                st.dataframe(csv_data)
                debug_log(f"CSV Data Columns: {list(csv_data.columns)}")
            except Exception as e:
                if st.session_state.debug_mode:
                    st.error(f"Error reading CSV: {e}")
                debug_log(f"Error reading CSV: {e}")

        # --- Image Upload ---
        st.subheader("ğŸ–¼ï¸ Upload an Image")
        uploaded_image = st.file_uploader("Choose an image:", type=["png", "jpg", "jpeg"], key="image_uploader")
        if uploaded_image:
            st.session_state.uploaded_image= add_user_image(uploaded_image)
            debug_log(f"Uploaded image path: {st.session_state.uploaded_image}")

        # --- Thinking Protocol Upload ---
        st.subheader("ğŸ§  Upload Thinking Protocol")
        uploaded_thinking_protocol = st.file_uploader("Choose a thinking_protocol.md file:", type=["md"], key="thinking_protocol_uploader")
        if uploaded_thinking_protocol:
            try:
                thinking_protocol_content = uploaded_thinking_protocol.read().decode("utf-8")
                st.session_state.thinking_protocol = thinking_protocol_content
                append_message("user", thinking_protocol_content)  # æ·»åŠ ä¸ºç”¨æˆ·æ¶ˆæ¯
                st.success("Thinking Protocol uploaded successfully!")
                debug_log("Thinking Protocol uploaded and added to messages.")
            except Exception as e:
                if st.session_state.debug_mode:
                    st.error(f"Error reading Thinking Protocol: {e}")
                debug_log(f"Error reading Thinking Protocol: {e}")

        st.subheader("Editor Location")
        location = st.radio(
            "Choose where to display the editor:",
            ["Main", "Sidebar"],
            index=0 if st.session_state.editor_location == "Main" else 1
        )
        st.session_state.editor_location = location
        debug_log(f"Editor location set to: {st.session_state.editor_location}")

        # --- è°ƒè¯•åŒºå—ç§»åŠ¨åˆ°ä¾§è¾¹æ  ---
        with st.expander("ğŸ› ï¸ è°ƒè¯•ä¸ä¼šè¯ä¿¡æ¯", expanded=False):
            if st.session_state.debug_mode:
                st.subheader("è°ƒè¯•æ—¥å¿—")
                if st.session_state.debug_logs:
                    debug_logs_combined = "\n".join(st.session_state.debug_logs)
                    st.text_area("Debug Logs", value=debug_logs_combined, height=200)
                else:
                    st.write("æ²¡æœ‰è°ƒè¯•æ—¥å¿—ã€‚")

                st.subheader("è°ƒè¯•é”™è¯¯")
                if st.session_state.debug_errors:
                    debug_errors_combined = "\n".join(st.session_state.debug_errors)
                    st.text_area("Debug Errors", value=debug_errors_combined, height=200)
                else:
                    st.write("æ²¡æœ‰è°ƒè¯•é”™è¯¯ã€‚")

            st.subheader("ä¼šè¯ä¿¡æ¯ (messages.json)")
            if "messages" in st.session_state:
                messages_json = json.dumps(st.session_state.messages, ensure_ascii=False, indent=4)
                st.text_area("messages.json", value=messages_json, height=300)

                # æ·»åŠ ä¸‹è½½æŒ‰é’®
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ messages.json",
                    data=messages_json,
                    file_name="messages.json",
                    mime="application/json"
                )

                st.markdown("---")  # æ·»åŠ åˆ†éš”çº¿

                # æ–°å¢æŒ‰é’®ï¼šæ˜¾ç¤ºåŸå§‹æ¶ˆæ¯
                if st.button("ğŸ“„ æ˜¾ç¤ºåŸå§‹æ¶ˆæ¯"):
                    st.subheader("ğŸ” åŸå§‹æ¶ˆæ¯å†…å®¹")
                    st.json(st.session_state.messages)  # ä½¿ç”¨ st.json æ ¼å¼åŒ–æ˜¾ç¤º
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ° messagesã€‚")

    # --- Display Message History ---
    for idx, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            if isinstance(message["content"], list):
                # è™•ç†åˆ—è¡¨å½¢å¼çš„è¨Šæ¯å…§å®¹ï¼Œä¾‹å¦‚ image_url
                for item in message["content"]:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        image_url = item["image_url"]["url"]
                        st.image(image_url, caption="ğŸ“· ä¸Šå‚³çš„åœ–ç‰‡", use_column_width=True)
                        debug_log(f"Displaying image from {message['role']}: {image_url}")
                    else:
                        st.write(item)
                        debug_log(f"Displaying non-image content from {message['role']}: {item}")
            elif isinstance(message["content"], str) and "```python" in message["content"]:
                # è™•ç†åŒ…å« Python ä»£ç¢¼å¡Šçš„æ–‡å­—è¨Šæ¯
                code_match = re.search(r'```python\s*(.*?)\s*```', message["content"], re.DOTALL)
                if code_match:
                    code = code_match.group(1).strip()
                    st.code(code, language="python")
                    debug_log(f"Displaying code from {message['role']}: {code}")
                else:
                    st.write(message["content"])  # é¡¯ç¤ºä¸Šå‚³å°è©±
                    debug_log(f"Displaying message {idx} from {message['role']}: {message['content']}")
            else:
                # è™•ç†æ™®é€šçš„æ–‡å­—è¨Šæ¯
                st.write(message["content"])
                debug_log(f"Displaying message {idx} from {message['role']}: {message['content']}")

    # --- User Input ---
    user_input = st.chat_input("Hi! Ask me anything...")
    if user_input:
        append_message("user", user_input)
        with st.chat_message("user"):
            st.write(user_input)
            debug_log(f"User input added to messages: {user_input}")

        with st.spinner("Thinking..."):
            try:
                # # Initialize OpenAI client if not already done
                if openai_api_key or gemini_api_key:
                    client = initialize_client(openai_api_key)
                else:
                    raise ValueError("OpenAI API Key is not provided.")

                debug_log(f"Uploaded file path: {st.session_state.uploaded_file_path}")
                debug_log(f"Uploaded image path: {st.session_state.uploaded_image_path}")

                # --- Ensure system prompt is added only once ---
                if not any(msg["role"] == "system" for msg in st.session_state.messages):
                    system_prompt = "You are an assistant that helps with data analysis."
                    append_message("system", system_prompt)
                    debug_log("System prompt added to messages.")

                # --- Decide which prompt to use ---
                if st.session_state.uploaded_image_path is not None and st.session_state.image_base64:
                    # Image uploaded, image data already added as a separate message
                    prompt = user_input  # Use user input directly
                    debug_log("User input with image data already appended.")
                else:
                    # No image uploaded, use complex JSON logic
                    if st.session_state.uploaded_file_path is not None:
                        try:
                            df_temp = pd.read_csv(st.session_state.uploaded_file_path)
                            csv_columns = ", ".join(df_temp.columns)
                            debug_log(f"CSV columns: {csv_columns}")
                        except Exception as e:
                            csv_columns = "Unable to read columns"
                            if st.session_state.debug_mode:
                                st.error(f"Error reading columns: {e}")
                            debug_log(f"Error reading columns: {e}")
                    else:
                        csv_columns = "No file uploaded"
                        debug_log("No CSV file uploaded.")

                    if st.session_state.uploaded_file_path is not None and csv_columns != "No file uploaded":
                        prompt = f"""Please respond with a JSON object in the format:
{{
    "content": "é€™æ˜¯æˆ‘çš„è§€å¯Ÿè·Ÿåˆ†æ: {{analysis}}",
    "code": "import pandas as pd\\nimport streamlit as st\\nimport matplotlib.pyplot as plt\\n# Read CSV file (use st.session_state.uploaded_file_path variable)\\ndata = pd.read_csv(st.session_state.uploaded_file_path)\\n\\n# Add your plotting or analysis logic here\\n\\n# For example, to display a plot using st.pyplot():\\n# fig, ax = plt.subplots()\\n# ax.scatter(data['colA'], data['colB'])\\n# st.pyplot(fig)"
}}
Important:
1) å¿…é ˆä½¿ç”¨ st.session_state.uploaded_file_path ä½œç‚º CSV è·¯å¾‘ (instead of a hardcoded path)
2) Must use st.pyplot() to display any matplotlib figure
3) Return only valid JSON (escape any special characters if needed)

Based on the request: {user_input}.
Available columns: {csv_columns}.
ç„¶å¾Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰
"""
                        debug_log("Prompt constructed for CSV input with JSON response.")
                        append_message("system", prompt)
                        debug_log("System prompt appended to messages.")
                    else:
                        prompt = f"Please answer this question entirely in Traditional Chinese: {user_input}"
                        debug_log("Prompt constructed for plain text input.")
                        append_message("system", prompt)
                        debug_log("Plain text system prompt appended to messages.")

                # Make the API request and get the response
                model_params = {
                    "model": selected_model,
                    "temperature": 0.5,
                    "max_tokens": 4096
                }

                response_content = get_llm_response(client, model_params)
                debug_log(f"Full assistant response: {response_content}")

                if response_content:
                    # After getting the response, append assistant message
                    append_message("assistant", response_content)
                    with st.chat_message("assistant"):
                        st.write(response_content)  # é¿å…äºŒæ¬¡é¡¯ç¤º
                        debug_log(f"Assistant response added to messages: {response_content}")

                    # Extract JSON and code
                    json_str = extract_json_block(response_content)
                    try:
                        response_json = json.loads(json_str)
                        debug_log("JSON parsing successful.")
                    except Exception as e:
                        debug_log(f"json.loads parsing error: {e}")
                        debug_error(f"json.loads parsing error: {e}")
                        response_json = {"content": json_str, "code": ""}
                        debug_log("Fallback to raw response for content.")

                    content = response_json.get("content", "Here is my analysis:")
                    append_message("assistant", content)
                    # with st.chat_message("assistant"):
                    #     # st.write(content)    # é¿å…äºŒæ¬¡é¡¯ç¤º
                    #     debug_log(f"Content from JSON appended to messages: {content}")

                    code = response_json.get("code", "")
                    if code:
                        code_block = f"```python\n{code}\n```"
                        append_message("assistant", code_block)
                        with st.chat_message("assistant"):
                            st.code(code, language="python")
                        st.session_state.ace_code = code
                        debug_log("ace_code updated with new code.")

                    # --- If deep analysis mode is checked & code is present -> execute code and re-analyze chart ---
                    if st.session_state.deep_analysis_mode and code:
                        st.write("### [Deep Analysis] Automatically executing the generated code and sending the chart to GPT-4o for analysis...")
                        debug_log("Deep analysis mode activated.")

                        global_vars = {
                            "uploaded_file_path": st.session_state.uploaded_file_path,
                            "uploaded_image_path": st.session_state.uploaded_image_path,
                        }
                        exec_result = execute_code(st.session_state.ace_code, global_vars=global_vars)
                        st.write("#### Execution Result")
                        st.text(exec_result)
                        debug_log(f"Execution result: {exec_result}")

                        fig = plt.gcf()
                        buf = BytesIO()
                        fig.savefig(buf, format="png")
                        buf.seek(0)
                        chart_base64 = base64.b64encode(buf.read()).decode("utf-8")
                        st.session_state.deep_analysis_image = chart_base64
                        debug_log("Chart has been converted to base64.")

                        # Prepare deep analysis prompt
                        prompt_2 = f"""åŸºæ–¼åœ–ç‰‡çµ¦æˆ‘æ›´å¤šè³‡è¨Š"""
                        debug_log(f"Deep Analysis Prompt: {prompt_2}")

                        # Append prompt_2 to messages
                        append_message("user", prompt_2)
                        debug_log("Deep analysis prompt appended to messages.")

                        # æŠŠåœ–ç‰‡åŠ åˆ°äºŒæ¬¡åˆ†æè£¡
                        image_content = [{
                            "type": "image_url",
                            "image_url": {"url": f"data:image/png;base64,{chart_base64}"}
                        }]
                        append_message("user", image_content)  # æ·»åŠ åœ–ç‰‡åˆ°æ¶ˆæ¯

                        # Make the API request for deep analysis
                        second_raw_response = get_llm_response(client, model_params)
                        debug_log(f"Deep analysis response: {second_raw_response}")

                        if second_raw_response:
                            # Append assistant response
                            append_message("assistant", second_raw_response)
                            st.session_state.second_response = second_raw_response
                            with st.chat_message("assistant"):
                                st.write(second_raw_response)
                                debug_log(f"Deep analysis response added to messages: {second_raw_response}")

                            # Prepare final summary prompt
                            prompt_3 = f"""
First response content: {content}
Second response chart analysis content: {second_raw_response}

è«‹æŠŠå‰å…©æ¬¡çš„åˆ†æå…§å®¹åšåˆ†æç¸½çµï¼Œæœ‰æ•¸æ“šçš„è©±å°±é¡¯ç¤ºå¾—æ¼‚äº®ä¸€é»ï¼Œä¸»è¦æ˜¯éœ€è¦è®“ä½¿ç”¨è€…æ„Ÿåˆ°å¾ˆå²å®³ã€‚ä¸¦ä¸”ä»¥ç¹é«”ä¸­æ–‡ä½œç‚ºå›ç­”ç”¨çš„èªè¨€ã€‚
å¦å¤–éœ€è¦è§£é‡‹å‚³çµ¦å¦³çš„åœ–è¡¨ï¼Œä»¥ä¸€å€‹æ²’æœ‰è³‡æ–™ç§‘å­¸èƒŒæ™¯çš„å°ç™½è§£é‡‹æˆ‘æ‰€å‚³çš„åœ–è¡¨ã€‚é‚„æœ‰æ ¹æ“šç¬¬äºŒæ¬¡çš„åœ–è¡¨åˆ†æå¾—å‡ºçš„çµè«–ï¼Œç›´æ¥é æ¸¬ä¹‹å¾Œçš„èµ°å‘ï¼Œä¾‹å¦‚:"ä¹‹å¾Œé€™å€‹æ•¸å€¼çš„èµ°å‘æœƒå‘ˆç¾å‘ä¸Šçš„è¶¨å‹¢"ç­‰...
ä¸è¦è·Ÿä½¿ç”¨è€…èªªç”šéº¼å¦³å¯ä»¥ä½¿ç”¨RFMåˆ†æï¼Œäº¤å‰åˆ†æä¹‹é¡çš„æ–¹æ³•ã€‚æˆ‘éœ€è¦å¦³ç›´æ¥é æ¸¬ä¹‹å¾Œçš„èµ°å‘ï¼Œæ¯”å¦‚å¾€ä¸Šé‚„æ˜¯å¾€ä¸‹ã€‚
"""
                            debug_log(f"Final Summary Prompt: {prompt_3}")

                            # Append prompt_3 to messages
                            append_message("user", prompt_3)
                            debug_log("Final summary prompt appended to messages.")

                            # Make the API request for final summary
                            third_raw_response = get_llm_response(client, model_params)
                            debug_log(f"Final summary response: {third_raw_response}")

                            if third_raw_response:
                                # Append assistant response
                                append_message("assistant", third_raw_response)
                                st.session_state.third_response = third_raw_response
                                with st.chat_message("assistant"):
                                    st.write(third_raw_response)
                                    debug_log(f"Final summary response added to messages: {third_raw_response}")

                                # Display the chart
                                st.write("#### [Deep Analysis] Chart:")
                                try:
                                    img_data = base64.b64decode(st.session_state.deep_analysis_image)
                                    st.image(img_data, caption="Chart generated from deep analysis", use_column_width=True)
                                    debug_log("Deep analysis chart displayed.")
                                except Exception as e:
                                    if st.session_state.debug_mode:
                                        st.error(f"Error displaying chart: {e}")
                                    debug_log(f"Error displaying chart: {e}")

            except Exception as e:
                if st.session_state.debug_mode:
                    st.error(f"An error occurred: {e}")
                debug_log(f"An error occurred: {e}")

    # --- Persistent Code Editor ---
    if "ace_code" not in st.session_state:
        st.session_state.ace_code = ""

    if st.session_state.editor_location == "Main":
        with st.expander("ğŸ–‹ï¸ Persistent Code Editor (Main)", expanded=False):
            edited_code = st_ace(
                value=st.session_state.ace_code,
                language="python",
                theme="monokai",
                height=300,
                key="persistent_editor_main"
            )
            if edited_code != st.session_state.ace_code:
                st.session_state.ace_code = edited_code
                debug_log("ace_code updated from main editor.")

            if st.button("â–¶ï¸ Execute Code", key="execute_code_main"):
                global_vars = {
                    "uploaded_file_path": st.session_state.uploaded_file_path,
                    "uploaded_image_path": st.session_state.uploaded_image_path,
                }
                debug_log(f"Executing code with uploaded_file_path = {st.session_state.uploaded_file_path}")
                debug_log(f"Executing code with uploaded_image_path = {st.session_state.uploaded_image_path}")

                result = execute_code(st.session_state.ace_code, global_vars=global_vars)
                st.write("### Execution Result")
                st.text(result)
                debug_log(f"Code execution result: {result}")

    else:
        with st.sidebar.expander("ğŸ–‹ï¸ Persistent Code Editor (Sidebar)", expanded=False):
            edited_code = st_ace(
                value=st.session_state.ace_code,
                language="python",
                theme="monokai",
                height=300,
                key="persistent_editor_sidebar"
            )
            if edited_code != st.session_state.ace_code:
                st.session_state.ace_code = edited_code
                debug_log("ace_code updated from sidebar editor.")

            if st.button("â–¶ï¸ Execute Code", key="execute_code_sidebar"):
                global_vars = {
                    "uploaded_file_path": st.session_state.uploaded_file_path,
                    "uploaded_image_path": st.session_state.uploaded_image_path,
                }
                debug_log(f"Executing code with uploaded_file_path = {st.session_state.uploaded_file_path}")
                debug_log(f"Executing code with uploaded_image_path = {st.session_state.uploaded_image_path}")

                result = execute_code(st.session_state.ace_code, global_vars=global_vars)
                st.write("### Execution Result")
                st.text(result)
                debug_log(f"Code execution result: {result}")

if __name__ == "__main__":
    main()
