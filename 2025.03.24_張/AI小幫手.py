import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import json
import traceback
import re
import os
import dotenv
import base64
from io import BytesIO
from openai import OpenAI
from PIL import Image
import google.generativeai as genai  # æ–°å¢Geminiä¾èµ–
from streamlit_ace import st_ace
import time
import matplotlib.font_manager as fm
import matplotlib

import random
import tempfile
from reportlab.lib.pagesizes import letter
from io import BytesIO
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import Paragraph, SimpleDocTemplate, Spacer
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.enums import TA_LEFT
# --- åˆå§‹åŒ–è®¾ç½® ---

font_path = "C:/fonts/msjh.ttc"
fm.fontManager.addfont(font_path)
matplotlib.rcParams['font.family'] = fm.FontProperties(fname=font_path).get_name()
matplotlib.rcParams['axes.unicode_minus'] = False


dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
LLM_MODELS = [  # ä¿®æ”¹åçš„æ¨¡å‹åˆ—è¡¨
    "gpt-4o",
    "gpt-3.5-turbo-16k",
    "gemini-1.5-flash",
    "gemini-1.5-pro",
    "models/gemini-2.0-flash"
]

MAX_MESSAGES = 10  # Limit message history

def initialize_client(api_key):
    return OpenAI(api_key=api_key) if api_key else None

def debug_log(msg):
    if st.session_state.get("debug_mode", False):
        st.session_state.debug_logs.append(f"**DEBUG LOG:** {msg}")
        st.write(msg)
        print(msg)

def debug_error(msg):
    if st.session_state.get("debug_mode", False):
        st.session_state.debug_errors.append(f"**DEBUG ERROR:** {msg}")
        print(msg)

def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    debug_log(f"Saving file to {file_path}")
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    debug_log(f"Files in {UPLOAD_DIR}: {os.listdir(UPLOAD_DIR)}")
    return file_path

def load_image_base64(image):
    """Convert image to Base64 encoding."""
    try:
        buffer = BytesIO()
        image.save(buffer, format="PNG")  # Use PNG for consistency
        return base64.b64encode(buffer.getvalue()).decode('utf-8')
    except Exception as e:
        debug_error(f"Error converting image to base64: {e}")
        return ""

def append_message(role, content):
    """Append a message and ensure the total number of messages does not exceed MAX_MESSAGES."""
    st.session_state.messages.append({"role": role, "content": content})
    if len(st.session_state.messages) > MAX_MESSAGES:
        # Remove the oldest messages except the system prompt
        st.session_state.messages = [st.session_state.messages[0]] + st.session_state.messages[-(MAX_MESSAGES - 1):]
        debug_log("Message history trimmed to maintain token limits.")

def add_user_image(uploaded_file):
    """æ·»åŠ ç”¨æˆ¶åœ–ç‰‡æ¶ˆæ¯åˆ°session state"""
    try:
        st.session_state["last_uploaded_filename"] = uploaded_file.name
        current_model = st.session_state.get("selected_model", "").lower()
        use_base64 = "gpt" in current_model  
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.uploaded_image_path = file_path
        
        if use_base64:
            # ç‚ºOpenAIæ¨¡å‹ç”ŸæˆBase64 URL
            image = Image.open(file_path)
            image_base64 = load_image_base64(image)
            image_url = f"data:image/{file_path.split('.')[-1]};base64,{image_base64}"
        else:
            image_url = file_path
        
        image_msg = {
            "type": "image_url",
            "image_url": {
                "url": image_url,
                "detail": "auto"
            }
        }
        
        if use_base64:
            append_message("user", [image_msg])
        else:
            return image_url
        debug_log(f"åœ–ç‰‡æ¶ˆæ¯å·²æ·»åŠ ï¼š{image_url[:50]}...")
        
        st.session_state.image_base64 = image_base64 if use_base64 else None
        st.rerun()
        
    except Exception as e:
        st.write(f"æ·»åŠ åœ–ç‰‡æ¶ˆæ¯å¤±æ•—ï¼š{str(e)}")
        st.error("åœ–ç‰‡è™•ç†ç•°å¸¸ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")

def reset_session_messages():
    """Clear conversation history from the session."""
    if "messages" in st.session_state:
        st.session_state.pop("messages")
        st.success("Memory cleared!")
        debug_log("Conversation history cleared.")

def execute_code(code, global_vars=None):
    try:
        exec_globals = global_vars if global_vars else {}
        debug_log("Ready to execute the following code:")
        if st.session_state.get("debug_mode", False):
            st.session_state.debug_logs.append(f"```python\n{code}\n```")
        debug_log(f"Executing code with global_vars: {list(exec_globals.keys())}")
        exec(code, exec_globals)
        output = exec_globals.get("output", "(No output returned)")
        debug_log(f"Execution output: {output}")
        return f"Code executed successfully. Output: {output}"
    except Exception as e:
        error_msg = f"Error executing code:\n{traceback.format_exc()}"
        debug_log(f"Execution error: {error_msg}")
        if st.session_state.get("debug_mode", False):
            return error_msg
        else:
            return "Error executing code (hidden in non-debug mode)."

def extract_json_block(response: str) -> str:
    pattern = r'```(?:json)?\s*(.*?)\s*```'
    match = re.search(pattern, response, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
        debug_log(f"Extracted JSON block: {json_str}")
        return json_str
    else:
        debug_log("No JSON block found in response.")
        return response.strip()

# ------------------------------
# èˆŠç‰ˆæœ¬çš„ OpenAI èˆ‡ Gemini å›è¦†æ–¹æ³•ä¿ç•™ï¼ˆä¾›åƒè€ƒï¼‰
# ------------------------------

def get_gemini_response(model_params, max_retries=3):
    """
    æ•´åˆæ–°ç‰ˆ Gemini è«‹æ±‚æ–¹æ³•ï¼Œæ”¯æ´å…ˆè®€å–åœ–ç‰‡ (generate_content) å†é€²è¡Œå®Œæ•´å°è©± (send_message)ã€‚
    æµç¨‹ï¼š
      1) åµæ¸¬æ˜¯å¦æœ‰åœ–ç‰‡ (æœ€å¾Œä¸€å‰‡ user è¨Šæ¯)
      2) å¦‚æœæœ‰ï¼Œå…ˆç”¨ generate_content() å–å¾—ä¸€æ®µå›è¦†ä¸¦åŠ åˆ° messages
      3) æœ€å¾Œç”¨ send_message() æŠŠæ•´å€‹ messages ç™¼é€çµ¦ Geminiï¼Œå–å¾—æœ€çµ‚å›è¦†
    """
    api_key = st.session_state.get("gemini_api_key_input", "")
    debug_log(f"gemini api key: {api_key}")
    if not api_key:
        st.error("æœªè¨­å®š Gemini API é‡‘é‘°")
        return ""
    genai.configure(api_key=api_key)
    model_name = model_params.get("model", "gemini-1.5-flash")
    debug_log(f"gemini model: {model_name}")
    model = genai.GenerativeModel(model_name)
    st.session_state.gemini_chat = model.start_chat(history=[])
    debug_log("Gemini chat session created.")

    gemini_system_prompt = {
        "role": "system",
        "content": "è«‹ä»¥ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”æ‰€æœ‰å›è¦†å¿…é ˆä»¥ #zh-tw å›è¦†é‚„æœ‰å›è¦†æ™‚ä¸ç”¨åœ¨é–‹é ­åŠ ä¸Š#zh-twã€‚"
    }
    st.session_state.messages.insert(0, gemini_system_prompt)
    debug_log("Gemini system prompt for #zh-t added.")

    if st.session_state.uploaded_image_path:
        debug_log("Detected user message with image, using generate_content() first...")
        gen_content = True
    else:
        gen_content = False

    last_user_msg_with_image = None
    for msg in reversed(st.session_state.messages):
        if msg["role"] == "user" and isinstance(msg["content"], list):
            for item in msg["content"]:
                if isinstance(item, dict) and item.get("type") == "image_url":
                    last_user_msg_with_image = msg
                    break
        if last_user_msg_with_image:
            break

    if gen_content:
        debug_log("Detected user message with image, using generate_content() first... by gen_content")
        text_parts = []
        image_data = st.session_state.uploaded_image_path
        debug_log("Using generate_content() first... by gen_content")
        try:
            debug_log("entering generate_content()")
            retries = 0
            while retries < max_retries:
                try:
                    debug_log("entering generate_content() try block")
                    imageee = genai.upload_file(path=image_data, display_name="Image")
                    debug_log(f"imageee: {imageee}")
                    response_gc = model.generate_content(["è«‹ä½ ç¹é«”ä¸­æ–‡è§£è®€åœ–ç‰‡", imageee])
                    debug_log(f"response_gc: {response_gc.text}")
                    generate_content_reply = response_gc.text
                    debug_log(f"Gemini generate_content reply: {generate_content_reply}")
                    append_message("assistant", generate_content_reply)
                    break
                except genai.GenerationError as e:
                    debug_error(f"generate_content() å¤±æ•—: {e}")
                    retries += 1
                    time.sleep(5 * retries)
                except Exception as e:
                    debug_error(f"generate_content() å…¶ä»–éŒ¯èª¤: {e}")
                    return "generate_content Error"
        except Exception as e:
            debug_error(f"generate_content() å…¶ä»–éŒ¯èª¤: {e}")

    converted_history = []
    for msg in st.session_state.messages:
        role = msg.get("role")
        parts = []
        if isinstance(msg["content"], list):
            for item in msg["content"]:
                if isinstance(item, dict) and item.get("type") == "image_url":
                    parts.append(f"[Image included, base64 size={len(item['image_url']['url'])} chars]")
                else:
                    parts.append(str(item))
        else:
            parts.append(str(msg["content"]))
        converted_history.append({"role": role, "parts": parts})
    converted_history_json = json.dumps(converted_history, ensure_ascii=False)
    debug_log(f"converted history (json) => {converted_history_json}")
    retries = 0
    wait_time = 5
    while retries < max_retries:
        try:
            debug_log("Calling send_message with entire conversation...")
            response_sm = st.session_state.gemini_chat.send_message(converted_history_json)
            final_reply = response_sm.text.strip()
            debug_log(f"Gemini send_message final reply => {final_reply}")
            return final_reply
        except genai.GenerationError as e:
            debug_error(f"send_message() ç”ŸæˆéŒ¯èª¤: {e}")
            st.warning(f"Gemini ç”ŸæˆéŒ¯èª¤ï¼Œ{wait_time}ç§’å¾Œé‡è©¦...")
            time.sleep(wait_time)
            retries += 1
            wait_time *= 2
        except Exception as e:
            debug_error(f"send_message() APIè«‹æ±‚ç•°å¸¸: {e}")
            st.error(f"Gemini APIè«‹æ±‚ç•°å¸¸: {e}")
            return ""
    return "è«‹æ±‚å¤±æ•—æ¬¡æ•¸éå¤šï¼Œè«‹ç¨å¾Œé‡è©¦"

def get_openai_response(client, model_params, max_retries=3):
    """å¤„ç†OpenAI APIè¯·æ±‚"""
    retries = 0
    wait_time = 5
    model_name = model_params.get("model", "gpt-4-turbo")
    while retries < max_retries:
        try:
            request_params = {
                "model": model_name,
                "messages": st.session_state.messages,
                "temperature": model_params.get("temperature", 0.3),
                "max_tokens": model_params.get("max_tokens", 4096),
                "stream": False
            }
            if any(msg.get("content") and isinstance(msg["content"], list) for msg in st.session_state.messages):
                request_params["max_tokens"] = 4096
                debug_log("Detected multimodal input, adjusting max_tokens")
            response = client.chat.completions.create(**request_params)
            response_content = response.choices[0].message.content.strip()
            debug_log(f"OpenAIåŸå§‹å“åº”ï¼š\n{response_content}")
            return response_content
        except Exception as e:
            if 'rate limit' in str(e).lower() or '429' in str(e):
                debug_error(f"é€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆå°è¯• {retries+1}/{max_retries}ï¼‰ï¼š{e}")
                st.warning(f"è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œ{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)
                retries += 1
                wait_time *= 2
            elif 'invalid api key' in str(e).lower():
                debug_error(f"APIå¯†é’¥æ— æ•ˆï¼š{e}")
                st.error("OpenAI APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥åé‡è¯•")
                return ""
            else:
                debug_error(f"OpenAIè¯·æ±‚å¼‚å¸¸ï¼š{str(e)}")
                st.error(f"è¯·æ±‚å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
                return ""
    debug_error(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ{max_retries}æ¬¡ï¼‰")
    st.error("è¯·æ±‚å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œè¯·ç¨åå†è¯•")
    return ""



def get_llm_response(client, model_params, max_retries=3):
    """ç²å–LLMæ¨¡å‹å›è¦†ï¼ˆæ”¯æŒOpenAIå’ŒGeminiï¼‰"""
    model_name = model_params.get("model", "gpt-4-turbo")
    debug_log(f"starting to get llm response...{model_name}")
    if "gpt" in model_name:
        debug_log("GPT")
        return get_openai_response(client, model_params, max_retries)
    elif "gemini" in model_name:
        debug_log("Gemini")
        return get_gemini_response(model_params=model_params, max_retries=max_retries)
    else:
        st.error(f"ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹: {model_name}")
        return ""

# ------------------------------
# æ–°å¢å¤šæ¨¡å‹äº¤å‰é©—è­‰å‡½æ•¸
# ------------------------------

def get_cross_validated_response(model_params_gemini, max_retries=3):
    """
    å¤šæ¨¡å‹äº¤å‰é©—è­‰ï¼ˆåƒ…ä½¿ç”¨ Gemini æ¨¡å‹é©—è­‰ï¼‰ï¼š
    1. åœ¨è¨˜æ†¶æµä¸­æ·»åŠ ä¸€å‰‡ç³»çµ±æç¤ºï¼Œè¦æ±‚ Gemini ä½¿ç”¨å…¨éƒ¨å°è©±è¨˜æ†¶é€²è¡Œäº¤å‰é©—è­‰ï¼Œ
       æ¸…æ¥šèªªæ˜å…¶ä»»å‹™ï¼šæª¢æŸ¥å…ˆå‰å›ç­”çš„æ­£ç¢ºæ€§ã€æŒ‡å‡ºæ½›åœ¨éŒ¯èª¤ä¸¦æä¾›æ•¸æ“šæˆ–å…·é«”ç†ç”±æ”¯æŒï¼Œ
       ä¸¦å°æ¯”ä¸åŒæ¨¡å‹çš„å„ªç¼ºé»ï¼ˆè‹¥é©ç”¨ï¼‰ã€‚
    2. å‘¼å« Gemini æ¨¡å‹ (ä¾‹å¦‚ gemini-1.5-flash æˆ– models/gemini-2.0-flash) ç²å–å›ç­”ã€‚
    3. ç§»é™¤è©²ç³»çµ±æç¤ºå¾Œè¿”å› Gemini çš„å›æ‡‰çµæœã€‚
    
    æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä¸å†å‘ OpenAI ç™¼é€è«‹æ±‚ã€‚
    """
    # ç‚º Gemini æ¨¡å‹æ·»åŠ æ›´æ˜ç¢ºçš„ç³»çµ±æç¤ºï¼Œèªªæ˜å…¶ä»»å‹™å…§å®¹
    cross_validation_prompt = {
        "role": "system",
        "content": (
            "è«‹ä»”ç´°é–±è®€ä»¥ä¸‹å…¨éƒ¨å°è©±è¨˜æ†¶ï¼Œå°å…ˆå‰æ¨¡å‹çš„å›ç­”é€²è¡Œäº¤å‰é©—è­‰ã€‚"
            "ä½ çš„ä»»å‹™æ˜¯æª¢æŸ¥å›ç­”çš„æ­£ç¢ºæ€§ï¼ŒæŒ‡å‡ºå…¶ä¸­å¯èƒ½å­˜åœ¨çš„éŒ¯èª¤æˆ–ä¸è¶³ï¼Œ"
            "ä¸¦æä¾›å…·é«”çš„æ•¸æ“šã€ç†ç”±æˆ–ä¾‹å­ä¾†æ”¯æŒä½ çš„åˆ†æã€‚"
            "è«‹å‹™å¿…ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"
            "åœ¨å›ç­”æ™‚è«‹å›ç­”çš„è©³ç´°ï¼Œå…§å®¹éœ€è¦ä½ ç›¡å¯èƒ½çš„å¤šã€‚"
            "ä¸¦ä¸”è¶Šæ¼‚äº®è¶Šå¥½"
        )
    }
    st.session_state.messages.insert(0, cross_validation_prompt)
    
    # å‘¼å« Gemini æ¨¡å‹ï¼Œå…§éƒ¨æœƒå°‡å®Œæ•´è¨˜æ†¶æµä½œç‚ºè¼¸å…¥
    response_gemini = get_gemini_response(model_params_gemini, max_retries)
    
    # ç§»é™¤å‰›å‰›æ·»åŠ çš„ç³»çµ±æç¤ºï¼Œä»¥å…å½±éŸ¿å¾ŒçºŒå°è©±
    st.session_state.messages.pop(0)
    
    final_response = {
        "gemini_response": response_gemini
    }
    return final_response

def simulate_system_message_addition(final_response, report_type="äº¤å‰éªŒè¯æŠ¥å‘Š"):
    """å°†æœ€ç»ˆå“åº”æ¨¡æ‹Ÿä¸ºç³»ç»Ÿç”Ÿæˆæ¶ˆæ¯æ³¨å…¥è®°å¿†æµ"""
    if not final_response or not isinstance(final_response, dict):
        debug_error("æ— æ•ˆçš„final_responseæ ¼å¼")
        return

    # æå–å“åº”å†…å®¹
    response_content = final_response.get("gemini_response", "")
    if not response_content:
        debug_error("final_responseä¸­ç¼ºå°‘gemini_responseé”®")
        return

    # æ„é€ ç³»ç»Ÿæ¶ˆæ¯æ ¼å¼
    formatted_message = {
        "role": "system",
        "content": [
            {
                "type": "text",
                "text": f"# ğŸ” è‡ªåŠ¨ç”Ÿæˆ{report_type}\n{response_content}"
            }
        ]
    }

    # è°ƒç”¨ç°æœ‰æ¶ˆæ¯è¿½åŠ æœºåˆ¶
    append_message(formatted_message["role"], formatted_message["content"])
    debug_log(f"å·²æ³¨å…¥ç³»ç»Ÿæ¶ˆæ¯: {str(formatted_message)[:100]}...")
    
    # æ–°å¢ï¼šæ¸²æŸ“åœ–è¡¨
    if "charts_data" in final_response:
        for chart in final_response["charts_data"]:
            chart_id = chart["id"]
            if chart_id in st.session_state.chart_mapping:
                real_url = st.session_state.chart_mapping[chart_id]
                try:
                    st.image(
                        real_url,
                        caption=f"åœ–è¡¨ {chart_id}",
                        use_container_width=True
                    )
                except Exception as e:
                    st.error(f"åœ–è¡¨ {chart_id} æ¸²æŸ“å¤±æ•—: {str(e)}")

def _render_integrated_report(report_data):
    """ç§æœ‰å‡½æ•¸ï¼šæ¸²æŸ“å ±å‘Šå…§å®¹èˆ‡æ§åˆ¶é …ï¼ˆå®Œæ•´ä¿®æ­£ç‰ˆï¼‰"""
    
    # ===================================================================
    # 1. åŸºæœ¬é©—è­‰èˆ‡éŒ¯èª¤è™•ç†
    # ===================================================================
    if not isinstance(report_data, dict):
        st.error("âŒ ç„¡æ•ˆçš„å ±å‘Šæ•¸æ“šæ ¼å¼")
        debug_error(f"ç„¡æ•ˆå ±å‘Šæ•¸æ“šé¡å‹: {type(report_data)}")
        return

    # ===================================================================
    # 2. æ–‡å­—å ±å‘Šæ¸²æŸ“
    # ===================================================================
    if "gemini_response" in report_data and report_data["gemini_response"]:
        try:
            st.markdown("## ğŸ“ æ•´åˆåˆ†æå ±å‘Š")
            st.markdown(report_data["gemini_response"])
        except Exception as e:
            st.error("å ±å‘Šå…§å®¹æ¸²æŸ“å¤±æ•—")
            debug_error(f"æ–‡å­—æ¸²æŸ“éŒ¯èª¤: {str(e)}\n{traceback.format_exc()}")
    else:
        st.warning("âš ï¸ å ±å‘Šå…§å®¹ç¼ºå¤±ï¼Œå¯èƒ½ç”Ÿæˆå¤±æ•—æˆ–ç„¡æœ‰æ•ˆåˆ†æçµæœ")

    # ===================================================================
    # 3. åœ–è¡¨æ¸²æŸ“æ ¸å¿ƒé‚è¼¯
    # ===================================================================
    if "charts_data" in report_data and report_data["charts_data"]:
        st.markdown("---")
        st.markdown("## ğŸ“Š ç›¸é—œåœ–è¡¨")
        
        # åˆå§‹åŒ–æ˜ å°„è¡¨æª¢æŸ¥
        if "chart_mapping" not in st.session_state:
            st.error("åœ–è¡¨æ˜ å°„è¡¨ä¸Ÿå¤±ï¼Œè«‹é‡æ–°ç”Ÿæˆå ±å‘Š")
            return

        for chart in report_data["charts_data"]:
            # 3.1 æ•¸æ“šæœ‰æ•ˆæ€§é©—è­‰
            if not isinstance(chart, dict) or "id" not in chart:
                debug_error(f"ç„¡æ•ˆåœ–è¡¨æ•¸æ“šæ ¼å¼: {chart}")
                continue
                
            chart_id = chart["id"]
            
            # 3.2 å¾æ˜ å°„è¡¨ç²å–çœŸå¯¦æ•¸æ“š
            if chart_id not in st.session_state.chart_mapping:
                st.warning(f"åœ–è¡¨ {chart_id} æ•¸æ“šç¼ºå¤±ï¼Œå¯èƒ½å·²è¢«æ¸…ç†")
                debug_error(f"æ˜ å°„è¡¨ç¼ºå°‘ {chart_id}ï¼Œç•¶å‰æ˜ å°„è¡¨: {list(st.session_state.chart_mapping.keys())}")
                continue
                
            real_url = st.session_state.chart_mapping[chart_id]

            # 3.3 å‹•æ…‹æ¸²æŸ“
            try:
                col1, col2 = st.columns([0.8, 0.2])
                with col1:
                    # çµ±ä¸€å¾æ˜ å°„è¡¨åŠ è¼‰
                    st.image(
                        real_url,
                        caption=f"åœ–è¡¨ {chart_id}",
                        use_container_width=True,
                        output_format="PNG"  # ç¢ºä¿ç›¸å®¹æ€§
                    )
                with col2:
                    # æ·»åŠ äº’å‹•å…ƒç´ 
                    with st.expander("ğŸ” åŸå§‹æ•¸æ“š"):
                        st.code(f"åœ–è¡¨ID: {chart_id}\nå­˜å„²è·¯å¾‘: {real_url[:100]}...", language="text")
                        
            except Exception as e:
                error_msg = f"åœ–è¡¨ {chart_id} æ¸²æŸ“å¤±æ•—: {str(e)}"
                st.error(error_msg)
                debug_error(f"{error_msg}\n{traceback.format_exc()}")

    # ===================================================================
    # 4. PDFä¸‹è¼‰æŒ‰éˆ•ï¼ˆå¼·åŒ–éŒ¯èª¤è™•ç†ï¼‰
    # ===================================================================
    if report_data.get("pdf_buffer"):
        try:
            # ä½¿ç”¨è‡¨æ™‚æ–‡ä»¶ç¢ºä¿è·¨å¹³å°ç›¸å®¹æ€§
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(report_data["pdf_buffer"].getvalue())
                tmp_path = tmp.name
                
            with open(tmp_path, "rb") as f:
                st.download_button(
                    label="â¬‡ï¸ ä¸‹è¼‰å®Œæ•´å ±å‘Š (PDF)",
                    data=f,
                    file_name="æ•´åˆåˆ†æå ±å‘Š.pdf",
                    mime="application/pdf",
                    help="åŒ…å«æ–‡å­—åˆ†æèˆ‡æ‰€æœ‰é—œè¯åœ–è¡¨",
                    key=f"dl_{hash(time.time())}"  # é¿å…æŒ‰éˆ•IDè¡çª
                )
                
        except Exception as e:
            st.error("PDFæ–‡ä»¶ç”Ÿæˆå¤±æ•—ï¼Œè«‹é‡è©¦æˆ–è¯ç¹«ç®¡ç†å“¡")
            debug_error(f"PDFä¸‹è¼‰éŒ¯èª¤: {str(e)}\n{traceback.format_exc()}")
            
        tmp_path = ""
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp_path = tmp.name  # ç²å–å®Œæ•´è·¯å¾‘
                # ... å¯«å…¥æ•¸æ“š
        finally:
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except Exception as e:
                    debug_error(f"è‡¨æ™‚æ–‡ä»¶æ¸…ç†å¤±æ•—: {str(e)}")

def generate_integrated_report(model_params_gemini, max_retries=3):
    """ç›´æ¥åˆ†æå®Œæ•´è¨˜æ†¶æµç”Ÿæˆæ•´åˆå ±å‘Š"""
    default_report = {
        "gemini_response": "å ±å‘Šç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ",
        "charts_data": [],
        "pdf_buffer": None
    }
    
    try:
        # ==== æå–åˆ†æææ–™ ====
        analysis_materials = {
            "gpt_reports": [],
            "gemini_reports": [],
            "charts": [],
            "code_blocks": []
        }
        
        # ç¢ºä¿åœ–è¡¨æ˜ å°„è¡¨å­˜åœ¨
        if "chart_mapping" not in st.session_state:
            st.session_state.chart_mapping = {}

        # è®€å– CSV æ•¸æ“šä»¥æä¾›çµ¦åˆ†ææç¤º
        csv_data = None
        if hasattr(st.session_state, 'uploaded_file_path') and st.session_state.uploaded_file_path:
            try:
                csv_data = pd.read_csv(st.session_state.uploaded_file_path)
                debug_log(f"CSV data loaded for analysis: {csv_data.shape}")
            except Exception as e:
                debug_error(f"Error reading CSV for analysis: {e}")
        
        # éå†æ¶ˆæ¯å†å²æå–å…³é”®ä¿¡æ¯
        for idx, msg in enumerate(st.session_state.messages):
            # å¤„ç† Assistant æ¶ˆæ¯ï¼šæå–æ¨¡å‹æŠ¥å‘Š
            if msg["role"] == "assistant":
                content = msg["content"]
                # æ ‡å‡†åŒ–å†…å®¹æ ¼å¼ï¼ˆå¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯ï¼‰
                if isinstance(content, list):
                    content = " ".join([
                        item.get("text", "") 
                        for item in content 
                        if isinstance(item, dict)
                    ])
                # è¯†åˆ«æŠ¥å‘Šæ¥æº
                if "GPT" in content or "gpt" in content.lower():
                    analysis_materials["gpt_reports"].append((idx, content))
                elif "Gemini" in content or "gemini" in content.lower():
                    analysis_materials["gemini_reports"].append((idx, content))
            
            # å¤„ç† User æ¶ˆæ¯ï¼šæå–å›¾è¡¨ä½†ä¸åŒ…å« Base64 æ•°æ®
            if msg["role"] == "user" and isinstance(msg["content"], list):
                for item in msg["content"]:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        chart_id = f"chart_{len(analysis_materials['charts']) + 1}"
                        image_url = item["image_url"]["url"]
                        
                        # å°‡åœ–ç‰‡è·¯å¾‘ä¿å­˜åˆ°æ˜ å°„è¡¨ä¸­ï¼Œä½†ä¸è½‰æ›ç‚º Base64 ä»¥ç¯€çœ Token
                        # åƒ…ç•¶ç”Ÿæˆ PDF æˆ–é¡¯ç¤ºåœ–è¡¨æ™‚æ‰å¯¦éš›è®€å–åœ–ç‰‡
                        st.session_state.chart_mapping[chart_id] = image_url
                        
                        # åœ¨åˆ†æææ–™ä¸­åƒ…ä¿å­˜åœ–è¡¨ ID å’Œåƒè€ƒæ¨™ç±¤ï¼Œè€Œéå®Œæ•´åœ–åƒæ•¸æ“š
                        analysis_materials["charts"].append({
                            "id": chart_id,
                            "label": f"Chart {len(analysis_materials['charts']) + 1}"
                        })
                        
                        debug_log(f"å·²ç´¢å¼•åœ–è¡¨: {chart_id} -> {image_url[:30]}...")
            
            # æå–ä»£ç ç‰‡æ®µï¼ˆç‹¬ç«‹äºè§’è‰²ï¼‰
            if "```python" in str(msg["content"]):
                code_blocks = re.findall(r'```python(.*?)```', str(msg["content"]), re.DOTALL)
                if code_blocks:
                    analysis_materials["code_blocks"].append({
                        "position": idx,
                        "code": code_blocks[0].strip()
                    })

        # æ§‹å»ºåœ–è¡¨å¼•ç”¨ä¿¡æ¯ï¼ˆåƒ…ä½¿ç”¨ ID è€Œéæ•¸æ“šï¼‰
        chart_references = []
        for idx, chart in enumerate(analysis_materials["charts"]):
            chart_references.append(f"   - {chart['id']}: åƒè€ƒæ¨™ç±¤ '{chart['label']}'")
        
        # å»ºæ§‹å„ªåŒ–çš„åˆ†ææç¤ºè© (ä¸åŒ…å« Base64 æ•¸æ“š)
        analysis_prompt = f"""
[ç³»çµ±è§’è‰²]
æ‚¨ç¾åœ¨æ˜¯è³‡æ–™ç§‘å­¸èˆ‡AIåˆ†æå°ˆå®¶ï¼Œè«‹åŸºæ–¼å®Œæ•´å°è©±è¨˜æ†¶æµèˆ‡CSVè³‡æ–™åŸ·è¡Œå…¨é¢åˆ†æã€‚æ‚¨çš„ä»»å‹™æ˜¯æä¾›è©³ç›¡ä¸”è³‡æ–™é©…å‹•çš„å ±å‘Šï¼Œçªé¡¯é—œéµç™¼ç¾å’Œå¯¦ç”¨è¦‹è§£ã€‚

[è¼¸å…¥è³‡æ–™çµæ§‹]
### åŸå§‹è¨Šæ¯æµæ¦‚è¦½
è¨Šæ¯ç¸½æ•¸ï¼š{len(st.session_state.messages)}
æœ€æ–°GPTå ±å‘Šä½ç½®ï¼š{analysis_materials['gpt_reports'][-1][0] if analysis_materials['gpt_reports'] else 'ç„¡'}
æœ€æ–°Geminiå ±å‘Šä½ç½®ï¼š{analysis_materials['gemini_reports'][-1][0] if analysis_materials['gemini_reports'] else 'ç„¡'}

### CSVè³‡æ–™åŸºæœ¬è³‡è¨Šï¼ˆå¿…é ˆæ·±å…¥åˆ†æï¼‰
æª”æ¡ˆåç¨±ï¼š{st.session_state.uploaded_file_path.split('/')[-1] if hasattr(st.session_state, 'uploaded_file_path') and st.session_state.uploaded_file_path else 'ç„¡ä¸Šå‚³æª”æ¡ˆ'}
è³‡æ–™å¤§å°ï¼š{f"{csv_data.shape[0]} åˆ— Ã— {csv_data.shape[1]} æ¬„" if csv_data is not None else 'ç„¡è³‡æ–™'}
æ¬„ä½åç¨±ï¼š{', '.join(csv_data.columns.tolist()) if csv_data is not None else 'ç„¡è³‡æ–™'}
{
    "æ•¸å€¼å‹æ¬„ä½çµ±è¨ˆï¼š\n" + "\n".join([f"   - {col}: å¹³å‡å€¼={csv_data[col].mean():.2f}, ä¸­ä½æ•¸={csv_data[col].median():.2f}, æ¨™æº–å·®={csv_data[col].std():.2f}" 
                                for col in csv_data.select_dtypes(include=['number']).columns]) 
    if csv_data is not None and not csv_data.select_dtypes(include=['number']).empty 
    else "ç„¡æ•¸å€¼å‹æ¬„ä½çµ±è¨ˆè³‡æ–™"
}
{
    "é¡åˆ¥å‹æ¬„ä½çµ±è¨ˆï¼š\n" + "\n".join([f"   - {col}: å”¯ä¸€å€¼æ•¸é‡={csv_data[col].nunique()}, æœ€å¸¸è¦‹å€¼='{csv_data[col].mode()[0]}' (å‡ºç¾{csv_data[col].value_counts().iloc[0]}æ¬¡)" 
                                for col in csv_data.select_dtypes(include=['object', 'category']).columns]) 
    if csv_data is not None and not csv_data.select_dtypes(include=['object', 'category']).empty 
    else "ç„¡é¡åˆ¥å‹æ¬„ä½çµ±è¨ˆè³‡æ–™"
}
{
    "ç¼ºå¤±å€¼åˆ†æï¼š\n" + "\n".join([f"   - {col}: {csv_data[col].isna().sum()}å€‹ç¼ºå¤±å€¼ ({csv_data[col].isna().mean()*100:.1f}%)" 
                               for col in csv_data.columns if csv_data[col].isna().any()]) 
    if csv_data is not None and csv_data.isna().any().any() 
    else "ç„¡ç¼ºå¤±å€¼"
}

### å¯é©—è­‰ç´ æ
1. åˆ†æåœ–è¡¨ï¼ˆå…±{len(analysis_materials['charts'])}å¼µï¼‰ï¼š
{chr(10).join(chart_references)}

2. ç¨‹å¼ç¢¼ç‰‡æ®µï¼ˆå…±{len(analysis_materials['code_blocks'])}æ®µï¼‰ï¼š
{chr(10).join([f"   - ä½ç½®{cb['position']}: {cb['code'][:50]}..." for cb in analysis_materials['code_blocks']])}

3. å…ˆå‰åˆ†æç¸½çµ
- è«‹å¾å°è©±æ­·å²ä¸­æå–æ‰€æœ‰æ¨¡å‹ä¹‹å‰çš„åˆ†æçµè«–
- ç¢ºä¿é€™äº›å…ˆå‰çš„ç™¼ç¾ä¸æœƒåœ¨æ–°å ±å‘Šä¸­ä¸Ÿå¤±
- ç‰¹åˆ¥æ³¨æ„ä¹‹å‰ç”Ÿæˆçš„æ•¸æ“šæŒ‡æ¨™å’Œè¶¨å‹¢é æ¸¬

[æ ¸å¿ƒä»»å‹™]
åŸ·è¡Œä¸‰ç¶­åº¦äº¤å‰é©—è­‰ä¸¦æä¾›æ·±å…¥CSVè³‡æ–™åˆ†æï¼š

ğŸ” CSVæ•¸æ“šæ·±å…¥åˆ†æï¼ˆå¿…é ˆå®Œæˆï¼‰
   - æä¾›ä¸Šå‚³CSVæ–‡ä»¶çš„å…¨é¢åˆ†æï¼Œè­˜åˆ¥é—œéµæ¨¡å¼ã€ç›¸é—œæ€§å’Œç•°å¸¸
   - äº¤å‰åˆ†ææ•¸å€¼å‹å’Œé¡åˆ¥å‹æ¬„ä½ä¹‹é–“çš„é—œä¿‚
   - æŒ–æ˜ä¸¦é¡¯ç¤ºä¸å®¹æ˜“è¢«å¯Ÿè¦ºçš„æ•¸æ“šæ´è¦‹
   - åŸºæ–¼CSVæ•¸æ“šæä¾›å…·é«”çš„å»ºè­°å’Œè¡Œå‹•æ–¹æ¡ˆ

ğŸ“Š æ¨¡å‹åˆ†ææ¯”è¼ƒ
   - æ¯”å°GPTèˆ‡Geminiåœ¨é—œéµçµè«–é»çš„å·®ç•°
   - è­˜åˆ¥çŸ›ç›¾ç´šåˆ¥ï¼ˆè¼•å¾®/ä¸­åº¦/åš´é‡ï¼‰
   - ç¯„ä¾‹ï¼šåœ¨éŠ·å”®é æ¸¬ä¸­ï¼ŒGPTé æ¸¬Q3å¢é•·{{x}}%è€ŒGeminié æ¸¬{{y}}%ï¼Œå·®ç•°æºæ–¼...

âš™ï¸ è­‰æ“šéˆå®Œæ•´æ€§å¯©æŸ¥
   - é©—è­‰åœ–è¡¨èˆ‡çµè«–çš„å°æ‡‰é—œä¿‚ï¼ˆè«‹é€šéåœ–è¡¨IDå¼•ç”¨ï¼Œå¦‚ï¼šåœ–è¡¨ chart_1 é¡¯ç¤º...ï¼‰
   - æª¢æŸ¥ç¨‹å¼ç¢¼ç‰‡æ®µæ˜¯å¦æ”¯æŒåˆ†æçµè«–
   - ç¯„ä¾‹ï¼šåœ¨ä½ç½®{analysis_materials['code_blocks'][0]['position'] if analysis_materials['code_blocks'] else 'N/A'}çš„ç¨‹å¼ç¢¼ä¸­...

[å¼·åˆ¶æ ¼å¼]
```markdown
# æ•´åˆè³‡æ–™åˆ†æå ±å‘Š

## CSVæª”æ¡ˆç¶œåˆåˆ†æ
- æª”æ¡ˆç¸½è¦½ï¼šè©³è¿°æª”æ¡ˆç‰¹å¾µå’Œé—œéµçµæ§‹
- æ•¸æ“šé—œéµç™¼ç¾ï¼šè«‹ä»¥æ®µè½å¼è«–è¿°è€Œéè¡¨æ ¼æ–¹å¼å‘ˆç¾è‡³å°‘3é …é—œéµç™¼ç¾ï¼Œæ¯é …ç™¼ç¾éœ€åŒ…æ‹¬ï¼š
  - åˆ†æç¶­åº¦ï¼šå…·é«”åˆ†æçš„è§’åº¦æˆ–æŒ‡æ¨™
  - æ•¸æ“šè­‰æ“šï¼šæ”¯æŒé€™ä¸€ç™¼ç¾çš„æ•¸æ“šäº‹å¯¦ï¼Œå«å…·é«”æ•¸å€¼
  - æ¥­å‹™å½±éŸ¿ï¼šè©²ç™¼ç¾å°ç›¸é—œæ¥­å‹™å¯èƒ½ç”¢ç”Ÿçš„å½±éŸ¿
  - å»ºè­°è¡Œå‹•ï¼šåŸºæ–¼ç™¼ç¾æå‡ºçš„å…·é«”å¯åŸ·è¡Œå»ºè­°

## æ ¸å¿ƒå·®ç•°åˆ†æ
é‡å°GPTèˆ‡Geminiåˆ†æå·®ç•°ï¼Œè«‹ä»¥æ®µè½å¼è«–è¿°è€Œéè¡¨æ ¼æ–¹å¼å‘ˆç¾ä»¥ä¸‹å…§å®¹ï¼š
1. ã€å·®ç•°ä¸€ã€‘ï¼šè©³ç´°æè¿°å·®ç•°ç¶­åº¦ã€å„æ¨¡å‹è§€é»åŠæ•´åˆçµè«–
2. ã€å·®ç•°äºŒã€‘ï¼šè©³ç´°æè¿°å·®ç•°ç¶­åº¦ã€å„æ¨¡å‹è§€é»åŠæ•´åˆçµè«–
3. ã€å·®ç•°ä¸‰ã€‘ï¼šè©³ç´°æè¿°å·®ç•°ç¶­åº¦ã€å„æ¨¡å‹è§€é»åŠæ•´åˆçµè«–

## æ•¸æ“šæ´å¯Ÿ
### 1. è¶¨å‹¢èˆ‡æ¨¡å¼
- æ™‚é–“åºåˆ—ç™¼ç¾ï¼šè©³ç´°åˆ†ææ™‚é–“ç›¸é—œçš„è¶¨å‹¢ã€å‘¨æœŸæ€§å’Œè®ŠåŒ–é»
- ç›¸é—œæ€§åˆ†æï¼šè©³ç´°åˆ†æå„è®Šé‡é–“çš„é—œè¯æ€§ï¼ŒåŒ…æ‹¬å¼·ç›¸é—œå’Œå¼±ç›¸é—œé—œä¿‚
- ç•°å¸¸é»è­˜åˆ¥ï¼šæ˜ç¢ºæŒ‡å‡ºæ•¸æ“šä¸­çš„ç•°å¸¸é»ï¼Œåˆ†ææˆå› ä¸¦è©•ä¼°å½±éŸ¿

### 2. é æ¸¬èˆ‡å»ºè­°
- è¶¨å‹¢é æ¸¬ï¼šåŸºæ–¼æ­·å²æ•¸æ“šå’Œæ¨¡å¼å°æœªä¾†è¶¨å‹¢ä½œå‡ºå…·é«”é æ¸¬
- æ¥­å‹™å»ºè­°ï¼šæä¾›3-5æ¢å…·é«”å¯è¡Œçš„æ¥­å‹™å»ºè­°ï¼Œæ¯æ¢åŒ…å«ç†ç”±å’Œé æœŸæ”¶ç›Š
- å¾ŒçºŒåˆ†ææ–¹å‘ï¼šè‡³å°‘3å€‹å€¼å¾—é€²ä¸€æ­¥æ¢ç´¢çš„åˆ†ææ–¹å‘

## æ–¹æ³•è«–è©•ä¼°
- åˆ†ææŠ€è¡“æ¯”è¼ƒï¼šè©³ç´°æ¯”è¼ƒä¸åŒåˆ†ææ–¹æ³•çš„å„ªç¼ºé»å’Œé©ç”¨å ´æ™¯
- æ¨¡å‹æº–ç¢ºæ€§ï¼šè©•ä¼°å„åˆ†ææ¨¡å‹çš„æº–ç¢ºåº¦ã€ç½®ä¿¡å€é–“å’Œå¯é æ€§
- äº¤å‰é©—è­‰çµæœï¼šæè¿°äº¤å‰é©—è­‰çš„æ–¹æ³•èˆ‡çµæœï¼Œä»¥åŠå°çµè«–å¯ä¿¡åº¦çš„å½±éŸ¿

## é¢¨éšªèˆ‡å„ªåŒ–å»ºè­°
- è³‡æ–™å•é¡Œï¼šæ˜ç¢ºæŒ‡å‡ºæ•¸æ“šæ”¶é›†ã€è™•ç†æˆ–åˆ†æä¸­çš„æ½›åœ¨å•é¡Œ
- åˆ†æå±€é™æ€§ï¼šå¦èª åˆ†ææœ¬æ¬¡åˆ†æçš„å±€é™æ€§å’Œæ½›åœ¨åå·®
- æ”¹é€²å»ºè­°ï¼šè‡³å°‘3æ¢æå‡æ•¸æ“šè³ªé‡æˆ–åˆ†ææ•ˆæœçš„å…·é«”å»ºè­°

## å„ªåŒ–è·¯ç·š
1. ç«‹å³è¡Œå‹•ï¼šè©³ç´°èª¬æ˜å¯ç«‹å³å¯¦æ–½çš„æ”¹é€²æªæ–½ï¼ŒåŒ…æ‹¬å…·é«”æ­¥é©Ÿå’Œé æœŸæ•ˆæœ
2. ä¸­æœŸè¨ˆåŠƒï¼šåˆ†æä¸­æœŸï¼ˆ3-6å€‹æœˆï¼‰å„ªåŒ–æ–¹å‘å’Œå…·é«”å¯¦æ–½è¨ˆåŠƒ
3. é•·æœŸæˆ°ç•¥ï¼šæå‡ºé•·æœŸæ•¸æ“šåˆ†ææˆ°ç•¥å»ºè­°ï¼ŒåŒ…æ‹¬è³‡æºéœ€æ±‚å’Œé æœŸæ”¶ç›Š
```

[ç‰¹åˆ¥æŒ‡ä»¤]
1. å¿…é ˆä½¿ç”¨è¨Šæ¯ä½ç½®æ¨™è¨˜ä¾†æºï¼ˆå¦‚ï¼š@msg_12ï¼‰
2. ç¦ç”¨æ¨¡ç³Šè©å½™ï¼ˆ"å¯èƒ½"ã€"å¤§æ¦‚"ç­‰ï¼‰ï¼Œéœ€æ˜ç¢ºçµè«–
3. å¼•ç”¨åœ–è¡¨æ™‚ï¼Œåªéœ€ä½¿ç”¨åœ–è¡¨IDï¼ˆå¦‚ chart_1, chart_2 ç­‰ï¼‰ï¼Œç„¡éœ€æè¿°åœ–è¡¨å…§å®¹
4. æ–°å¢é©—è­‰å“ˆå¸Œï¼š{{"hash": "{hash(str(st.session_state.messages))}"}}
5. å¿…é ˆè©³ç´°åˆ†æä¸Šå‚³çš„CSVè³‡æ–™ï¼Œä¸¦å°‡å…¶ä½œç‚ºå ±å‘Šçš„æ ¸å¿ƒéƒ¨åˆ†
6. ç¦æ­¢ç”Ÿæˆç©ºæ³›çš„çµè«–ï¼Œæ¯å€‹çµè«–å¿…é ˆæœ‰CSVæ•¸æ“šæ”¯æŒ
7. ç¢ºä¿å ±å‘Šçµæ§‹å®Œæ•´ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ¨™é¡Œéƒ¨åˆ†
8. ç¦æ­¢ä½¿ç”¨è¡¨æ ¼æ ¼å¼é€²è¡Œå‘ˆç¾ï¼Œä½¿ç”¨æ®µè½å¼è«–è¿°ä»£æ›¿
9. å ±å‘Šå¿…é ˆæœ‰è¶³å¤ æ·±åº¦ï¼Œæ¯å€‹éƒ¨åˆ†è‡³å°‘åŒ…å«3-5å€‹æ®µè½çš„è©³ç´°åˆ†æ
10. é©ç•¶ä½¿ç”¨è¦é»ç¬¦è™Ÿå’Œç·¨è™Ÿå‘ˆç¾å¤šå€‹è§€é»ï¼Œä½†ä¸»è¦çµè«–éœ€ç”¨å®Œæ•´æ®µè½å±•é–‹
"""
        # ç”Ÿæˆ Gemini å“åº”
        cross_validation_prompt = {
            "role": "system",
            "content": analysis_prompt
        }
        
        # æ’å…¥ç³»çµ±æç¤ºä¸¦ç²å–éŸ¿æ‡‰
        st.session_state.messages.insert(0, cross_validation_prompt)
        response_gemini = get_gemini_response(model_params_gemini, max_retries)
        st.session_state.messages.pop(0)

        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š - æ­¤æ™‚å¾æ˜ å°„è¡¨æŸ¥è©¢åœ–ç‰‡æ•¸æ“š
        final_report = {
            "gemini_response": response_gemini,
            "charts_data": [],
            "pdf_buffer": None
        }
        
        # å°‡åœ–è¡¨å¼•ç”¨è½‰æ›ç‚ºå®Œæ•´åœ–è¡¨æ•¸æ“šï¼ˆåƒ…åœ¨å ±å‘Šæ¸²æŸ“éšæ®µï¼‰
        for chart in analysis_materials["charts"]:
            chart_id = chart["id"]
            if chart_id in st.session_state.chart_mapping:
                final_report["charts_data"].append({
                    "id": chart_id,
                    "label": chart.get("label", f"Chart {chart_id}")
                })
        
        # ç”Ÿæˆ PDF
        pdf_buffer = _generate_pdf(final_report)
        final_report["pdf_buffer"] = pdf_buffer
        
        # å°‡æœ€çµ‚å ±å‘Šä¿å­˜åˆ°session_stateä¸­ï¼Œä¾›æ–°é é¢ä½¿ç”¨
        st.session_state.integrated_report = final_report
        
        # åœ¨ç•¶å‰é é¢æ¸²æŸ“å ±å‘Š
        _render_integrated_report(final_report)
        
        # æ·»åŠ å‰å¾€æ–°é é¢çš„æŒ‰éˆ•
        st.markdown("---")
        st.markdown("## ğŸ”„ å ±å‘Šé é¢")
        st.info("æ‚¨å¯ä»¥åœ¨ç¨ç«‹å ±å‘Šé é¢æŸ¥çœ‹å®Œæ•´å ±å‘Šå…§å®¹")
        
        return final_report
        
    except Exception as e:
        debug_error(f"ç”Ÿæˆæ•´åˆå ±å‘Šå¤±æ•—: {str(e)}\n{traceback.format_exc()}")
        st.error("å ±å‘Šç”Ÿæˆç•°å¸¸ï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
        return default_report

def _generate_pdf(report_data):
    """å°†æŠ¥å‘Šå†…å®¹ä¸å›¾è¡¨ç”ŸæˆPDF"""


    # è¨»å†Šä¸­æ–‡å­—é«” - ä½¿ç”¨ç³»çµ±è‡ªå¸¶çš„ä¸­æ–‡å­—é«”
    try:
        # å˜—è©¦è¨»å†ŠWindowsä¸‹çš„å¾®è»Ÿé›…é»‘å­—é«”
        pdfmetrics.registerFont(TTFont('SimSun', 'C:/Windows/Fonts/simsun.ttc'))
        cn_font_name = 'SimSun'
    except:
        try:
            # å˜—è©¦è¨»å†ŠArial Unicode MS (å»£æ³›æ”¯æŒUnicodeå­—ç¬¦)
            pdfmetrics.registerFont(TTFont('Arial', 'C:/Windows/Fonts/arial.ttf'))
            cn_font_name = 'Arial'
        except:
            # å¦‚æœä¸Šè¿°å­—é«”éƒ½ç„¡æ³•æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜èªå­—é«”
            cn_font_name = 'Helvetica'
            debug_error("ç„¡æ³•æ‰¾åˆ°æ”¯æŒä¸­æ–‡çš„å­—é«”ï¼ŒPDFä¸­çš„ä¸­æ–‡å¯èƒ½é¡¯ç¤ºä¸æ­£ç¢º")
    
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    story = []
    
    # å»ºç«‹æ–‡æª”æ¨£å¼
    styles = getSampleStyleSheet()
    normal_style = ParagraphStyle(
        'NormalWithCN',
        parent=styles['Normal'],
        fontName=cn_font_name,
        fontSize=10,
        leading=14,  # è¡Œé–“è·
        wordWrap='CJK',  # æ”¯æ´ä¸­æ—¥éŸ“æ–‡å­—æ›è¡Œ
    )
    
    title_style = ParagraphStyle(
        'TitleWithCN',
        parent=styles['Heading1'],
        fontName=cn_font_name,
        fontSize=16,
        leading=20,
        alignment=TA_LEFT,
    )
    
    # æ·»åŠ æ¨™é¡Œ
    story.append(Paragraph("æ•´åˆåˆ†æå ±å‘Š", title_style))
    story.append(Spacer(1, 12))
    
    # æª¢æŸ¥å¿…è¦å­—æ®µ
    if "gemini_response" not in report_data:
        debug_error("PDFç”Ÿæˆå¤±æ•—ï¼šç¼ºå°‘ gemini_response å­—æ®µ")
        story.append(Paragraph("å ±å‘Šç”Ÿæˆå¤±æ•—ï¼Œç¼ºå°‘å¿…è¦æ•¸æ“š", normal_style))
        doc.build(story)
        buffer.seek(0)
        return buffer
    
    # è™•ç†å ±å‘Šå†…å®¹ (ä½¿ç”¨Paragraphä¾†æ”¯æŒæ›è¡Œå’Œæ ¼å¼åŒ–)
    text_content = report_data["gemini_response"] or "ï¼ˆå ±å‘Šå†…å®¹ç‚ºç©ºï¼‰"
    
    # è™•ç†Markdownæ ¼å¼
    text_content = text_content.replace('\n\n', '<br/><br/>')
    text_content = text_content.replace('\n', '<br/>')
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹
    story.append(Paragraph(text_content, normal_style))
    
    # æ’å…¥åœ–è¡¨ï¼ˆåƒ…è™•ç†æœ‰æ•ˆæ•¸æ“šï¼‰
    if report_data.get("charts_data"):
        story.append(Spacer(1, 20))
        story.append(Paragraph("ğŸ“Š ç›¸é—œåœ–è¡¨", title_style))
        story.append(Spacer(1, 12))
    
    for idx, chart in enumerate(report_data.get("charts_data", [])):
        chart_id = chart["id"]
        if chart_id in st.session_state.chart_mapping:
            chart_fig = st.session_state.chart_mapping[chart_id]
            try:
                # ä½¿ç”¨Paragraphæ·»åŠ åœ–è¡¨æ¨™é¡Œ
                chart_title = f"åœ–è¡¨ {idx+1}: {chart.get('label', chart_id)}"
                story.append(Spacer(1, 10))
                story.append(Paragraph(chart_title, normal_style))
                
                # è™•ç†ä¸åŒé¡å‹çš„åœ–è¡¨æ•¸æ“š
                img_bytes = None
                
                # æª¢æŸ¥æ•¸æ“šé¡å‹ä¸¦ç›¸æ‡‰è™•ç†
                try:
                    if hasattr(chart_fig, 'write_image'):  # Plotlyåœ–è¡¨å°è±¡
                        debug_log(f"è™•ç†Plotlyåœ–è¡¨: {chart_id}")
                        img_bytes = BytesIO()
                        chart_fig.write_image(img_bytes, format='png')
                        img_bytes.seek(0)
                    elif isinstance(chart_fig, str):  # å­—ç¬¦ä¸²ï¼ˆURLæˆ–base64ï¼‰
                        if chart_fig.startswith('data:image'):
                            debug_log(f"è™•ç†Base64åœ–åƒ: {chart_id}")
                            # è™•ç†base64ç·¨ç¢¼çš„åœ–åƒ
                            header, data = chart_fig.split(",", 1)
                            img_bytes = BytesIO(base64.b64decode(data))
                        else:
                            # è¨˜éŒ„ä¸æ”¯æŒçš„å­—ç¬¦ä¸²æ ¼å¼
                            debug_log(f"åœ–è¡¨æ ¼å¼ä¸æ”¯æŒ: {chart_id} - å­—ç¬¦ä¸²ä½†éBase64")
                            story.append(Paragraph(f"åœ–è¡¨ {chart_id} æ ¼å¼ä¸æ”¯æŒï¼Œç„¡æ³•åœ¨PDFä¸­é¡¯ç¤º", normal_style))
                            continue
                    else:
                        # ä½¿ç”¨é»˜èªæ–¹å¼å˜—è©¦è™•ç†
                        debug_log(f"å˜—è©¦é»˜èªè™•ç†åœ–è¡¨: {chart_id} - é¡å‹ {type(chart_fig)}")
                        img_bytes = BytesIO()
                        chart_fig.write_image(img_bytes, format='png')
                        img_bytes.seek(0)
                except Exception as e:
                    debug_error(f"è™•ç†åœ–è¡¨æ•¸æ“šå¤±æ•—: {chart_id} - {str(e)}")
                    story.append(Paragraph(f"åœ–è¡¨ {chart_id} è™•ç†å¤±æ•—: {str(e)}", normal_style))
                    continue
                
                # æ·»åŠ åœ–ç‰‡åˆ°æ–‡æª”
                if img_bytes:
                    from reportlab.platypus import Image
                    # èª¿æ•´åœ–ç‰‡å¤§å°ï¼Œç¢ºä¿ä¸è¶…éé é¢å¯¬åº¦ï¼Œè¨­å®šæœ€å¤§å¯¬åº¦ç‚º400ï¼ˆå°æ–¼é é¢å¯¬åº¦456ï¼‰
                    img = Image(img_bytes, width=400, height=300, kind='proportional')
                    story.append(img)
                    story.append(Paragraph(f"åœ–è¡¨ID: {chart_id}", normal_style))
                    story.append(Spacer(1, 10))
                else:
                    story.append(Paragraph(f"åœ–è¡¨ {chart_id} ç„¡æ³•ç²å–åœ–åƒæ•¸æ“š", normal_style))
            except Exception as e:
                debug_error(f"PDFæ’å…¥åœ–è¡¨å¤±æ•—: {str(e)}")
                story.append(Paragraph(f"åœ–è¡¨ {chart_id} è™•ç†å¤±æ•—: {str(e)}", normal_style))
    
    # ç”ŸæˆPDF
    try:
        doc.build(story)
        buffer.seek(0)
        return buffer
    except Exception as e:
        error_msg = f"PDFç”Ÿæˆå¤±æ•—: {str(e)}"
        traceback_msg = traceback.format_exc()
        debug_error(error_msg)
        debug_error(traceback_msg)
        st.error(error_msg)
        st.code(traceback_msg, language="python")
        return None

def _render_integrated_report(report_data):
    """æ¸²æŸ“å ±å‘Šå…§å®¹èˆ‡æ§åˆ¶é …"""
    if not isinstance(report_data, dict):
        st.error("æ— æ•ˆçš„æŠ¥å‘Šæ•°æ®æ ¼å¼")
        return
    
    # æ¸²æŸ“æ–‡å­—æŠ¥å‘Š
    if "gemini_response" in report_data:
        st.markdown(report_data["gemini_response"])
    else:
        st.warning("æŠ¥å‘Šå†…å®¹ç¼ºå¤±")
    
    # ===================================================================
    # 3. åœ–è¡¨æ¸²æŸ“ (PDFå‹å¥½ç‰ˆæœ¬)
    # ===================================================================
    if "charts_data" in report_data and report_data["charts_data"]:
        st.markdown("---")
        st.markdown("## ğŸ“Š ç›¸é—œåœ–è¡¨")
        
        # åˆå§‹åŒ–æ˜ å°„è¡¨æª¢æŸ¥
        if "chart_mapping" not in st.session_state:
            st.error("åœ–è¡¨æ˜ å°„è¡¨æœªåˆå§‹åŒ–")
            return

        for idx, chart in enumerate(report_data["charts_data"]):
            # æ•¸æ“šæœ‰æ•ˆæ€§é©—è­‰
            if not isinstance(chart, dict) or "id" not in chart:
                debug_error(f"ç„¡æ•ˆåœ–è¡¨æ•¸æ“šæ ¼å¼: {chart}")
                continue
                
            chart_id = chart["id"]
            
            # å¾æ˜ å°„è¡¨ç²å–çœŸå¯¦æ•¸æ“š
            if chart_id not in st.session_state.chart_mapping:
                st.warning(f"åœ–è¡¨ {chart_id} æ•¸æ“šç¼ºå¤±")
                continue
                
            real_url = st.session_state.chart_mapping[chart_id]

            # åœ–è¡¨æ¸²æŸ“ - å–®åˆ—æ¸…æ™°ç‰ˆé¢
            try:
                # æ¨™é¡Œå’Œåœ–è¡¨ç·¨è™Ÿ
                st.subheader(f"åœ–è¡¨ {idx+1}: {chart.get('label', chart_id)}")
                
                # åœ–ç‰‡é¡¯ç¤º - å›ºå®šå¯¬åº¦é©åˆPDF
                st.image(real_url, caption=f"åœ–è¡¨ {chart_id}", use_container_width=False, width=650, output_format="PNG")
                
                # åœ–è¡¨å…ƒæ•¸æ“š - ç°¡æ½”æ ¼å¼
                st.caption(f"åœ–è¡¨ID: {chart_id}")
                
                # åˆ†éš”ç·šç¢ºä¿PDFä¸­çš„åœ–è¡¨é–“è·
                if idx < len(report_data["charts_data"]) - 1:
                    st.markdown("---")
                    
            except Exception as e:
                st.error(f"åœ–è¡¨ {chart_id} æ¸²æŸ“å¤±æ•—: {str(e)}")
                debug_error(f"åœ–è¡¨æ¸²æŸ“éŒ¯èª¤: {str(e)}")
    # PDFä¸‹è½½æŒ‰é’®
    if report_data.get("pdf_buffer"):
        try:
            tmp_path = None
            try:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                    tmp.write(report_data["pdf_buffer"].getvalue())
                    tmp_path = tmp.name
                    
                with open(tmp_path, "rb") as f:
                    st.download_button(
                        label="â¬‡ï¸ ä¸‹è¼‰å®Œæ•´å ±å‘Š (PDF)",
                        data=f,
                        file_name="æ•´åˆåˆ†æå ±å‘Š.pdf",
                        mime="application/pdf",
                        help="åŒ…å«æ–‡å­—åˆ†æèˆ‡å ±å‘Šåœ–è¡¨",
                        key=f"dl_{hash(time.time())}"  # é¿å…æŒ‰éˆ•IDè¡çª
                    )
            finally:
                # ç¢ºä¿è‡¨æ™‚æ–‡ä»¶è¢«æ¸…ç†
                if tmp_path and os.path.exists(tmp_path):
                    try:
                        os.unlink(tmp_path)
                    except Exception as e:
                        debug_error(f"è‡¨æ™‚æ–‡ä»¶æ¸…ç†å¤±æ•—: {str(e)}")
                
        except Exception as e:
            st.error("PDFæ–‡ä»¶ç”Ÿæˆå¤±æ•—ï¼Œè«‹é‡è©¦æˆ–è¯ç¹«ç®¡ç†å“¡")
            debug_error(f"PDFç”Ÿæˆç•°å¸¸: {str(e)}")

def generate_questions():
    """åŸºäºå¯¹è¯å†å²ç”Ÿæˆ3ä¸ªåç»­é—®é¢˜"""
    if not st.session_state.messages:
        return []
    
    # æ ¼å¼åŒ–æœ€è¿‘çš„æ¶ˆæ¯æ­·å²
    recent_messages = st.session_state.messages[-10:]
    formatted_history = ""
    for msg in recent_messages:
        role = msg["role"]
        content = msg["content"]
        
        # è™•ç†å¤šæ¨¡æ…‹æ¶ˆæ¯å…§å®¹
        if isinstance(content, list):
            content = " ".join([item.get("text", "") for item in content if isinstance(item, dict) and "text" in item])
        
        formatted_history += f"{role.upper()}: {content}\n\n"
    
    # æ„å»ºç”Ÿæˆé—®é¢˜çš„prompt
    prompt = f"""
è¯·åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œç”Ÿæˆ3ä¸ªç”¨æˆ·å¯èƒ½ç»§ç»­æå‡ºçš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. é—®é¢˜éœ€ç›´æ¥ç›¸å…³äºå¯¹è¯å†…å®¹
2. ä½¿ç”¨ç¹ä½“ä¸­æ–‡
3. ç”¨æ•°å­—ç¼–å·åˆ—è¡¨æ ¼å¼è¿”å›ï¼Œä¸è¦å…¶ä»–å†…å®¹

å½“å‰å¯¹è¯å†å²ï¼š
{formatted_history}
"""
    
    # ä½¿ç”¨ç°æœ‰æ¨¡å‹ç”Ÿæˆé—®é¢˜
    try:
        # å„ªå…ˆä½¿ç”¨ session ä¸­çš„ API keyï¼Œå¦‚æœæ²’æœ‰å†å¾ç’°å¢ƒè®Šé‡ç²å–
        api_key = st.session_state.get("openai_api_key_input", os.getenv("OPENAI_API_KEY", ""))
        if not api_key:
            debug_error("ç¼ºå°‘ OpenAI API å¯†é‘°ï¼Œç„¡æ³•ç”Ÿæˆå•é¡Œå»ºè­°")
            return []
            
        client = initialize_client(api_key)
        
        # å‰µå»ºæ­£ç¢ºçš„ API è«‹æ±‚æ ¼å¼
        messages = [{"role": "user", "content": prompt}]
        response = get_openai_response(
            client,
            {
                "model": "gpt-4o",
                "temperature": 0.7,
                "max_tokens": 200,
                "messages": messages
            }
        )
        
        # æå–é—®é¢˜åˆ—è¡¨
        questions = []
        for line in response.split('\n'):
            if re.match(r'^\d+[\.ã€]', line.strip()):
                question = re.sub(r'^\d+[\.ã€]\s*', '', line).strip()
                questions.append(question)
                if len(questions) >= 3:
                    break
        valid_questions = [q for q in questions if len(q.strip()) > 0]
        return valid_questions[:3]  # ç¡®ä¿æœ€å¤šè¿”å›3ä¸ªé—®é¢˜
    
    except Exception as e:
        debug_error(f"ç”Ÿæˆå•é¡Œå¤±æ•—: {str(e)}")
        return []

def show_question_suggestions():
    """åœ¨è¾“å…¥æ¡†ä¸Šæ–¹æ˜¾ç¤ºé—®é¢˜å»ºè®®"""
    if "generated_questions" in st.session_state and st.session_state.generated_questions:
        st.markdown("""
        <style>
            .question-box {
                border: 1px solid #e0e0e0;
                border-radius: 8px;
                padding: 12px;
                margin: 8px 0;
                cursor: pointer;
                transition: all 0.2s;
            }
            .question-box:hover {
                background-color: #f5f5f5;
            }
            .source-count {
                color: #666;
                font-size: 0.8em;
                float: right;
            }
        </style>
        """, unsafe_allow_html=True)

        st.markdown("**ğŸ“š æ¨èé—®é¢˜**")
        for i, q in enumerate(st.session_state.generated_questions):
            # æ¨¡æ‹Ÿæ¥æºæ•°é‡ï¼ˆå®é™…å¯æ ¹æ®éœ€æ±‚ä»æ•°æ®è·å–ï¼‰
            source_count = random.randint(1, 5)  
            
            # ä½¿ç”¨columnsåˆ›å»ºå¡ç‰‡å¼å¸ƒå±€
            col1, col2 = st.columns([0.8, 0.2])
            with col1:
                clicked = st.button(
                    q,
                    key=f"sug_q_{i}",
                    use_container_width=True,
                    help="ç‚¹å‡»æäº¤æ­¤é—®é¢˜",
                    type="secondary"
                )
            with col2:
                st.markdown(f'<div class="source-count">{source_count} ä¸ªæ¥æº</div>', unsafe_allow_html=True)
            
            if clicked:
                if "question_input" in st.session_state:
                    del st.session_state.question_input
                
                # ç›´æ¥æ¨¡æ‹Ÿç”¨æˆ·æ¶ˆæ¯
                append_message("user", q)
                debug_log(f"å·²æ¨¡æ‹Ÿç”¨æˆ·æé—®: {q}")
                
                # å¼·åˆ¶åˆ·æ–°å¹¶è·³è½¬åˆ°æ¶ˆæ¯å¤„ç†
                st.session_state.need_process_question = True  # æ–°å¢çŠ¶æ€æ ‡è®°
                st.rerun()
# æ–°å¢ç»Ÿä¸€çš„é—®é¢˜å¤„ç†å‡½æ•°
def process_question():
    """å¤„ç†è‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜"""
    try:
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        last_user_msg = next(
            msg for msg in reversed(st.session_state.messages)
            if msg["role"] == "user"
        )
        
        # èª¿ç”¨æ¨¡å‹ç”Ÿæˆå›å¤
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›ç­”..."):
            # å„ªå…ˆä½¿ç”¨ session ä¸­çš„ API keyï¼Œå¦‚æœæ²’æœ‰å†å¾ç’°å¢ƒè®Šé‡ç²å–
            api_key = st.session_state.get("openai_api_key_input", os.getenv("OPENAI_API_KEY", ""))
            if not api_key:
                st.error("ç¼ºå°‘ OpenAI API å¯†é‘°ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”")
                debug_error("ç¼ºå°‘ OpenAI API å¯†é‘°ï¼Œç„¡æ³•ç”Ÿæˆå›ç­”")
                return
                
            client = initialize_client(api_key)
            
            # å–æœ€è¿‘ 10 æ¢æ¶ˆæ¯ä½œç‚ºä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬æœ€æ–°çš„ç”¨æˆ¶å•é¡Œ
            recent_messages = st.session_state.messages[-10:]
            
            # å‰µå»º API è«‹æ±‚æ‰€éœ€çš„æ¶ˆæ¯æ ¼å¼
            formatted_messages = []
            for msg in recent_messages:
                content = msg["content"]
                # è™•ç†å¤šæ¨¡æ…‹æ¶ˆæ¯
                if isinstance(content, list):
                    # éæ¿¾å‡ºç´”æ–‡æœ¬å…§å®¹
                    text_parts = [item.get("text", "") for item in content if isinstance(item, dict) and "text" in item]
                    content = " ".join(text_parts)
                
                formatted_messages.append({
                    "role": msg["role"],
                    "content": content
                })
            
            # ä½¿ç”¨åˆé©çš„æ¨¡å‹åƒæ•¸
            model_name = st.session_state.get("selected_model", "gpt-4o")
            model_params = {
                "model": model_name,
                "temperature": 0.7,
                "messages": formatted_messages
            }
            
            # èª¿ç”¨ OpenAI API ç²å–å›è¦†
            response = get_openai_response(client, model_params)
            
            # æ·»åŠ åŠ©æ‰‹å›å¤
            append_message("assistant", response)
            
            # æ¸²æŸ“æ¶ˆæ¯
            with st.chat_message("assistant"):
                st.write(response)
                
            # ç”Ÿæˆæ–°çš„å•é¡Œå»ºè­°
            st.session_state.generated_questions = generate_questions()
                
    except StopIteration:
        debug_error("æœªæ‰¾åˆ°ç”¨æˆ·é—®é¢˜æ¶ˆæ¯")
    except Exception as e:
        debug_error(f"é—®é¢˜å¤„ç†å¤±è´¥: {str(e)}")
        st.error(f"å›ç­”ç”Ÿæˆå¤±æ•—: {str(e)}")







# ------------------------------
# ä¸»æ‡‰ç”¨å…¥å£
# ------------------------------

def main():
    st.set_page_config(page_title="Chatbot + Data Analysis", page_icon="ğŸ¤–", layout="wide")
    st.title("ğŸ¤– Chatbot + ğŸ“Š Data Analysis + ğŸ§  Memory + ğŸ–‹ï¸ Canvas (With Debug & Deep Analysis)")

    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "ace_code" not in st.session_state:
        st.session_state.ace_code = ""
    if "editor_location" not in st.session_state:
        st.session_state.editor_location = "Sidebar"
    if "uploaded_file_path" not in st.session_state:
        st.session_state.uploaded_file_path = None
    if "uploaded_image_path" not in st.session_state:
        st.session_state.uploaded_image_path = None
    if "image_base64" not in st.session_state:
        st.session_state.image_base64 = None
    if "debug_mode" not in st.session_state:
        st.session_state.debug_mode = False
    if "deep_analysis_mode" not in st.session_state:
        st.session_state.deep_analysis_mode = False
    if "second_response" not in st.session_state:
        st.session_state.second_response = ""
    if "third_response" not in st.session_state:
        st.session_state.third_response = ""
    if "deep_analysis_image" not in st.session_state:
        st.session_state.deep_analysis_image = None
    if "debug_logs" not in st.session_state:
        st.session_state.debug_logs = []
    if "debug_errors" not in st.session_state:
        st.session_state.debug_errors = []
    if "thinking_protocol" not in st.session_state:
        st.session_state.thinking_protocol = None
    if "gemini_ai_chat" not in st.session_state:
        st.session_state.gemini_ai_chat = None
    if "gemini_ai_history" not in st.session_state: 
        st.session_state.gemini_ai_history = []
    if "generated_questions" not in st.session_state:
        st.session_state.generated_questions = []
    with st.sidebar:
        st.subheader("ğŸ”‘ API Key Settings")
        default_openai_key = os.getenv("OPENAI_API_KEY", "")
        openai_api_key = st.text_input("OpenAI API Key", value=default_openai_key, type="password")
        default_gemini_key = os.getenv("GEMINI_API_KEY", "")
        gemini_api_key = st.text_input("Gemini API Key", 
                                       value=default_gemini_key, 
                                       type="password",
                                       key="gemini_api_key")
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        if gemini_api_key:
            st.session_state["gemini_api_key_input"] = gemini_api_key 

        selected_model = st.selectbox(
            "é¸æ“‡æ¨¡å‹", 
            LLM_MODELS, 
            index=0, 
            key="selected_model"
        )
        
        if "selected_model" in st.session_state:
            current_model = st.session_state.selected_model.lower()
            if "gemini" in current_model:
                gemini_key = os.getenv("GEMINI_API_KEY") or st.session_state.get("gemini_api_key")
                if not gemini_key:
                    st.error("ä½¿ç”¨Geminiæ¨¡å‹éœ€åœ¨ä¸‹æ–¹è¼¸å…¥APIé‡‘é‘° ğŸ”‘")
                    st.stop()
            elif "gpt" in current_model:
                openai_key = os.getenv("OPENAI_API_KEY") or st.session_state.get("openai_api_key")
                if not openai_key:
                    st.error("ä½¿ç”¨OpenAIæ¨¡å‹éœ€åœ¨ä¸‹æ–¹è¼¸å…¥APIé‡‘é‘° ğŸ”‘")
                    st.stop()

        st.session_state.debug_mode = st.checkbox("Debug Mode", value=False)
        st.session_state.deep_analysis_mode = st.checkbox("Deep Analysis Mode", value=True)

        if "memory" not in st.session_state:
            st.session_state.memory = []

        if "conversation_initialized" not in st.session_state:
            openai_api_key = os.getenv("OPENAI_API_KEY") or st.session_state.get("openai_api_key_input")
            if openai_api_key or gemini_api_key:
                client = initialize_client(openai_api_key)
                st.session_state.conversation_initialized = True
                st.session_state.messages = []
                debug_log("Conversation initialized with OpenAI client.")
            else:
                st.warning("â¬…ï¸ è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥OpenAI APIé‡‘é‘°ä»¥åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äºº")

        if st.session_state.debug_mode:
            debug_log(f"Currently using model => {selected_model}")

        if st.button("ğŸ—‘ï¸ Clear Memory"):
            st.session_state.memory = []
            st.session_state.messages = []
            st.session_state.ace_code = ""
            st.session_state.uploaded_file_path = None
            st.session_state.uploaded_image_path = None
            st.session_state.image_base64 = None
            st.session_state.deep_analysis_mode = False
            st.session_state.second_response = ""
            st.session_state.third_response = ""
            st.session_state.deep_analysis_image = None
            st.session_state.debug_logs = []
            st.session_state.debug_errors = []
            st.session_state.thinking_protocol = None
            st.session_state.gemini_ai_chat = None
            st.success("Memory cleared!")
            debug_log("Memory has been cleared.")

        st.subheader("ğŸ§  Memory State")
        if st.session_state.messages:
            memory_content = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.messages])
            st.text_area("Current Memory", value=memory_content, height=200)
            debug_log(f"Current memory content: {memory_content}")
        else:
            st.text_area("Current Memory", value="No messages yet.", height=200)
            debug_log("No messages in memory.")

        st.subheader("ğŸ“‚ Upload a CSV File")
        uploaded_file = st.file_uploader("Choose a CSV file:", type=["csv"])
        csv_data = None
        if uploaded_file:
            st.session_state.uploaded_file_path = save_uploaded_file(uploaded_file)
            debug_log(f"Uploaded file path: {st.session_state.uploaded_file_path}")
            try:
                csv_data = pd.read_csv(st.session_state.uploaded_file_path)
                st.write("### Data Preview")
                st.dataframe(csv_data)
                debug_log(f"CSV Data Columns: {list(csv_data.columns)}")
            except Exception as e:
                csv_columns = "Unable to read columns"
                if st.session_state.debug_mode:
                    st.error(f"Error reading CSV: {e}")
                debug_log(f"Error reading CSV: {e}")
        else:
            csv_columns = "No file uploaded"
            debug_log("No CSV file uploaded.")

        st.subheader("ğŸ–¼ï¸ Upload an Image")
        uploaded_image = st.file_uploader("Choose an image:", type=["png", "jpg", "jpeg"], key="image_uploader")
        if uploaded_image:
            st.session_state.uploaded_image = add_user_image(uploaded_image)
            debug_log(f"Uploaded image path: {st.session_state.uploaded_image}")

            
        st.subheader("å®¢åˆ¶åŒ–æç¤ºè©")  # æ–°å¢å€å¡Š
        if st.button("âœ¨ ç”Ÿæˆç›¸é—œå•é¡Œ", key="generate_questions_btn"):
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ¨è–¦å•é¡Œ..."):
                st.session_state.generated_questions = generate_questions()

        st.subheader("ğŸ§  Upload Thinking Protocol")
        uploaded_thinking_protocol = st.file_uploader("Choose a thinking_protocol.md file:", type=["md"], key="thinking_protocol_uploader")
        if uploaded_thinking_protocol:
            try:
                thinking_protocol_content = uploaded_thinking_protocol.read().decode("utf-8")
                st.session_state.thinking_protocol = thinking_protocol_content
                append_message("user", thinking_protocol_content)
                st.success("Thinking Protocol uploaded successfully!")
                debug_log("Thinking Protocol uploaded and added to messages.")
            except Exception as e:
                if st.session_state.debug_mode:
                    st.error(f"Error reading Thinking Protocol: {e}")
                debug_log(f"Error reading Thinking Protocol: {e}")

        st.subheader("Editor Location")
        location = st.radio(
            "Choose where to display the editor:",
            ["Main", "Sidebar"],
            index=0 if st.session_state.editor_location == "Main" else 1
        )
        st.session_state.editor_location = location
        debug_log(f"Editor location set to: {st.session_state.editor_location}")

        with st.expander("ğŸ› ï¸ è°ƒè¯•ä¸ä¼šè¯ä¿¡æ¯", expanded=False):
            if st.session_state.debug_mode:
                st.subheader("è°ƒè¯•æ—¥å¿—")
                if st.session_state.debug_logs:
                    debug_logs_combined = "\n".join(st.session_state.debug_logs)
                    st.text_area("Debug Logs", value=debug_logs_combined, height=200)
                else:
                    st.write("æ²¡æœ‰è°ƒè¯•æ—¥å¿—ã€‚")
                st.subheader("è°ƒè¯•é”™è¯¯")
                if st.session_state.debug_errors:
                    debug_errors_combined = "\n".join(st.session_state.debug_errors)
                    st.text_area("Debug Errors", value=debug_errors_combined, height=200)
                else:
                    st.write("æ²¡æœ‰è°ƒè¯•é”™è¯¯ã€‚")
            st.subheader("ä¼šè¯ä¿¡æ¯ (messages.json)")
            if "messages" in st.session_state:
                messages_json = json.dumps(st.session_state.messages, ensure_ascii=False, indent=4)
                st.text_area("messages.json", value=messages_json, height=300)
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½ messages.json",
                    data=messages_json,
                    file_name="messages.json",
                    mime="application/json"
                )
                st.markdown("---")
                if st.button("ğŸ“„ æ˜¾ç¤ºåŸå§‹æ¶ˆæ¯"):
                    st.subheader("ğŸ” åŸå§‹æ¶ˆæ¯å†…å®¹")
                    st.json(st.session_state.messages)
            else:
                st.write("æ²¡æœ‰æ‰¾åˆ° messagesã€‚")

    for idx, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            if isinstance(message["content"], list):
                for item in message["content"]:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        image_url = item["image_url"]["url"]
                        st.image(image_url, caption="ğŸ“· ä¸Šå‚³çš„åœ–ç‰‡", use_container_width=True)
                        debug_log(f"Displaying image from {message['role']}: {image_url}")
                    else:
                        st.write(item)
                        debug_log(f"Displaying non-image content from {message['role']}: {item}")
            elif isinstance(message["content"], str) and "```python" in message["content"]:
                code_match = re.search(r'```python\s*(.*?)\s*```', message["content"], re.DOTALL)
                if code_match:
                    code = code_match.group(1).strip()
                    st.code(code, language="python")
                    debug_log(f"Displaying code from {message['role']}: {code}")
                else:
                    st.write(message["content"])
                    debug_log(f"Displaying message {idx} from {message['role']}: {message['content']}")
    if st.session_state.get("need_process_question", False):
        st.session_state.need_process_question = False
        process_question()
    user_input = st.chat_input("Hi! Ask me anything...", key="main_input")
    show_question_suggestions()  
    if user_input:
        append_message("user", user_input)
        with st.chat_message("user"):
            st.write(user_input)
            debug_log(f"User input added to messages: {user_input}")

        with st.spinner("Thinking..."):
            try:
                if openai_api_key or gemini_api_key:
                    client = initialize_client(openai_api_key)
                else:
                    raise ValueError("OpenAI API Key is not provided.")
                debug_log(f"Uploaded file path: {st.session_state.uploaded_file_path}")
                debug_log(f"Uploaded image path: {st.session_state.uploaded_image_path}")

                if not any(msg["role"] == "system" for msg in st.session_state.messages):
                    system_prompt = "You are an assistant that helps with data analysis."
                    append_message("system", system_prompt)
                    debug_log("System prompt added to messages.")

                if "question_input" in st.session_state:
                    user_input = st.session_state.pop("question_input")
                    if user_input:
                        append_message("user", user_input)

                if st.session_state.uploaded_image_path is not None and st.session_state.image_base64:
                    prompt = user_input
                    debug_log("User input with image data already appended.")
                else:
                    if st.session_state.uploaded_file_path is not None:
                        try:
                            df_temp = pd.read_csv(st.session_state.uploaded_file_path)
                            csv_columns = ", ".join(df_temp.columns)
                            debug_log(f"CSV columns: {csv_columns}")
                        except Exception as e:
                            csv_columns = "Unable to read columns"
                            if st.session_state.debug_mode:
                                st.error(f"Error reading CSV: {e}")
                            debug_log(f"Error reading CSV: {e}")
                    else:
                        csv_columns = "No file uploaded"
                        debug_log("No CSV file uploaded.")

                    if st.session_state.uploaded_file_path is not None and csv_columns != "No file uploaded":
                        prompt = f"""Please respond with a JSON object in the format:
{{
    "content": "é€™æ˜¯æˆ‘çš„è§€å¯Ÿè·Ÿåˆ†æ: {{analysis}}",
    "code": "import pandas as pd\\nimport streamlit as st\\nimport matplotlib.pyplot as plt\\n# Read CSV file (use st.session_state.uploaded_file_path variable)\\ndata = pd.read_csv(st.session_state.uploaded_file_path)\\n\\n# Add your plotting or analysis logic here\\n\\n# For example, to display a plot using st.pyplot():\\n# fig, ax = plt.subplots()\\n# ax.scatter(data['colA'], data['colB'])\\n# st.pyplot(fig)"
}}
Important:
1) å¿…é ˆä½¿ç”¨ st.session_state.uploaded_file_path ä½œç‚º CSV è·¯å¾‘ (instead of a hardcoded path)
2) Must use st.pyplot() to display any matplotlib figure
3) Return only valid JSON (escape any special characters if needed)
4) è«‹ç¢ºä¿åœ–è¡¨ä¸­çš„å­—é«”å·²ç¶“å¥—ç”¨ä»¥ä¸‹å­—å‹ï¼šå­—å‹ä½ç½®ï¼š{font_path}ã€‚è«‹æ³¨æ„ï¼Œé€™æ˜¯å¿…è¦çš„æ­¥é©Ÿï¼Œç¢ºä¿æ‰€æœ‰æ¨™é¡Œã€æ¨™ç±¤ç­‰æ–‡å­—éƒ½ä»¥æŒ‡å®šå­—å‹é¡¯ç¤ºã€‚
Based on the request: {user_input}.
Available columns: {csv_columns}.

!é‡è¦!éœ€æ±‚å…±æœ‰3
1.åœ–è¡¨çš„é¡è‰²è€ƒæ…®ä½¿ç”¨å…¶ä»–çš„ï¼Œä¸è¦ä½¿ç”¨é è¨­
2.åœ¨ç”Ÿæˆä»£ç¢¼æ™‚éœ€è¦è€ƒæ…®plotçš„ç¾è§€æ€§
3.ç„¶å¾Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰
"""
                        debug_log("Prompt constructed for CSV input with JSON response.")
                        append_message("system", prompt)
                        debug_log("System prompt appended to messages.")
                    else:
                        prompt = f"Please answer this question entirely in Traditional Chinese: {user_input}"
                        debug_log("Prompt constructed for plain text input.")
                        append_message("system", prompt)
                        debug_log("Plain text system prompt appended to messages.")

                model_params = {
                    "model": selected_model,
                    "temperature": 0.5,
                    "max_tokens": 16384
                }
                response_content = get_llm_response(client, model_params)
                debug_log(f"Full assistant response: {response_content}")

                if response_content:
                    append_message("assistant", response_content)
                    with st.chat_message("assistant"):
                        st.write(response_content)
                        debug_log(f"Assistant response added to messages: {response_content}")

                    json_str = extract_json_block(response_content)
                    try:
                        response_json = json.loads(json_str)
                        debug_log("JSON parsing successful.")
                    except Exception as e:
                        debug_log(f"json.loads parsing error: {e}")
                        debug_error(f"json.loads parsing error: {e}")
                        response_json = {"content": json_str, "code": ""}
                        debug_log("Fallback to raw response for content.")

                    content = response_json.get("content", "Here is my analysis:")
                    append_message("assistant", content)
                    code = response_json.get("code", "")
                    if code:
                        code_block = f"```python\n{code}\n```"
                        append_message("assistant", code_block)
                        with st.chat_message("assistant"):
                            st.code(code, language="python")
                        st.session_state.ace_code = code
                        debug_log("ace_code updated with new code.")

                    if st.session_state.deep_analysis_mode and code:
                        st.write("### [Deep Analysis] Automatically executing the generated code and sending the chart to GPT-4o for analysis...")
                        debug_log("Deep analysis mode activated.")
                        global_vars = {
                            "uploaded_file_path": st.session_state.uploaded_file_path,
                            "uploaded_image_path": st.session_state.uploaded_image_path,
                        }
                        exec_result = execute_code(st.session_state.ace_code, global_vars=global_vars)
                        st.write("#### Execution Result")
                        st.text(exec_result)
                        debug_log(f"Execution result: {exec_result}")
                        fig = plt.gcf()
                        buf = BytesIO()
                        fig.savefig(buf, format="png")
                        buf.seek(0)
                        chart_base64 = base64.b64encode(buf.read()).decode("utf-8")
                        st.session_state.deep_analysis_image = chart_base64
                        debug_log("Chart has been converted to base64.")
                        prompt_2 = f"""åŸºæ–¼åœ–ç‰‡çµ¦æˆ‘æ›´å¤šè³‡è¨Š"""
                        debug_log(f"Deep Analysis Prompt: {prompt_2}")
                        append_message("user", prompt_2)
                        debug_log("Deep analysis prompt appended to messages.")
                        image_content = [{
                            "type": "image_url",
                            "image_url": {"url": f"data:image/png;base64,{chart_base64}"}
                        }]
                        append_message("user", image_content)
                        second_raw_response = get_llm_response(client, model_params)
                        debug_log(f"Deep analysis response: {second_raw_response}")
                        if second_raw_response:
                            append_message("assistant", second_raw_response)
                            st.session_state.second_response = second_raw_response
                            with st.chat_message("assistant"):
                                st.write(second_raw_response)
                                debug_log(f"Deep analysis response added to messages: {second_raw_response}")
                            prompt_3 = f"""
First response content: {content}
Second response chart analysis content: {second_raw_response}

è«‹æŠŠå‰å…©æ¬¡çš„åˆ†æå…§å®¹åšåˆ†æç¸½çµï¼Œæœ‰æ•¸æ“šçš„è©±å°±é¡¯ç¤ºå¾—æ¼‚äº®ä¸€é»ï¼Œä¸»è¦æ˜¯éœ€è¦è®“ä½¿ç”¨è€…æ„Ÿåˆ°å¾ˆå²å®³ã€‚ä¸¦ä¸”ä»¥ç¹é«”ä¸­æ–‡ä½œç‚ºå›ç­”ç”¨çš„èªè¨€ã€‚
å¦å¤–éœ€è¦è§£é‡‹å‚³çµ¦å¦³çš„åœ–è¡¨ï¼Œä»¥ä¸€å€‹æ²’æœ‰è³‡æ–™ç§‘å­¸èƒŒæ™¯çš„å°ç™½è§£é‡‹æˆ‘æ‰€å‚³çš„åœ–è¡¨ã€‚é‚„æœ‰æ ¹æ“šç¬¬äºŒæ¬¡çš„åœ–è¡¨åˆ†æå¾—å‡ºçš„çµè«–ï¼Œç›´æ¥é æ¸¬ä¹‹å¾Œçš„èµ°å‘ï¼Œä¾‹å¦‚:"ä¹‹å¾Œé€™å€‹æ•¸å€¼çš„èµ°å‘æœƒå‘ˆç¾å‘ä¸Šçš„è¶¨å‹¢"ç­‰...
ä¸è¦è·Ÿä½¿ç”¨è€…èªªç”šéº¼å¦³å¯ä»¥ä½¿ç”¨RFMåˆ†æï¼Œäº¤å‰åˆ†æä¹‹é¡çš„æ–¹æ³•ã€‚æˆ‘éœ€è¦å¦³ç›´æ¥é æ¸¬ä¹‹å¾Œçš„èµ°å‘ï¼Œæ¯”å¦‚å¾€ä¸Šé‚„æ˜¯å¾€ä¸‹ã€‚
"""
                            debug_log(f"Final Summary Prompt: {prompt_3}")
                            append_message("user", prompt_3)
                            debug_log("Final summary prompt appended to messages.")
                            third_raw_response = get_llm_response(client, model_params)
                            debug_log(f"Final summary response: {third_raw_response}")
                            if third_raw_response:
                                append_message("assistant", third_raw_response)
                                st.session_state.third_response = third_raw_response
                                with st.chat_message("assistant"):
                                    st.write(third_raw_response)
                                    debug_log(f"Final summary response added to messages: {third_raw_response}")
                                st.write("#### [Deep Analysis] Chart:")
                                try:
                                    img_data = base64.b64decode(st.session_state.deep_analysis_image)
                                    st.image(img_data, caption="Chart generated from deep analysis", use_container_width=True)
                                    debug_log("Deep analysis chart displayed.")
                                except Exception as e:
                                    if st.session_state.debug_mode:
                                        st.error(f"Error displaying chart: {e}")
                                    debug_log(f"Error displaying chart: {e}")
            except Exception as e:
                if st.session_state.debug_mode:
                    st.error(f"An error occurred: {e}")
                debug_log(f"An error occurred: {e}")

    # æ–°å¢ï¼šå¤šæ¨¡å‹äº¤å‰é©—è­‰æŒ‰éˆ•
    if st.button("å¤šæ¨¡å‹äº¤å‰é©—è­‰"):
        if openai_api_key:
            client = initialize_client(openai_api_key)
        else:
            st.error("OpenAI API Key is required for cross validation.")
            st.stop()
    
        # è¨­å®šå…©å€‹æ¨¡å‹çš„åƒæ•¸ï¼ˆå¯æ ¹æ“šéœ€è¦èª¿æ•´ï¼‰
        model_params_openai = {
            "model": "gpt-4o",
            "temperature": 0.5,
            "max_tokens": 4096
        }
        model_params_gemini = {
            "model": "models/gemini-2.0-flash",
            "temperature": 0.5,
            "max_tokens": 4096
        }
        with st.spinner("æ­£åœ¨åŸ·è¡Œå¤šæ¨¡å‹äº¤å‰é©—è­‰..."):
            cross_validated_response = get_cross_validated_response(model_params_gemini)
            # å°‡äº¤å‰é©—è­‰çµæœæ·»åŠ åˆ°è¨˜æ†¶æµä¸­ï¼Œé€™æ¨£æ•´åˆå ±å‘Šå¯ä»¥ä½¿ç”¨é€™äº›çµæœ
            simulate_system_message_addition(cross_validated_response, "äº¤å‰é©—è­‰å ±å‘Š")
            
            # é¡¯ç¤ºäº¤å‰é©—è­‰çµæœ
            with st.expander("ğŸ” äº¤å‰é©—è­‰çµæœ", expanded=True):
                st.markdown("### Gemini äº¤å‰é©—è­‰")
                st.markdown(cross_validated_response["gemini_response"])
        
        # é¡¯ç¤ºæ•´åˆå ±å‘Šï¼ˆåœ–è¡¨å’ŒPDFä¸‹è¼‰æŒ‰éˆ•ç”±_render_integrated_reportå‡½æ•¸è™•ç†ï¼‰
        with st.spinner("æ­£åœ¨ç”Ÿæˆæ•´åˆå ±å‘Š..."):
            st.markdown("## ğŸ“Š æ•´åˆåˆ†æå ±å‘Š")
            generate_integrated_report(model_params_gemini)
                    
    if "ace_code" not in st.session_state:
        st.session_state.ace_code = ""

    if st.session_state.editor_location == "Main":
        with st.expander("ğŸ–‹ï¸ Persistent Code Editor (Main)", expanded=False):
            edited_code = st_ace(
                value=st.session_state.ace_code,
                language="python",
                theme="monokai",
                height=300,
                key="persistent_editor_main"
            )
            if edited_code != st.session_state.ace_code:
                st.session_state.ace_code = edited_code
                debug_log("ace_code updated from main editor.")
            if st.button("â–¶ï¸ Execute Code", key="execute_code_main"):
                global_vars = {
                    "uploaded_file_path": st.session_state.uploaded_file_path,
                    "uploaded_image_path": st.session_state.uploaded_image_path,
                }
                debug_log(f"Executing code with uploaded_file_path = {st.session_state.uploaded_file_path}")
                debug_log(f"Executing code with uploaded_image_path = {st.session_state.uploaded_image_path}")
                result = execute_code(st.session_state.ace_code, global_vars=global_vars)
                st.write("### Execution Result")
                st.text(result)
                debug_log(f"Code execution result: {result}")
            
    else:
        with st.sidebar.expander("ğŸ–‹ï¸ Persistent Code Editor (Sidebar)", expanded=False):
            edited_code = st_ace(
                value=st.session_state.ace_code,
                language="python",
                theme="monokai",
                height=300,
                key="persistent_editor_sidebar"
            )
            if edited_code != st.session_state.ace_code:
                st.session_state.ace_code = edited_code
                debug_log("ace_code updated from sidebar editor.")
            if st.button("â–¶ï¸ Execute Code", key="execute_code_sidebar"):
                global_vars = {
                    "uploaded_file_path": st.session_state.uploaded_file_path,
                    "uploaded_image_path": st.session_state.uploaded_image_path,
                }
                debug_log(f"Executing code with uploaded_file_path = {st.session_state.uploaded_file_path}")
                debug_log(f"Executing code with uploaded_image_path = {st.session_state.uploaded_image_path}")
                result = execute_code(st.session_state.ace_code, global_vars=global_vars)
                st.write("### Execution Result")
                st.text(result)
                debug_log(f"Code execution result: {result}")

if __name__ == "__main__":
    main()
