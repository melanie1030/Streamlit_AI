# åŒ¯å…¥ Streamlit å¥—ä»¶ï¼Œç”¨æ–¼å»ºç«‹ç¶²é æ‡‰ç”¨ä»‹é¢
import streamlit as st

# --- é é¢ç‹€æ…‹åˆå§‹åŒ– ---
if "active_page" not in st.session_state:
    st.session_state.active_page = "main"  # é è¨­ä¸»é ï¼Œå¯ä¾éœ€æ±‚èª¿æ•´

# --- é‹ç‡Ÿè¨ºæ–·å ±å‘Š AI ç”Ÿæˆå‡½æ•¸ï¼ˆå…¨åŸŸå¯ç”¨ï¼‰---
def generate_operation_diagnosis(data_summary, risk_summary):
    """ç”Ÿæˆé‹ç‡Ÿè¨ºæ–·å ±å‘Š"""
    prompt = f"""ä½œç‚ºä¸€ä½è·¨é ˜åŸŸçš„ç­–ç•¥é¡§å•ï¼Œè«‹æ ¹æ“šä¸‹åˆ— CSV æ•¸æ“šåˆ†æçµæœï¼Œæ’°å¯«ä¸€ä»½**å…¨é¢çš„è¨ºæ–·å ±å‘Š**ï¼Œå…§å®¹é ˆçµåˆé‡åŒ–æŒ‡æ¨™èˆ‡è³ªåŒ–æ´å¯Ÿï¼Œä¸¦ä¸é™æ–¼æ¬„ä½å±¤é¢çš„æè¿°ï¼š

æ•¸æ“šæ¦‚æ³ï¼š
{data_summary}

é¢¨éšªåˆ†æï¼š
{risk_summary}

è«‹å¾ä»¥ä¸‹é¢å‘é€²è¡Œæ·±å…¥åˆ†æï¼š
1. ç‡Ÿé‹èˆ‡è²¡å‹™ç¾æ³
2. å¸‚å ´èˆ‡å®¢æˆ¶æ´å¯Ÿ
3. æŠ€è¡“èˆ‡ç³»çµ±ç¾æ³
4. ä¾›æ‡‰éˆèˆ‡æµç¨‹æ•ˆç‡
5. äººæ‰èˆ‡çµ„ç¹”è³‡æº
6. ä¸»è¦é¢¨éšªèˆ‡ç·©è§£ç­–ç•¥
7. å„ªåŒ–å»ºè­°ï¼ˆçŸ­ / ä¸­ / é•·æœŸè¡Œå‹•è¨ˆç•«ï¼‰
8. æœªä¾†ç™¼å±•æ–¹å‘èˆ‡æ©Ÿæœƒ

è«‹ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¿æŒå°ˆæ¥­ã€å®¢è§€çš„èªæ°£ã€‚"""
    try:
        response = client.chat.completions.create(
            model="gpt-4-0125-preview",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ“…é•·æ•´åˆå•†æ¥­ã€æŠ€è¡“èˆ‡ç®¡ç†è§€é»çš„ç­–ç•¥é¡§å•ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ç”Ÿæˆè¨ºæ–·å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

# ================= é€šç”¨è³‡æ–™æ‘˜è¦èˆ‡é¢¨éšªè¨ˆç®—å‡½å¼ ================= #

def summarize_dataframe_generic(df, max_columns: int = 5):
    """æ ¹æ“šä»»ä½•é¡å‹çš„ DataFrame ç”¢ç”Ÿé€šç”¨æ•¸æ“šæ¦‚æ³èˆ‡é¢¨éšªæŒ‡æ¨™ã€‚
    å›å‚³ (data_summary:str, risk_metrics:dict)"""
    import numpy as np
    lines = []
    risk = {}

    # åŸºæœ¬çµæ§‹
    lines.append(f"1. è³‡æ–™ç­†æ•¸ï¼š{len(df)}")
    lines.append(f"2. æ¬„ä½æ•¸ï¼š{df.shape[1]}")
    col_list_preview = ", ".join(df.columns[:max_columns]) + (" ..." if df.shape[1] > max_columns else "")
    lines.append(f"3. æ¬„ä½é è¦½ï¼š{col_list_preview}")

    # ç¼ºå¤±å€¼
    missing_ratio = df.isna().mean().mean()
    risk["æ•´é«”ç¼ºå¤±ç‡"] = f"{missing_ratio:.2%}"
    lines.append(f"4. æ•´é«”ç¼ºå¤±ç‡ï¼š{missing_ratio:.2%}")

    # æ•¸å€¼æ¬„çµ±è¨ˆ
    num_cols = df.select_dtypes(include=[np.number]).columns
    if len(num_cols) > 0:
        for col in num_cols[:max_columns]:
            col_series = df[col].dropna()
            if col_series.empty:
                continue
            lines.append(f"- æ•¸å€¼æ¬„ã€Š{col}ã€‹ï¼šå¹³å‡ {col_series.mean():.2f}ï¼Œæœ€å° {col_series.min():.2f}ï¼Œæœ€å¤§ {col_series.max():.2f}")
    else:
        lines.append("- ç„¡æ•¸å€¼æ¬„ä½")

    # åˆ†é¡æ¬„é è¦½
    cat_cols = df.select_dtypes(exclude=[np.number, 'datetime']).columns
    if len(cat_cols) > 0:
        for col in cat_cols[:max_columns]:
            top_vals = df[col].astype(str).value_counts().nlargest(3)
            preview = ", ".join([f"{idx}({cnt})" for idx, cnt in top_vals.items()])
            lines.append(f"- åˆ†é¡æ¬„ã€Š{col}ã€‹ï¼šTop3 â†’ {preview}")
    else:
        lines.append("- ç„¡åˆ†é¡æ¬„ä½")

    # å…¶ä»–é¢¨éšªæŒ‡æ¨™
    dup_ratio = df.duplicated().mean()
    risk["é‡è¤‡åˆ—æ¯”ä¾‹"] = f"{dup_ratio:.2%}"
    if len(num_cols) > 0:
        overall_std = df[num_cols].std().mean()
        risk["æ•¸å€¼æ¬„å¹³å‡æ¨™æº–å·®"] = f"{overall_std:.2f}"

    data_summary = "\n".join(lines)
    return data_summary, risk

# è¨­å®š Streamlit é é¢å±¬æ€§ï¼ˆé€™æ®µå¿…é ˆåœ¨ç¬¬ä¸€è¡Œï¼‰
st.set_page_config(
    page_title="éŠ·è²¨æç›Šåˆ†æå°å¹«æ‰‹",  # é é¢æ¨™é¡Œ
    page_icon="ğŸª",               # æ¨™é¡Œæ—çš„å°åœ–ç¤ºï¼ˆé€™è£¡æ˜¯ä¸€å®¶åº—ï¼‰
    layout="wide",               # ç¶²é ä½ˆå±€ç‚ºå¯¬ç‰ˆ
    initial_sidebar_state="expanded"  # å´é‚Šæ¬„é è¨­ç‚ºå±•é–‹
)

# --- å´é‚Šæ¬„ä½¿ç”¨èªªæ˜ ---
with st.sidebar:
    st.header("ä½¿ç”¨èªªæ˜")
    st.markdown("""
    **å®Œæ•´æ“ä½œæŒ‡å—**  
    1. **ä¸Šå‚³ä¸»è³‡æ–™**ï¼šé»æ“Šã€Œé¸æ“‡æª”æ¡ˆã€ä¸Šå‚³éŠ·å”®è³‡æ–™ `CSV`ï¼Œç³»çµ±å°‡å³æ™‚è§£æä¸¦å‘ˆç¾æ‘˜è¦ã€‚  
       **ä¸Šå‚³æ¬„ä½è³‡æ–™**ï¼š åŒæ¨£æ–¹å¼ä¸Šå‚³æ¬„ä½èªªæ˜ CSV/TXTï¼Œæé«˜æ¬„ä½è§£é‡‹ç²¾åº¦ã€‚ 
    2. **è³‡æ–™å“è³ªå„€è¡¨æ¿**ï¼šé¡¯ç¤ºæ•´é«”æŒ‡æ¨™ã€æ¬„ä½å“è³ªè©•ä¼°ã€æ•¸å€¼/åˆ†é¡åˆ†ä½ˆèˆ‡ç›¸é—œä¿‚æ•¸ç†±åœ–ã€‚ 
    3. **è³‡æ–™æ¢ç´¢å™¨**ï¼šæŸ¥çœ‹å®Œæ•´è³‡æ–™æ‘˜è¦ã€‚
    4. **CDO åˆæ­¥å ±å‘Š**ï¼šåœ¨ã€ŒCDO å ±å‘Šã€åˆ†é é–±è®€ AI å°è³‡æ–™å“è³ªèˆ‡ç•°å¸¸çš„åˆ†ææ‘˜è¦ã€‚
    5. **AI åˆ†æå°è©±**ï¼šåœ¨ã€ŒAIåˆ†æå°è©±ã€åˆ†é èˆ‡ AI å°è©±ï¼Œå¯å³æ™‚åŸ·è¡Œä¸¦é¡¯ç¤ºè¡¨æ ¼æˆ–åœ–è¡¨ã€‚ 
    6. **PygWalker äº’å‹•å¼æ¢ç´¢**ï¼šæ‹–æ”¾æ¬„ä½å³å¯ç”Ÿæˆåœ–è¡¨ï¼Œä¸¦å¯é€é AI å•ç­”é€²è¡Œåˆ†æã€‚
    7. **é‹ç‡Ÿè¨ºæ–·å ±å‘Š**ï¼šè‡ªå‹•ç”Ÿæˆã€Œé‹ç‡Ÿè¨ºæ–·å ±å‘Šã€ä¸¦æä¾›ä½¿ç”¨è€…ä¸‹è¼‰ï¼ŒåŸºæ–¼åˆ†æçµæœæä¾›å®Œæ•´ç¶“ç‡Ÿè¨ºæ–·èˆ‡å»ºè­°ã€‚  
    8. **è«®è©¢æœå‹™å›é¥‹**ï¼šä½¿ç”¨è€…èƒ½ç•™ä¸‹éœ€æ±‚åŠéœ€è¦æ”¹é€²çš„åœ°æ–¹ï¼ŒAI æœƒå³æ™‚å›è¦†çµ¦æ‚¨ï¼Œä¸¦å›é¥‹çµ¦é‹ç‡Ÿåœ˜éšŠï¼Œæ­¡è¿éš¨æ™‚æä¾›æ‚¨çš„éœ€æ±‚ã€‚ 
    """)
    st.markdown("---")

# ä»¥ä¸‹æ˜¯ä¸»ç¨‹å¼æ¨¡çµ„

# åŒ¯å…¥å¿…è¦å¥—ä»¶
import streamlit as st              # Streamlitï¼šç¶²é æ‡‰ç”¨æ¡†æ¶
import pandas as pd                # pandasï¼šè³‡æ–™è™•ç†
import os                          # osï¼šæª”æ¡ˆæ“ä½œ
import io                          # ioï¼šç”¨æ–¼å°‡ CSV ç•¶å­—ä¸²è™•ç†
import json                        # jsonï¼šJSON æ ¼å¼æ“ä½œï¼ˆç”¨æ–¼è³‡æ–™æ‘˜è¦ï¼‰
import datetime                    # datetimeï¼šå–å¾—ç›®å‰æ™‚é–“ï¼ˆç”¨æ–¼æª”æ¡ˆå‘½åï¼‰
import matplotlib.pyplot as plt    # matplotlibï¼šç¹ªåœ–ç”¨
import matplotlib                   # æ–°å¢ï¼Œä¾›å‹•æ…‹åŸ·è¡Œç’°å¢ƒä½¿ç”¨
import seaborn as sns              # seabornï¼šç¹ªåœ–ç”¨ï¼ˆèˆ‡ matplotlib çµåˆï¼‰
import seaborn                      # æ–°å¢ï¼Œæä¾› seaborn æ¨¡çµ„çµ¦å‹•æ…‹åŸ·è¡Œç’°å¢ƒ
import numpy as np                 # numpyï¼šæ•¸å­¸é‹ç®—

# è¨­å®š matplotlib ä½¿ç”¨çš„å­—é«”ç‚ºå¾®è»Ÿæ­£é»‘é«”ï¼Œé¿å…ä¸­æ–‡å­—äº‚ç¢¼
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']
plt.rcParams['axes.unicode_minus'] = False  # å…è¨±åº§æ¨™è»¸é¡¯ç¤ºè² è™Ÿ

# åŒ¯å…¥ html å¥—ä»¶ï¼Œè™•ç† HTML å…§å®¹è·³è„«å­—å…ƒ
import html
import tempfile
import shutil
# --- ä½¿ç”¨ Plotly å¥—ä»¶ä¾†ç”¢ç”Ÿäº’å‹•å¼åœ–è¡¨ ---
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# --- PDF è¼¸å‡ºç›¸é—œå¥—ä»¶ ---
# å®‰è£æ–¹æ³•ï¼špip install reportlab xhtml2pdf
from reportlab.platypus import (
    SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak, Table, TableStyle
)
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle  # é è¨­æ¨£å¼è¡¨
from reportlab.lib.units import inch                  # å®šç¾©å–®ä½
from reportlab.lib import colors                      # é¡è‰²æ¨£å¼
from reportlab.pdfbase import pdfmetrics              # å­—é«”è¨»å†Š
from reportlab.pdfbase.ttfonts import TTFont          # TrueType å­—é«”
import reportlab.rl_config
reportlab.rl_config.warnOnMissingFontGlyphs = 0       # é—œé–‰å­—é«”ç¼ºå¤±è­¦å‘Š

# --- LangChain/LLM äººå·¥æ™ºæ…§ç›¸é—œçµ„ä»¶ï¼ˆç”¨æ–¼æ–‡å­—ç”Ÿæˆåˆ†æï¼‰---
from langchain_google_genai import ChatGoogleGenerativeAI  # ä½¿ç”¨ Google çš„ç”Ÿæˆå¼ AI æ¨¡å‹
from langchain_core.prompts import PromptTemplate           # ç”¨æ–¼å»ºç«‹æç¤ºè©
from langchain.memory import ConversationBufferMemory       # å»ºç«‹å°è©±è¨˜æ†¶é«”


from openai import OpenAI
client = OpenAI(api_key=st.session_state.get("openai_api_key", ""))


# è¨­ç½® API é‡‘é‘°
LLM_API_KEY = st.session_state.get("google_api_key") 

# éƒµä»¶è¨­ç½®
EMAIL_SENDER = "skeswinnie@gmail.com"
EMAIL_PASSWORD = "dkyu hpmy tpai rjwf"

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei']
plt.rcParams['axes.unicode_minus'] = False

# è¨­å®šå…¨åŸŸ CSS æ¨£å¼ï¼Œä½¿ç”¨ markdown èˆ‡ HTML çš„æ–¹å¼è¼‰å…¥æ¨£å¼è¨­å®š
st.markdown("""
    <style>
    /* èƒŒæ™¯å‹•ç•«è¨­å®š */
    /* ä¸»è¦å…§å®¹å€åŸŸèƒŒæ™¯ */
    .stApp {
        background: linear-gradient(-45deg, #FFD1DC, #E0FFE0, #D1E8FF, #E6E6FA); /* ç²‰è‰²ç³»æ¼¸å±¤èƒŒæ™¯ */
        background-size: 400% 400%;
        animation: gradient 15s ease infinite;  /* èƒŒæ™¯å‹•ç•«ï¼š15ç§’å¾ªç’° */
        padding: 2rem;
    }

    /* å®šç¾©å‹•ç•«çš„é—œéµå½±æ ¼ */
    @keyframes gradient {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }

    
    /* å´é‚Šæ¬„æ¨£å¼ */
    section[data-testid="stSidebar"] {
        background: linear-gradient(135deg, rgba(255, 209, 220, 0.95), rgba(230, 230, 250, 0.3));
        border-right: 1px solid rgba(209, 232, 255, 0.6);
        box-shadow: 2px 0 20px rgba(0,0,0,0.1);
        transition: all 0.3s ease;
    }

    section[data-testid="stSidebar"]:hover {
        box-shadow: 2px 0 20px rgba(135, 206, 235, 0.2);
    }

    /* å´é‚Šæ¬„å…§éƒ¨å…ƒç´  */
    section[data-testid="stSidebar"] .stMarkdown {
        background-color: transparent;
        transition: all 0.3s ease;
    }
    
    /* æ¨™é¡Œæ¨£å¼ */
    h1, h2, h3, h4, h5, h6 {
        color: #2E4053;
        font-weight: 700;
        padding: 0.5rem;
        margin-bottom: 1rem;
        background: linear-gradient(120deg, rgba(255, 255, 255, 0.9), rgba(182, 251, 255, 0.3));
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        transition: all 0.3s ease;
    }

    h1:hover, h2:hover, h3:hover, h4:hover, h5:hover, h6:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 15px rgba(135, 206, 235, 0.3);
        background: linear-gradient(120deg, rgba(255, 255, 255, 0.95), rgba(131, 164, 212, 0.2));
    }

    /* æŒ‰éˆ•æ¨£å¼ */
    .stButton > button {
        background: linear-gradient(120deg, #87CEEB, #ADD8E6);
        color: #2E4053;
        border: none;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: 500;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        transition: all 0.3s ease;
        position: relative;
        overflow: hidden;
    }

    .stButton > button:hover {
        transform: translateY(-2px) scale(1.02);
        box-shadow: 0 4px 15px rgba(135, 206, 235, 0.4);
        background: linear-gradient(120deg, #ADD8E6, #87CEEB);
    }

    .stButton > button:active {
        transform: translateY(1px);
    }

    /* æ·»åŠ å½©è™¹é‚Šæ¡†æ•ˆæœ */
    @keyframes borderGlow {
        0% {
            border-color: #87CEEB;
        }
        25% {
            border-color: #ADD8E6;
        }
        50% {
            border-color: #B6FBFF;
        }
        75% {
            border-color: #FFD700;
        }
        100% {
            border-color: #87CEEB;
        }
    }

    /* å…¶ä»–æ¨£å¼ä¿æŒä¸è®Š */
    /* æ•¸æ“šæ¡†æ¨£å¼ */
    .dataframe {
        background: rgba(255, 255, 255, 0.9);
        border: none !important;
        border-radius: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        margin: 1rem 0;
        transition: all 0.3s ease;
    }

    .dataframe:hover {
        transform: translateY(-3px);
        box-shadow: 0 5px 15px rgba(255, 154, 139, 0.3);
    }

    .dataframe th {
        background-color: #faf9f6;
        color: #333333;
        font-weight: 600;
        padding: 12px !important;
        transition: all 0.3s ease;
    }

    .dataframe td {
        color: #333333;
        border: none !important;
        border-bottom: 1px solid rgba(182, 251, 255, 0.3) !important;
        padding: 12px !important;
        transition: all 0.2s ease;
    }

    .dataframe tr:hover {
        background-color: rgba(131, 164, 212, 0.1);
        transform: scale(1.01);
    }

    /* åœ–è¡¨å®¹å™¨ */
    .stPlot {
        background: linear-gradient(135deg, rgba(255, 255, 255, 0.9), rgba(131, 164, 212, 0.1));
        border-radius: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        padding: 1rem;
        margin: 1rem 0;
        transition: all 0.3s ease;
    }

    .stPlot:hover {
        transform: translateY(-3px) scale(1.01);
        box-shadow: 0 5px 15px rgba(255, 154, 139, 0.3);
    }

    /* é¸æ“‡å™¨æ¨£å¼ */
    .stSelectbox {
        background: linear-gradient(135deg, rgba(255, 255, 255, 0.9), rgba(255, 154, 139, 0.1));
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        transition: all 0.3s ease;
    }

    .stSelectbox:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 15px rgba(255, 106, 136, 0.2);
    }

    /* æŒ‡æ¨™å¡ç‰‡æ¨£å¼ */
    div[data-testid="stMetricValue"] {
        background: linear-gradient(120deg, rgba(255, 255, 255, 0.9), rgba(182, 251, 255, 0.2));
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        margin: 0.5rem 0;
        transition: all 0.3s ease;
    }

    div[data-testid="stMetricValue"]:hover {
        transform: translateY(-2px) scale(1.02);
        box-shadow: 0 4px 15px rgba(131, 164, 212, 0.3);
    }

    /* Tab æ¨£å¼ */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background: linear-gradient(135deg, rgba(255, 255, 255, 0.9), rgba(131, 164, 212, 0.1));
        border-radius: 10px;
        padding: 0.5rem;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        transition: all 0.3s ease;
    }

    .stTabs [data-baseweb="tab-list"]:hover {
        box-shadow: 0 4px 15px rgba(255, 106, 136, 0.2);
    }

    .stTabs [data-baseweb="tab"] {
        background: linear-gradient(120deg, rgba(255, 255, 255, 0.9), rgba(182, 251, 255, 0.2));
        border-radius: 6px;
        padding: 0.5rem 1rem;
        color: #2E4053;
        transition: all 0.3s ease;
    }

    .stTabs [data-baseweb="tab"]:hover {
        transform: translateY(-1px);
        background: linear-gradient(120deg, rgba(255, 255, 255, 0.95), rgba(131, 164, 212, 0.2));
    }

    .stTabs [data-baseweb="tab-highlight"] {
        background: linear-gradient(120deg, #87CEEB, #ADD8E6);
        border-radius: 6px;
        transition: all 0.3s ease;
    }

    /* æ·»åŠ å…¨å±€å‹•ç•«æ•ˆæœ */
    * {
        transition: background-color 0.3s ease;
    }

    /* æ·»åŠ è¼‰å…¥å‹•ç•« */
    @keyframes fadeIn {
        from {
            opacity: 0;
            transform: translateY(10px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }

    .element-container {
        animation: fadeIn 0.5s ease-out;
    }
    </style>
""", unsafe_allow_html=True)

# create_figure_layout å‡½æ•¸èªªæ˜
# å»ºç«‹åœ–è¡¨çš„çµ±ä¸€æ¨£å¼ä½ˆå±€
def create_figure_layout():
    return {
        'plot_bgcolor': 'white',     # åœ–è¡¨å…§éƒ¨èƒŒæ™¯é¡è‰²
        'paper_bgcolor': 'white',    # æ•´å¼µåœ–è¡¨çš„èƒŒæ™¯é¡è‰²
        'xaxis_title': "",           # X è»¸æ¨™é¡Œç•™ç©º
        'yaxis_title': "",           # Y è»¸æ¨™é¡Œç•™ç©º
        'showlegend': False          # ä¸é¡¯ç¤ºåœ–ä¾‹
    }


# æ–°å¢è¡¨æ ¼å’Œæ–‡å­—æ¨£å¼
st.markdown("""
    <style>
    /* æ¨™é¡Œæ–‡å­—æ¨£å¼ */
    h1, h2, h3, h4, h5, h6 {
        color: #333333;
        font-weight: 600;
    }

    /* ä¸€èˆ¬æ–‡å­—æ¨£å¼ */
    p, span, div {
        color: #333333;
    }

    /* è¡¨æ ¼æ¨£å¼ */
    .dataframe {
        color: #333333;
        background-color: #ffffff;
    }

    .dataframe th {
        background-color: #faf9f6;
        color: #333333;
        font-weight: 600;
        padding: 8px;
    }

    .dataframe td {
        color: #333333;
        padding: 8px;
    }

    /* è¡¨æ ¼hoveræ•ˆæœ */
    .dataframe tr:hover {
        background-color: #faf9f6;
    }

    /* æ•¸æ“šæ¨™ç±¤æ¨£å¼ */
    .metric-label {
        color: #333333;
        font-weight: 500;
    }

    /* åœ–è¡¨æ¨™é¡Œå’Œè»¸æ¨™ç±¤ */
    .plot-container text {
        color: #333333 !important;
        fill: #333333 !important;
    }

    /* Streamlitç‰¹å®šå…ƒç´ æ¨£å¼ */
    .stMarkdown, .stText {
        color: #333333;
    }

    .st-bb {
        color: #333333;
    }

    .st-bw {
        color: #333333;
    }

    /* é€£çµé¡è‰² */
    a {
        color: #666666;
    }

    a:hover {
        color: #333333;
    }

    /* å¼·èª¿æ–‡å­— */
    .highlight {
        color: #666666;
        font-weight: 600;
    }
    </style>
""", unsafe_allow_html=True)    

# --- çµ„æ…‹è¨­å®šå€æ®µ ---
if "temp_data_storage_path" not in st.session_state:
    st.session_state.temp_data_storage_path = tempfile.mkdtemp(prefix="ai_analytics_temp_")
   

# è³‡æ–™å„²å­˜ç›®éŒ„ï¼ŒåŠ å…¥ AI åˆ†æå­è³‡æ–™å¤¾
TEMP_DATA_STORAGE = st.session_state.temp_data_storage_path # <-- TEMP_DATA_STORAGE ç¾åœ¨æŒ‡å‘è‡¨æ™‚ç›®éŒ„
# å¯ç”¨çš„æ¨¡å‹æ¸…å–®
AVAILABLE_MODELS = ["gemini-2.0-flash", "gemini-2.0-flash-lite", "gemini-2.5-pro-preview-05-06"]
DEFAULT_WORKER_MODEL = "gemini-2.0-flash-lite"   # é è¨­è³‡æ–™åˆ†ææ¨¡å‹
DEFAULT_JUDGE_MODEL = "gemini-2.0-flash"         # é è¨­åˆ¤æ–·/è©•ä¼°æ¨¡å‹

# PlaceholderLLM æ¨¡æ“¬æ¨¡å‹ï¼ˆç•¶ API ç„¡æ•ˆæ™‚ä½¿ç”¨ï¼‰
# --- LLM åˆå§‹åŒ–èˆ‡ä½”ä½æ¨¡æ“¬ ---
class PlaceholderLLM:
    """åœ¨ API é‡‘é‘°ä¸å¯ç”¨æ™‚æ¨¡æ“¬ LLM å›æ‡‰ã€‚"""

    def __init__(self, model_name="placeholder_model"):
        self.model_name = model_name
        st.warning(f"ç”±æ–¼ API é‡‘é‘°æœªè¨­ç½®æˆ–ç„¡æ•ˆï¼Œæ­£åœ¨ä½¿ç”¨ {self.model_name} çš„æ¨¡æ“¬æ¨¡å‹ PlaceholderLLMã€‚")

    def invoke(self, prompt_input):
        # å°‡æç¤ºè½‰æ›æˆå­—ä¸²ï¼ˆè‹¥æœ‰ to_string æ–¹æ³•å‰‡ä½¿ç”¨ï¼‰
        prompt_str_content = str(prompt_input.to_string() if hasattr(prompt_input, 'to_string') else prompt_input)

        # æ¨¡æ“¬ CDO åˆæ­¥æè¿°è³‡æ–™é›†
        if "CDO, your first task is to provide an initial description of the dataset" in prompt_str_content:
            data_summary_json = {}
            try:
                summary_marker = "Data Summary (for context):"
                if summary_marker in prompt_str_content:
                    json_str_part = prompt_str_content.split(summary_marker)[1].split("\n\nDetailed Initial Description by CDO:")[0].strip()
                    data_summary_json = json.loads(json_str_part)
            except Exception:
                pass  # å¿½ç•¥è§£æå¤±æ•—

            cols = data_summary_json.get("columns", ["N/A"])
            num_rows = data_summary_json.get("num_rows", "N/A")
            num_cols = data_summary_json.get("num_columns", "N/A")
            user_desc = data_summary_json.get("user_provided_column_descriptions", "ä½¿ç”¨è€…å°šæœªæä¾›æ¬„ä½æè¿°")
            dtypes_str = "\n".join(
                [f"- {col}: {data_summary_json.get('dtypes', {}).get(col, 'æœªçŸ¥')}" for col in cols])

            return {"text": f"""
*æ¨¡æ“¬ç‰ˆ CDO è³‡æ–™åˆæ­¥æè¿°ï¼ˆæ¨¡å‹ï¼š{self.model_name}ï¼‰*

**1. è³‡æ–™é›†æ¦‚è¦½ï¼ˆæ¨¡æ“¬ df.info()ï¼‰**
   - åˆ—æ•¸ï¼š{num_rows}ï¼Œæ¬„æ•¸ï¼š{num_cols}
   - å„æ¬„ä½è³‡æ–™å‹æ…‹ï¼š
{dtypes_str}
   - é ä¼°è¨˜æ†¶é«”ä½¿ç”¨é‡ï¼šï¼ˆæ¨¡æ“¬å€¼ï¼‰MB

**2. ä½¿ç”¨è€…æä¾›çš„æ¬„ä½èªªæ˜**
   - {user_desc}

**3. è®Šæ•¸å¯èƒ½å«æ„ï¼ˆç¯„ä¾‹ï¼‰**
   - `ORDERNUMBER`ï¼šæ¯ç­†è¨‚å–®çš„å”¯ä¸€è­˜åˆ¥ç¢¼
   - `QUANTITYORDERED`ï¼šè©²è¨‚å–®ä¸­æŸé …å•†å“çš„æ•¸é‡
   ï¼ˆä»¥ä¸Šç‚ºç¯„ä¾‹è§£é‡‹ï¼Œå¯¦éš›å«ç¾©è¦–ä½¿ç”¨è€…è³‡æ–™è€Œå®šï¼‰

**4. åˆæ­¥è³‡æ–™å“è³ªè©•ä¼°ï¼ˆç¯„ä¾‹ï¼‰**
   - **ç¼ºå¤±å€¼**ï¼šå¦‚ã€Œæ¬„ä½ 'ADDRESSLINE2' æœ‰ 80% ç¼ºå€¼ã€
   - **æ•´é«”çµæ§‹**ï¼šè³‡æ–™çµæ§‹åŸºæœ¬å®Œæ•´ã€‚
"""}

        # æ¨¡æ“¬ CEOã€CFOã€CDO éƒ¨é–€è§€é»
        elif "panel of expert department heads, including the CDO" in prompt_str_content:
            return {"text": f"""
*æ¨¡æ“¬å„éƒ¨é–€ä¸»ç®¡è§€é»ï¼ˆåŸºæ–¼ {self.model_name} æ¨¡å‹ï¼‰*

**åŸ·è¡Œé•·ï¼ˆCEOï¼‰**ï¼šé—œæ³¨ç‡Ÿæ”¶è¶¨å‹¢ï¼Œä¸¦è€ƒæ…®æ¬„ä½æ„ç¾©ã€‚
**è²¡å‹™é•·ï¼ˆCFOï¼‰**ï¼šè©•ä¼°å„åœ°å€åˆ©æ½¤ï¼Œçµåˆä½¿ç”¨è€…èªªæ˜ã€‚
**è³‡æ–™é•·ï¼ˆCDOï¼‰**ï¼šæ³¨æ„ç¼ºå€¼èˆ‡ä½¿ç”¨è€…è£œå……çš„æ¬„ä½è§£é‡‹ã€‚
"""}

        # æ¨¡æ“¬æ•´åˆå¾Œçš„åˆ†æç­–ç•¥
        elif "You are the Chief Data Officer (CDO) of the company." in prompt_str_content and "synthesize these diverse perspectives" in prompt_str_content:
            return {"text": f"""
*æ¨¡æ“¬ç‰ˆåˆ†æç­–ç•¥ç¶œåˆçµæœï¼ˆç”± CDO çµ±æ•´ï¼Œæ¨¡å‹ï¼š{self.model_name}ï¼‰*

1.  **è¦–è¦ºåŒ–æ ¸å¿ƒéŠ·å”®è¶¨å‹¢**ï¼šç¹ªè£½ 'SALES' å° 'ORDERDATE' æŠ˜ç·šåœ–ï¼ˆä¾ä½¿ç”¨è€…æè¿°åˆ¤è®€ SALESï¼‰
2.  **å•†å“ç·šè¡¨ç¾è¡¨æ ¼**ï¼šæŒ‰ 'PRODUCTLINE' åˆ—å‡º 'SALES'ã€'PRICEEACH'ã€'QUANTITYORDERED'
3.  **è¨‚å–®ç‹€æ…‹æè¿°**ï¼šçµ±è¨ˆ 'STATUS' æ¬„ä½çš„æ•¸é‡
4.  **ä¸»è¦æ¬„ä½çš„è³‡æ–™å“è³ªè¡¨æ ¼**ï¼šåˆ—å‡ºç¼ºå€¼æ¯”ä¾‹
5.  **æŒ‰åœ‹å®¶é¡¯ç¤ºéŠ·å”®é¡**ï¼šç¹ªè£½ 'SALES' å° 'COUNTRY' é•·æ¢åœ–
"""}

        # æ¨¡æ“¬ç¨‹å¼ç¢¼ç”Ÿæˆé‚è¼¯
        elif "Python code:" in prompt_str_content and "User Query:" in prompt_str_content:
            user_query_segment = prompt_str_content.split("User Query:")[1].split("\n")[0].lower()

            fallback_script = """
# æ¨™æº–å‡½å¼åº«
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np 
import os

analysis_result = "å·²åŸ·è¡Œåˆ†æé‚è¼¯ã€‚å¦‚é æœŸæœ‰ç‰¹å®šè¼¸å‡ºï¼Œè«‹ç¢ºèªç”Ÿæˆçš„ç¨‹å¼ç¢¼å…§å®¹ã€‚"
plot_data_df = None

# --- AI ç”¢ç”Ÿç¨‹å¼ç¢¼å€ ---
# Placeholderï¼šAI æœƒåœ¨é€™è£¡å¡«å…¥åˆ†æé‚è¼¯
# --- AI ç¨‹å¼ç¢¼å€çµæŸ ---

if 'analysis_result' not in locals() or (isinstance(analysis_result, str) and analysis_result == "å·²åŸ·è¡Œåˆ†æé‚è¼¯ã€‚å¦‚é æœŸæœ‰ç‰¹å®šè¼¸å‡ºï¼Œè«‹ç¢ºèªç”Ÿæˆçš„ç¨‹å¼ç¢¼å…§å®¹ã€‚"):
    if 'df' in locals() and isinstance(df, pd.DataFrame) and not df.empty:
        analysis_result = "è…³æœ¬åŸ·è¡Œå®Œæˆã€‚æœªè¨­ç½®ç‰¹å®šè¼¸å‡ºè®Šæ•¸ 'analysis_result'ï¼Œé è¨­é¡¯ç¤º df.head()ã€‚"
        plot_data_df = df.head().copy()
    else:
        analysis_result = "è…³æœ¬åŸ·è¡Œå®Œæˆã€‚æœªè¨­ç½® 'analysis_result'ï¼Œä¸”ç„¡å¯ç”¨è³‡æ–™æ¡†æ¶ï¼ˆdfï¼‰ã€‚"
"""

            # å›å‚³å¹³å‡å€¼åˆ†æè…³æœ¬
            if "average sales" in user_query_segment:
                return {"text": "analysis_result = df['SALES'].mean()\nplot_data_df = None"}

            # å›å‚³åœ–è¡¨ç¹ªè£½è…³æœ¬
            elif "plot" in user_query_segment or "visualize" in user_query_segment:
                placeholder_plot_filename = "placeholder_plot.png"
                placeholder_full_save_path = os.path.join(TEMP_DATA_STORAGE, placeholder_plot_filename).replace("\\", "/")

                generated_plot_code = f"""..."""  # é€™éƒ¨åˆ†æ˜¯æ•´æ®µåœ–è¡¨å‚™æ´è…³æœ¬ï¼Œå·²åœ¨åŸå§‹ç¢¼è©³åˆ—ï¼ˆç•¥ï¼‰

                return {"text": generated_plot_code}

            # å›å‚³æè¿°çµ±è¨ˆè¡¨æ ¼
            elif "table" in user_query_segment or "summarize" in user_query_segment:
                return {"text": "analysis_result = df.describe()\nplot_data_df = df.describe().reset_index()"}

            # é è¨­å‚™æ´è…³æœ¬
            else:
                return {"text": fallback_script}

        # æ¨¡æ“¬æ–‡å­—å ±å‘Š
        elif "Generate a textual report" in prompt_str_content:
            return {
                "text": f"### æ¨¡æ“¬åˆ†æå ±å‘Šï¼ˆæ¨¡å‹ï¼š{self.model_name}ï¼‰\næ­¤ç‚ºæ ¹æ“š CDO åˆ†æç­–ç•¥èˆ‡ä½¿ç”¨è€…æ¬„ä½èªªæ˜æ‰€ç”¢å‡ºçš„ä½”ä½å ±å‘Šã€‚"}

        # æ¨¡æ“¬åˆ†ææ‰¹åˆ¤æ„è¦‹
        elif "Critique the following analysis artifacts" in prompt_str_content:
            return {"text": f"""
### æ¨¡æ“¬åˆ†æè©•è«–ï¼ˆæ¨¡å‹ï¼š{self.model_name}ï¼‰
**æ•´é«”è©•ä¼°**ï¼šæ¨¡æ“¬å…§å®¹ã€‚åˆ†ææ‡‰åæ˜ ä½¿ç”¨è€…æä¾›çš„æ¬„ä½èªªæ˜ã€‚
**Python ç¨‹å¼ç¢¼**ï¼šæ¨¡æ“¬ã€‚
**è³‡æ–™å…§å®¹**ï¼šæ¨¡æ“¬ã€‚
**å ±å‘Šå…§å®¹**ï¼šæ¨¡æ“¬ã€‚
**å»ºè­°ï¼ˆçµ¦ AI æ¨¡å‹ï¼‰**ï¼šæ¨¡æ“¬ï¼Œæ‡‰ç´å…¥ä½¿ç”¨è€…èƒŒæ™¯è„ˆçµ¡ã€‚
"""}

        # æ¨¡æ“¬ HTML å ±å‘Šç”¢å‡º
        elif "Generate a single, complete, and runnable HTML file" in prompt_str_content:
            return {"text": """<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <title>æ¨¡æ“¬ Bento åˆ†æå ±å‘Š</title>
    <style>...</style>
</head>
<body>
    <div class="bento-grid">
        <div class="bento-item"><h2>åˆ†æç›®æ¨™</h2><p>æ¨¡æ“¬ï¼šä½¿ç”¨è€…è¼¸å…¥çš„å•é¡Œæœƒé¡¯ç¤ºåœ¨æ­¤ã€‚</p></div>
        <div class="bento-item"><h2>è³‡æ–™æ‘˜è¦èˆ‡ä½¿ç”¨è€…èªªæ˜</h2><p>æ¨¡æ“¬ï¼šCDO åˆæ­¥è³‡æ–™èªªæ˜å°‡å‡ºç¾åœ¨æ­¤ã€‚</p></div>
        <div class="bento-item" style="grid-column: span 2;">
            <h2>é‡è¦è³‡æ–™å“è³ªè­¦ç¤º</h2><p>æ¨¡æ“¬ï¼šæ­¤è™•é¡¯ç¤ºéºæ¼å€¼æˆ–å“è³ªå•é¡Œã€‚</p>
        </div>
        <div class="bento-item"><h2>å¯è¡Œçš„æ´å¯Ÿ</h2><p>æ¨¡æ“¬ï¼šåˆ†æçµè«–èˆ‡å¯è¡Œå»ºè­°ã€‚</p></div>
        <div class="bento-item"><h2>åˆ†ææ‰¹è©•æ‘˜è¦</h2><p>æ¨¡æ“¬ï¼šAI å°åˆ†æçµæœçš„è©•è«–ã€‚</p></div>
    </div>
</body>
</html>"""}

        # é è¨­ç„¡æ³•è¾¨è­˜æç¤ºæ™‚
        else:
            return {
                "text": f"{self.model_name} çš„æ¨¡æ“¬å›æ‡‰ï¼šç„¡æ³•è¾¨è­˜æç¤ºï¼Œå‰ 200 å­—å¦‚ä¸‹ï¼š\n{prompt_str_content[:200]}..."}


def get_llm_instance(model_name: str):
    """
    å–å¾—æˆ–åˆå§‹åŒ–ä¸€å€‹ LLM å¯¦ä¾‹ã€‚
    ä½¿ç”¨ç·©å­˜ï¼ˆst.session_state.llm_cacheï¼‰å­˜å„²å·²åˆå§‹åŒ–çš„æ¨¡å‹ã€‚
    å¦‚æœ API é‡‘é‘°æœªè¨­ç½®æˆ–åˆå§‹åŒ–å¤±æ•—ï¼Œå‰‡ä½¿ç”¨ PlaceholderLLMã€‚
    """
    if not model_name:
        st.error("æœªæä¾› LLM åˆå§‹åŒ–çš„æ¨¡å‹åç¨±ã€‚")
        return None
    if "llm_cache" not in st.session_state:
        st.session_state.llm_cache = {}

    # ç¸½æ˜¯å¾ session_state ä¸­ç²å–æœ€æ–°çš„é‡‘é‘°
    current_api_key = st.session_state.get("google_api_key") 

    if model_name not in st.session_state.llm_cache:
        # æª¢æŸ¥å¾ session_state ç²å–çš„é‡‘é‘°æ˜¯å¦æœ‰æ•ˆ
        if not current_api_key or current_api_key == "YOUR_API_KEY_HERE" or current_api_key == "API PLZ":
            st.session_state.llm_cache[model_name] = PlaceholderLLM(model_name)
        else:
            try:
                # Set temperature based on whether the model is a judge model or worker model
                temperature = 0.7 if st.session_state.get("selected_judge_model", "") == model_name else 0.2
                llm = ChatGoogleGenerativeAI(
                    model=model_name,
                    google_api_key=current_api_key, # ä½¿ç”¨å¾ session_state ç²å–çš„é‡‘é‘°
                    temperature=temperature,
                    convert_system_message_to_human=True  # Important for some models/versions
                )
                st.session_state.llm_cache[model_name] = llm
            except Exception as e:
                st.error(f"Failed to initialize Gemini LLM ({model_name}): {e}")
                st.session_state.llm_cache[model_name] = PlaceholderLLM(model_name)
    # å¦‚æœå·²ç¶“åœ¨ cache ä¸­ï¼Œæˆ–è€…ç¾åœ¨å·²ç¶“ç”¨æ–°çš„é‡‘é‘°åˆå§‹åŒ–äº†ï¼Œç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„é‡‘é‘°æ›´æ–° llm_cache ä¸­çš„å¯¦ä¾‹
    elif current_api_key and isinstance(st.session_state.llm_cache[model_name], PlaceholderLLM):
        # å¦‚æœä¹‹å‰æ˜¯ PlaceholderLLMï¼Œä¸”ç¾åœ¨æœ‰é‡‘é‘°äº†ï¼Œå˜—è©¦é‡æ–°åˆå§‹åŒ–
        try:
            temperature = 0.7 if st.session_state.get("selected_judge_model", "") == model_name else 0.2
            llm = ChatGoogleGenerativeAI(
                model=model_name,
                google_api_key=current_api_key,
                temperature=temperature,
                convert_system_message_to_human=True
            )
            st.session_state.llm_cache[model_name] = llm
            st.info(f"âœ… Gemini LLM ({model_name}) å·²æˆåŠŸé‡æ–°åˆå§‹åŒ–ã€‚")
        except Exception as e:
            st.warning(f"é‡æ–°åˆå§‹åŒ– Gemini LLM ({model_name}) å¤±æ•—ï¼š{e}ã€‚ä»ä½¿ç”¨ PlaceholderLLMã€‚")

    return st.session_state.llm_cache[model_name]

# ä½¿ç”¨ Streamlit å¿«å–è£é£¾å™¨ï¼Œé¿å…é‡è¤‡é‹ç®—åŒæ¨£è³‡æ–™ï¼ˆå¯æå‡æ•ˆèƒ½ï¼‰
@st.cache_data
def calculate_data_summary(df_input, user_column_descriptions_content=None):
    """
    è¨ˆç®—è¼¸å…¥ DataFrame çš„å…¨é¢æ‘˜è¦ã€‚
    åŒ…å«ï¼šè¡Œ/åˆ—æ•¸ã€æ¬„ä½å‹æ…‹ã€ç¼ºå¤±å€¼ç‹€æ³ã€æè¿°æ€§çµ±è¨ˆã€é è¦½å‰å¾Œè³‡æ–™åˆ—ï¼Œ
    ä»¥åŠæ•´åˆä½¿ç”¨è€…æä¾›çš„æ¬„ä½æè¿°ï¼ˆè‹¥æœ‰æä¾›ï¼‰ã€‚
    """
    if df_input is None or df_input.empty:
        return None

    # å»ºç«‹ä¸€ä»½å‰¯æœ¬ï¼Œé¿å…æ›´å‹•åŸå§‹è³‡æ–™
    df = df_input.copy()

    # å»ºç«‹æ‘˜è¦å­—å…¸
    data_summary = {
        "num_rows": len(df),  # ç¸½è¡Œæ•¸
        "num_columns": len(df.columns),  # ç¸½æ¬„æ•¸
        "columns": df.columns.tolist(),  # æ¬„ä½åç¨±
        "dtypes": {col: str(df[col].dtype) for col in df.columns},  # å„æ¬„ä½çš„è³‡æ–™å‹æ…‹
        "missing_values_total": int(df.isnull().sum().sum()),  # ç¸½ç¼ºå¤±å€¼æ•¸é‡
        "missing_values_per_column": df.isnull().sum().to_dict(),  # æ¯æ¬„çš„ç¼ºå¤±å€¼æ•¸é‡
        "descriptive_stats_sample": df.describe(include='all').to_json() if not df.empty else "N/A",  # æè¿°çµ±è¨ˆè³‡æ–™ï¼ˆå«æ‰€æœ‰å‹åˆ¥ï¼‰
        "preview_head": df.head().to_dict(orient='records'),  # å‰äº”ç­†é è¦½è³‡æ–™ï¼ˆè½‰æˆå­—å…¸åˆ—è¡¨ï¼‰
        "preview_tail": df.tail().to_dict(orient='records'),  # å¾Œäº”ç­†é è¦½è³‡æ–™
        "numeric_columns": df.select_dtypes(include=np.number).columns.tolist(),  # æ•¸å€¼å‹æ¬„ä½
        "categorical_columns": df.select_dtypes(include=['object', 'category']).columns.tolist()  # é¡åˆ¥å‹æ¬„ä½
    }

    # è¨ˆç®—ç¸½ç¼ºå¤±å€¼çš„ç™¾åˆ†æ¯”
    data_summary["missing_values_percentage"] = (
        data_summary["missing_values_total"] /
        (data_summary["num_rows"] * data_summary["num_columns"])
    ) * 100 if (data_summary["num_rows"] * data_summary["num_columns"]) > 0 else 0

    # æ•´åˆä½¿ç”¨è€…æä¾›çš„æ¬„ä½èªªæ˜ï¼ˆè‹¥æœ‰ï¼‰
    if user_column_descriptions_content:
        data_summary["user_provided_column_descriptions"] = user_column_descriptions_content

    return data_summary



def load_csv_and_get_summary(uploaded_csv_file, uploaded_desc_file=None):
    """
    è¼‰å…¥ä½¿ç”¨è€…ä¸Šå‚³çš„ CSV è³‡æ–™èˆ‡ï¼ˆå¯é¸ï¼‰æè¿°æª”æ¡ˆï¼Œ
    ç”¢ç”Ÿè³‡æ–™æ‘˜è¦ä¸¦æ›´æ–° Streamlit çš„æœƒè©±ç‹€æ…‹ã€‚
    åŒæ™‚é‡ç½® CDO åˆ†ææµç¨‹èˆ‡ç›¸é—œè®Šæ•¸ã€‚
    """
    try:
        # è®€å– CSV æª”æ¡ˆç‚º DataFrame
        df = pd.read_csv(uploaded_csv_file)
        st.session_state.current_dataframe = df  # å°‡ç›®å‰è³‡æ–™å­˜å…¥æœƒè©±ç‹€æ…‹
        st.session_state.data_source_name = uploaded_csv_file.name  # è¨˜éŒ„ä¾†æºæª”å

        user_column_descriptions_content = None
        if uploaded_desc_file:
            try:
                # è®€å…¥èªªæ˜æª”æ¡ˆä¸¦è§£ç¢¼ï¼ˆStreamlit ä¸Šå‚³æª”æ¡ˆé€šå¸¸ç‚ºä½å…ƒçµ„ï¼‰
                user_column_descriptions_content = uploaded_desc_file.getvalue().decode('utf-8')
                st.session_state.desc_file_name = uploaded_desc_file.name
            except Exception as e:
                st.error(f"è®€å–æè¿°æª”éŒ¯èª¤ã€Œ{uploaded_desc_file.name}ã€ï¼š{e}")
                # é è¨­ç‚ºè­¦å‘Šï¼Œä¸ä¸­æ–·æµç¨‹
        else:
            st.session_state.desc_file_name = None  # æ²’æœ‰æè¿°æª”æ™‚æ¸…ç©ºç‹€æ…‹

        st.session_state.show_pygwalker = False

        # æ¸…é™¤å…ˆå‰ç”¢ç”Ÿçš„åˆ†æçµæœ
        st.session_state.current_analysis_artifacts = {}

        # è¨ˆç®—æ–°çš„æ‘˜è¦
        summary_for_state = calculate_data_summary(df.copy(), user_column_descriptions_content)

        if summary_for_state:
            summary_for_state["source_name"] = uploaded_csv_file.name  # åŠ å…¥æª”åè³‡è¨Š
        st.session_state.data_summary = summary_for_state  # å„²å­˜æ‘˜è¦

        # é‡ç½® CDO åˆ†ææµç¨‹ç‹€æ…‹
        st.session_state.cdo_initial_report_text = None
        st.session_state.other_perspectives_text = None
        st.session_state.strategy_text = None
        if "cdo_workflow_stage" in st.session_state:
            del st.session_state.cdo_workflow_stage

        return True
    except Exception as e:
        st.error(f"è¼‰å…¥ CSV æˆ–ç”¢ç”Ÿæ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        st.session_state.current_dataframe = None
        st.session_state.data_summary = None
        return False



@st.cache_data
def get_overview_metrics(df):
    """
    è¨ˆç®—è³‡æ–™è¡¨çš„æ•´é«”æŒ‡æ¨™ï¼š
    å›å‚³åŒ…æ‹¬è¡Œæ•¸ã€åˆ—æ•¸ã€ç¼ºå¤±å€¼ç™¾åˆ†æ¯”ã€æ•¸å€¼æ¬„æ•¸ã€é‡è¤‡åˆ—æ•¸ç­‰ã€‚
    """
    if df is None or df.empty:
        return 0, 0, 0, 0, 0

    num_rows = len(df)  # è¡Œæ•¸
    num_cols = len(df.columns)  # åˆ—æ•¸
    missing_values_total = df.isnull().sum().sum()  # ç¸½ç¼ºå¤±å€¼
    total_cells = num_rows * num_cols  # ç¸½å„²å­˜æ ¼æ•¸
    missing_percentage = (missing_values_total / total_cells) * 100 if total_cells > 0 else 0  # ç¼ºå¤±ç™¾åˆ†æ¯”
    numeric_cols_count = len(df.select_dtypes(include=np.number).columns)  # æ•¸å€¼å‹æ¬„ä½æ•¸
    duplicate_rows = df.duplicated().sum()  # é‡è¤‡åˆ—æ•¸

    return num_rows, num_cols, missing_percentage, numeric_cols_count, duplicate_rows



@st.cache_data
def get_column_quality_assessment(df_input):
    """
    å° DataFrame ä¸­çš„æ¯å€‹æ¬„ä½é€²è¡Œè³‡æ–™å“è³ªè©•ä¼°ï¼ˆæœ€å¤šé¡¯ç¤ºå‰ max_cols_to_display æ¬„ï¼‰ã€‚
    æœƒè¨ˆç®—ï¼šè³‡æ–™å‹æ…‹ã€ç¼ºå¤±ç™¾åˆ†æ¯”ã€å”¯ä¸€å€¼æ•¸é‡ã€æ•¸å€¼ç¯„åœ/å¸¸è¦‹å€¼ï¼Œä»¥åŠå“è³ªåˆ†æ•¸ã€‚
    æœ€å¾Œå›å‚³ä¸€å€‹ DataFrame æ ¼å¼çš„è©•ä¼°çµæœè¡¨ã€‚
    """
    if df_input is None or df_input.empty:
        return pd.DataFrame()
    df = df_input.copy()
    quality_data = []
    max_cols_to_display = 10  # åƒ…è©•ä¼°å‰10æ¬„ï¼ˆé¿å…UIå¤ªæ…¢ï¼‰

    for col in df.columns[:max_cols_to_display]:
        dtype = str(df[col].dtype)
        missing_count = df[col].isnull().sum()
        missing_percent = (missing_count / len(df)) * 100 if len(df) > 0 else 0
        unique_values = df[col].nunique()
        range_common = ""  # é¡¯ç¤ºç¯„åœæˆ–å¸¸è¦‹å€¼

        # æ ¹æ“šè³‡æ–™å‹æ…‹æ±ºå®šé¡¯ç¤ºæ–¹å¼
        if pd.api.types.is_numeric_dtype(df[col]):
            if not df[col].dropna().empty:
                range_common = f"æœ€å°å€¼: {df[col].min():.2f}, æœ€å¤§å€¼: {df[col].max():.2f}"
            else:
                range_common = "ç„¡æ³•è©•ä¼°ï¼ˆçš†ç‚ºç¼ºå€¼ï¼‰"
        elif pd.api.types.is_datetime64_any_dtype(df[col]):
            if not df[col].dropna().empty:
                range_common = f"æœ€å°æ™‚é–“: {df[col].min()}, æœ€å¤§æ™‚é–“: {df[col].max()}"
            else:
                range_common = "ç„¡æ³•è©•ä¼°ï¼ˆçš†ç‚ºç¼ºå€¼ï¼‰"
        else:
            if not df[col].dropna().empty:
                common_vals = df[col].mode().tolist()
                range_common = f"æœ€å¸¸è¦‹å€¼: {', '.join(map(str, common_vals[:3]))}"
                if len(common_vals) > 3:
                    range_common += "..."
            else:
                range_common = "ç„¡æ³•è©•ä¼°ï¼ˆçš†ç‚ºç¼ºå€¼ï¼‰"

        # è¨ˆç®—å“è³ªåˆ†æ•¸ï¼ˆç°¡æ˜“æ–¹å¼ï¼‰
        score = 10
        if missing_percent > 50:
            score -= 5
        elif missing_percent > 20:
            score -= 3
        elif missing_percent > 5:
            score -= 1
        if unique_values == 1 and len(df) > 1:
            score -= 2  # è‹¥ç‚ºå¸¸æ•¸å€¼ï¼ˆåªæœ‰ä¸€ç¨®ï¼‰ï¼Œæ‰£åˆ†
        if unique_values == len(df) and not pd.api.types.is_numeric_dtype(df[col]):
            score -= 1  # è‹¥æ¯ç­†å€¼éƒ½ä¸åŒï¼Œä¸”éæ•¸å€¼æ¬„ä½ï¼Œå¯èƒ½æ˜¯ IDï¼Œä¹Ÿæ‰£åˆ†

        quality_data.append({
            "æ¬„ä½åç¨±": col,
            "è³‡æ–™å‹æ…‹": dtype,
            "ç¼ºå€¼æ¯”ä¾‹": f"{missing_percent:.2f}%",
            "å”¯ä¸€å€¼æ•¸é‡": unique_values,
            "ç¯„åœ / å¸¸è¦‹å€¼": range_common,
            "å“è³ªè©•åˆ†ï¼ˆæ»¿åˆ†10åˆ†ï¼‰": max(0, score)
        })

    return pd.DataFrame(quality_data)



def generate_data_quality_dashboard(df_input):
    """
    ä½¿ç”¨ Streamlit ç”Ÿæˆè³‡æ–™å“è³ªå„€è¡¨æ¿ï¼Œ
    åŒ…å«ï¼šæ•´é«”æ¦‚è¦½æŒ‡æ¨™ã€æ¬„ä½å“è³ªè©•ä¼°ã€æ•¸å€¼æ¬„åˆ†ä½ˆåœ–ã€åˆ†é¡æ¬„åˆ†ä½ˆåœ–ã€æ•¸å€¼æ¬„ç›¸é—œä¿‚æ•¸ç†±åœ–ã€‚
    """
    if df_input is None or df_input.empty:
        st.warning("âš ï¸ å°šæœªè¼‰å…¥è³‡æ–™æˆ– DataFrame ç‚ºç©ºï¼Œè«‹å…ˆä¸Šå‚³ CSV æª”æ¡ˆã€‚")
        return

    df = df_input.copy()
    st.header("ğŸ“Š è³‡æ–™å“è³ªå„€è¡¨æ¿")
    st.markdown("ä»¥ä¸‹ç‚ºæœ¬è³‡æ–™é›†çš„å“è³ªèˆ‡ç‰¹å¾µæ¦‚è¦½ï¼š")

    # --- é—œéµæŒ‡æ¨™ ---
    st.subheader("é—œéµè³‡æ–™é›†æŒ‡æ¨™")
    num_rows, num_cols, missing_percentage, numeric_cols_count, duplicate_rows = get_overview_metrics(df.copy())
    col1, col2, col3, col4, col5 = st.columns(5)
    col1.metric("ç¸½è¡Œæ•¸", f"{num_rows:,}")
    col2.metric("ç¸½æ¬„æ•¸", f"{num_cols:,}")
    if missing_percentage > 5:
        col3.metric("ç¼ºå€¼æ¯”ä¾‹", f"{missing_percentage:.2f}%", delta_color="inverse",
                    help="æ•´é«”è³‡æ–™é›†ä¸­ç¼ºå€¼æ‰€å ç™¾åˆ†æ¯”ã€‚è¶…é 5% é¡¯ç¤ºç´…è‰²è­¦å‘Šã€‚")
    else:
        col3.metric("ç¼ºå€¼æ¯”ä¾‹", f"{missing_percentage:.2f}%",
                    help="æ•´é«”è³‡æ–™é›†ä¸­ç¼ºå€¼æ‰€å ç™¾åˆ†æ¯”ã€‚")
    col4.metric("æ•¸å€¼æ¬„æ•¸", f"{numeric_cols_count:,}")
    col5.metric("é‡è¤‡åˆ—æ•¸", f"{duplicate_rows:,}", help="å®Œå…¨é‡è¤‡çš„åˆ—æ•¸é‡")
    st.markdown("---")

    # --- å„æ¬„ä½å“è³ªè©•ä¼° ---
    st.subheader("å„æ¬„ä½è³‡æ–™å“è³ªè©•ä¼°")
    if len(df.columns) > 10:
        st.caption(f"âš ï¸ åƒ…é¡¯ç¤ºå‰ 10 æ¬„ï¼Œå¯¦éš›å…± {len(df.columns)} æ¬„ã€‚å®Œæ•´å ±å‘Šè«‹è¦‹ PDFã€‚")

    quality_df = get_column_quality_assessment(df.copy())

    if not quality_df.empty:
        # æ ¹æ“šç¼ºå€¼èˆ‡å“è³ªåˆ†æ•¸åŠ ä¸Šé¡è‰²
        def style_quality_table(df_to_style):
            return df_to_style.style.apply(
                lambda row: ['background-color: #FFCDD2' if float(str(row["ç¼ºå€¼æ¯”ä¾‹"]).replace('%', '')) > 20
                             else ('background-color: #FFF9C4' if float(str(row["ç¼ºå€¼æ¯”ä¾‹"]).replace('%', '')) > 5 else '')
                             for _ in row], axis=1, subset=["ç¼ºå€¼æ¯”ä¾‹"]
            ).apply(
                lambda row: ['background-color: #FFEBEE' if row["å“è³ªè©•åˆ†ï¼ˆæ»¿åˆ†10åˆ†ï¼‰"] < 5
                             else ('background-color: #FFFDE7' if row["å“è³ªè©•åˆ†ï¼ˆæ»¿åˆ†10åˆ†ï¼‰"] < 7 else '')
                             for _ in row], axis=1, subset=["å“è³ªè©•åˆ†ï¼ˆæ»¿åˆ†10åˆ†ï¼‰"]
            )

        st.dataframe(style_quality_table(quality_df), use_container_width=True)
    else:
        st.info("âš ï¸ ç„¡æ³•ç”¢ç”Ÿæ¬„ä½å“è³ªè©•ä¼°è¡¨æ ¼ã€‚")

    from datetime import datetime

    # ç¢ºä¿åˆå§‹åŒ– current_time
    if "current_time" not in st.session_state:
        st.session_state.current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    from reportlab.pdfgen import canvas
    from reportlab.lib.pagesizes import letter
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    import numpy as np
    import plotly.express as px

    # è¨»å†Šä¸­æ–‡å­—å‹
    font_path = r"./fonts/msjh.ttc"
    try:
        pdfmetrics.registerFont(TTFont('CustomFont', font_path))
        default_font = 'CustomFont'
    except:
        st.warning("âš ï¸ ä¸­æ–‡å­—å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥è·¯å¾‘èˆ‡å­—å‹æ ¼å¼")
        default_font = 'Helvetica'

    # ç”Ÿæˆè³‡æ–™å“è³ª PDF å ±å‘Š
    def generate_pdf_report(df, file_name="data_quality_report.pdf"):
        c = canvas.Canvas(file_name, pagesize=letter)
        width, height = letter

        # æ¨™é¡Œ
        c.setFont(default_font, 16)
        c.drawString(100, height - 40, u"è³‡æ–™å“è³ªå ±å‘Š")
        c.setFont(default_font, 12)
        c.drawString(100, height - 80, f"å ±å‘Šæ—¥æœŸ: {st.session_state.current_time}")

        y_position = height - 120

        # --- é—œéµè³‡æ–™é›†æŒ‡æ¨™ ---
        c.setFont(default_font, 12)
        c.drawString(100, y_position, "é—œéµè³‡æ–™é›†æŒ‡æ¨™:")
        y_position -= 20

        c.setFont(default_font, 10)
        c.drawString(100, y_position, f"ç¸½è¡Œæ•¸: {df.shape[0]}")
        y_position -= 15
        c.drawString(100, y_position, f"ç¸½æ¬„æ•¸: {df.shape[1]}")
        y_position -= 15
        missing_percentage = df.isnull().mean().mean() * 100
        c.drawString(100, y_position, f"ç¼ºå€¼æ¯”ä¾‹: {missing_percentage:.2f}%")
        y_position -= 15
        numeric_cols_count = len(df.select_dtypes(include=np.number).columns)
        c.drawString(100, y_position, f"æ•¸å€¼æ¬„æ•¸: {numeric_cols_count}")
        y_position -= 15
        duplicate_rows = df.duplicated().sum()
        c.drawString(100, y_position, f"é‡è¤‡åˆ—æ•¸: {duplicate_rows}")
        y_position -= 25

        # --- å„æ¬„ä½è³‡æ–™å“è³ªè©•ä¼° ---
        c.setFont(default_font, 12)
        c.drawString(100, y_position, "å„æ¬„ä½è³‡æ–™å“è³ªè©•ä¼°:")
        y_position -= 20

        quality_df = get_column_quality_assessment(df)

        if not quality_df.empty:
            for idx, row in quality_df.iterrows():
                c.setFont(default_font, 10)
                c.drawString(100, y_position, f"{row['æ¬„ä½åç¨±']}: ç¼ºå€¼æ¯”ä¾‹ {row['ç¼ºå€¼æ¯”ä¾‹']}, å“è³ªè©•åˆ† {row['å“è³ªè©•åˆ†ï¼ˆæ»¿åˆ†10åˆ†ï¼‰']}")
                y_position -= 15
        else:
            c.drawString(100, y_position, "ç„¡æ³•ç”¢ç”Ÿæ¬„ä½å“è³ªè©•ä¼°è¡¨æ ¼")
            y_position -= 20

        y_position -= 25

        # --- æ•¸å€¼æ¬„ä½åˆ†ä½ˆ ---
        c.setFont(default_font, 12)
        c.drawString(100, y_position, "æ•¸å€¼æ¬„ä½åˆ†ä½ˆåœ–:")
        y_position -= 20

        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
        for col in numeric_cols:
            c.setFont(default_font, 10)
            c.drawString(100, y_position, f"{col} åˆ†ä½ˆï¼š")
            y_position -= 15
            c.drawString(100, y_position, f"å¹³å‡æ•¸: {df[col].mean():.2f}, ä¸­ä½æ•¸: {df[col].median():.2f}")
            y_position -= 15

        # --- é¡åˆ¥æ¬„ä½åˆ†ä½ˆ ---
        c.setFont(default_font, 12)
        c.drawString(100, y_position, "é¡åˆ¥æ¬„ä½åˆ†ä½ˆåœ–:")
        y_position -= 20

        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        for col in categorical_cols:
            c.setFont(default_font, 10)
            c.drawString(100, y_position, f"{col} åˆ†ä½ˆï¼š")
            y_position -= 15
            value_counts = df[col].value_counts(normalize=True).mul(100).round(2)
            for idx, value in value_counts.items():
                c.drawString(100, y_position, f"{idx}: {value}%")
                y_position -= 15

        # --- æ•¸å€¼æ¬„ä½ç›¸é—œä¿‚æ•¸ç†±åœ– ---
        c.setFont(default_font, 12)
        c.drawString(100, y_position, "æ•¸å€¼æ¬„ä½ç›¸é—œä¿‚æ•¸ç†±åœ–:")
        y_position -= 20

        numeric_cols_for_corr = df.select_dtypes(include=np.number).columns.tolist()

        if len(numeric_cols_for_corr) < 2:
            c.drawString(100, y_position, "âš ï¸ è‡³å°‘éœ€å…©å€‹æ•¸å€¼æ¬„ä½æ‰èƒ½ç¹ªè£½ç›¸é—œä¿‚æ•¸ç†±åœ–ã€‚")
            y_position -= 20
        else:
            try:
                corr_matrix = df[numeric_cols_for_corr].corr()
                fig_heatmap = px.imshow(corr_matrix, text_auto=True, aspect="auto",
                                        color_continuous_scale='RdBu_r',
                                        title="æ•¸å€¼æ¬„ä½ç›¸é—œä¿‚æ•¸ç†±åœ–")
                fig_heatmap.update_xaxes(side="bottom")
                fig_heatmap.update_layout(xaxis_tickangle=-45, yaxis_tickangle=0)
                fig_heatmap.write_image("corr_heatmap.png")
                c.drawImage("corr_heatmap.png", 100, y_position, width=400, height=200)
                y_position -= 220
            except ValueError as e:
                c.drawString(100, y_position, f"ç›¸é—œä¿‚æ•¸è¨ˆç®—éŒ¯èª¤: {e}")
                y_position -= 20

        # å„²å­˜ PDF
        c.save()
        st.success(f"è³‡æ–™å“è³ªå ±å‘Šå·²ç”Ÿæˆï¼š{file_name}")

        # --- ä¸‹è¼‰æŒ‰éˆ• ---
        with open(file_name, "rb") as pdf_file:
            st.download_button(
                label="ä¸‹è¼‰è³‡æ–™å“è³ªå ±å‘Š PDF",
                data=pdf_file,
                file_name=file_name,
                mime="application/pdf"
            )


    # æ›´æ–°ç¾æœ‰ç¨‹å¼ç¢¼ï¼Œè®“æŒ‰éˆ•è§¸ç™¼ PDF å ±å‘Šç”Ÿæˆ
    if st.button("ç”¢ç”Ÿå®Œæ•´è³‡æ–™å“è³ª PDF å ±å‘Š", key="dq_pdf_placeholder"):
        generate_pdf_report(df)

    st.markdown("---")

    # --- æ•¸å€¼æ¬„ä½åˆ†ä½ˆ ---
    st.subheader("æ•¸å€¼æ¬„ä½åˆ†ä½ˆåœ–")
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    if not numeric_cols:
        st.info("âš ï¸ æœ¬è³‡æ–™é›†ä¸­æœªåµæ¸¬åˆ°æ•¸å€¼å‹æ¬„ä½ã€‚")
    else:
        selected_numeric_col = st.selectbox("è«‹é¸æ“‡æ¬²åˆ†æåˆ†ä½ˆçš„æ•¸å€¼æ¬„ä½ï¼š", numeric_cols, key="dq_numeric_select")
        if selected_numeric_col:
            col_data = df[selected_numeric_col].dropna()
            if not col_data.empty:
                fig = px.histogram(col_data, x=selected_numeric_col, marginal="box",
                                   title=f"{selected_numeric_col} çš„åˆ†ä½ˆåœ–", opacity=0.75,
                                   histnorm='probability density')
                fig.add_trace(go.Scatter(x=col_data, y=[0] * len(col_data),
                                         mode='markers',
                                         marker=dict(color='rgba(0,0,0,0)'), showlegend=False))
                st.plotly_chart(fig, use_container_width=True)

                st.markdown("**ä¸»è¦çµ±è¨ˆæŒ‡æ¨™ï¼š**")
                stats_cols = st.columns(5)
                stats_cols[0].metric("å¹³å‡æ•¸", f"{col_data.mean():.2f}")
                stats_cols[1].metric("ä¸­ä½æ•¸", f"{col_data.median():.2f}")
                stats_cols[2].metric("æ¨™æº–å·®", f"{col_data.std():.2f}")
                stats_cols[3].metric("æœ€å°å€¼", f"{col_data.min():.2f}")
                stats_cols[4].metric("æœ€å¤§å€¼", f"{col_data.max():.2f}")
            else:
                st.info(f"âš ï¸ æ¬„ä½ '{selected_numeric_col}' åƒ…åŒ…å«ç¼ºå¤±å€¼ï¼Œç„¡æ³•ç¹ªè£½åœ–è¡¨ã€‚")

    st.markdown("---")

    # --- é¡åˆ¥æ¬„ä½åˆ†ä½ˆ ---
    st.subheader("é¡åˆ¥æ¬„ä½åˆ†ä½ˆåœ–")
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    if not categorical_cols:
        st.info("âš ï¸ è³‡æ–™é›†ä¸­æœªåµæ¸¬åˆ°é¡åˆ¥å‹æ¬„ä½ã€‚")
    else:
        selected_categorical_col = st.selectbox("è«‹é¸æ“‡æ¬²åˆ†æåˆ†ä½ˆçš„é¡åˆ¥æ¬„ä½ï¼š",
                                                categorical_cols, key="dq_categorical_select")
        if selected_categorical_col:
            col_data = df[selected_categorical_col].dropna()
            if not col_data.empty:
                value_counts = col_data.value_counts(normalize=True).mul(100).round(2)
                count_abs = col_data.value_counts()
                fig = px.bar(x=value_counts.index, y=value_counts.values,
                             title=f"{selected_categorical_col} çš„åˆ†ä½ˆæƒ…å½¢",
                             labels={'x': selected_categorical_col, 'y': 'ç™¾åˆ†æ¯” (%)'},
                             text=[f"{val:.1f}%ï¼ˆ{count_abs[idx]} ç­†ï¼‰" for idx, val in value_counts.items()])
                fig.update_layout(xaxis_title=selected_categorical_col, yaxis_title="ç™¾åˆ†æ¯” (%)")
                fig.update_traces(textposition='outside')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info(f"âš ï¸ æ¬„ä½ '{selected_categorical_col}' åƒ…åŒ…å«ç¼ºå¤±å€¼ï¼Œç„¡æ³•ç¹ªè£½åœ–è¡¨ã€‚")

    st.markdown("---")

    # --- æ•¸å€¼æ¬„ä½ç›¸é—œä¿‚æ•¸ç†±åœ– ---
    st.subheader("æ•¸å€¼æ¬„ä½ç›¸é—œä¿‚æ•¸ç†±åœ–")
    numeric_cols_for_corr = df.select_dtypes(include=np.number).columns.tolist()
    if len(numeric_cols_for_corr) < 2:
        st.info("âš ï¸ è‡³å°‘éœ€å…©å€‹æ•¸å€¼æ¬„ä½æ‰èƒ½ç¹ªè£½ç›¸é—œä¿‚æ•¸ç†±åœ–ã€‚")
    else:
        corr_matrix = df[numeric_cols_for_corr].corr()
        fig_heatmap = px.imshow(corr_matrix, text_auto=True, aspect="auto",
                                color_continuous_scale='RdBu_r',
                                title="æ•¸å€¼æ¬„ä½ç›¸é—œä¿‚æ•¸ç†±åœ–")
        fig_heatmap.update_xaxes(side="bottom")
        fig_heatmap.update_layout(xaxis_tickangle=-45, yaxis_tickangle=0)
        st.plotly_chart(fig_heatmap, use_container_width=True)



class LocalCodeExecutionEngine:
    """
    åœ¨å—æ§ç’°å¢ƒä¸­åŸ·è¡Œ Python ç¨‹å¼ç¢¼å­—ä¸²ã€‚
    ç¨‹å¼ç¢¼é æœŸæœƒå°åç‚º 'df' çš„ pandas DataFrame é€²è¡Œè™•ç†ã€‚
    æ”¯æ´æ“·å–çµæœã€åœ–è¡¨èˆ‡éŒ¯èª¤è³‡è¨Šã€‚
    """

    def execute_code(self, code_string, df_input):
        """
        åŸ·è¡Œæä¾›çš„ Python ç¨‹å¼ç¢¼å­—ä¸²ã€‚

        åƒæ•¸ï¼š
            code_string (str)ï¼šæ¬²åŸ·è¡Œçš„ Python ç¨‹å¼ç¢¼å­—ä¸²ã€‚
            df_input (pd.DataFrame)ï¼šæœƒä»¥ 'df' çš„è®Šæ•¸åæ³¨å…¥ç¨‹å¼ç¢¼ä½œç”¨åŸŸçš„è³‡æ–™è¡¨ã€‚

        å›å‚³ï¼š
            dictï¼šåŒ…å«åŸ·è¡Œé¡å‹èˆ‡çµæœçš„å­—å…¸ï¼Œå¯èƒ½åŒ…å«ï¼šéŒ¯èª¤è¨Šæ¯ã€åœ–è¡¨æª”æ¡ˆã€è¡¨æ ¼è³‡æ–™ã€æ–‡å­—å…§å®¹ç­‰ã€‚
        """
        if df_input is None:
            return {"type": "error", "message": "æœªè¼‰å…¥ä»»ä½•è³‡æ–™ï¼Œç„¡æ³•åŸ·è¡Œç¨‹å¼ç¢¼ã€‚"}

        # å»ºç«‹å®‰å…¨çš„åŸ·è¡Œç’°å¢ƒ
        exec_globals = globals().copy()
        exec_globals['plt'] = matplotlib.pyplot
        exec_globals['sns'] = seaborn
        exec_globals['pd'] = pd
        exec_globals['np'] = np
        exec_globals['os'] = os

        # å»ºç«‹æœ¬åœ°ä½œç”¨åŸŸï¼Œæ³¨å…¥è³‡æ–™èˆ‡å¸¸ç”¨æ¨¡çµ„
        local_scope = {
            'df': df_input.copy(),
            'pd': pd,
            'plt': matplotlib.pyplot,
            'sns': seaborn,
            'np': np,
            'os': os,
            'TEMP_DATA_STORAGE': TEMP_DATA_STORAGE
        }

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_data_df_saved_path = None
        default_analysis_result_message = "ç¨‹å¼ç¢¼å·²åŸ·è¡Œï¼Œä½†å°šæœªè¨­å®š 'analysis_result' çµæœè®Šæ•¸ã€‚"

        # åˆå§‹åŒ–è¼¸å‡ºè®Šæ•¸
        local_scope['analysis_result'] = default_analysis_result_message
        local_scope['plot_data_df'] = None

        try:
            os.makedirs(TEMP_DATA_STORAGE, exist_ok=True)  # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            exec(code_string, exec_globals, local_scope)  # åŸ·è¡Œä½¿ç”¨è€…ä»£ç¢¼

            analysis_result = local_scope.get('analysis_result')
            plot_data_df = local_scope.get('plot_data_df')

            # è‹¥æœªè¨­å®šçµæœè®Šæ•¸ï¼Œçµ¦äºˆè­¦å‘Š
            if isinstance(analysis_result, str) and analysis_result == default_analysis_result_message:
                st.warning("âš ï¸ ç¨‹å¼ç¢¼æœªè¨­å®š 'analysis_result' çµæœè®Šæ•¸ï¼Œå»ºè­°æ˜ç¢ºæŒ‡å®šã€‚")

            # è‹¥ç‚ºéŒ¯èª¤è¨Šæ¯é–‹é ­
            if isinstance(analysis_result, str) and analysis_result.startswith("Error:"):
                return {"type": "error", "message": analysis_result}

            # è™•ç†åœ–è¡¨çµæœï¼ˆè‹¥ analysis_result æ˜¯åœ–ç‰‡æª”åï¼‰
            if isinstance(analysis_result, str) and any(analysis_result.lower().endswith(ext)
                                                        for ext in [".png", ".jpg", ".jpeg", ".svg"]):
                plot_filename = os.path.basename(analysis_result)
                final_plot_path = os.path.join(TEMP_DATA_STORAGE, plot_filename)

                if not os.path.exists(final_plot_path):
                    # è‹¥çµ¦çš„æ˜¯å®Œæ•´è·¯å¾‘ï¼Œä»å˜—è©¦ä½¿ç”¨
                    if os.path.isabs(analysis_result) and os.path.exists(analysis_result):
                        final_plot_path = analysis_result
                        st.warning(f"âš ï¸ åœ–è¡¨ä»¥çµ•å°è·¯å¾‘å„²å­˜ï¼š{analysis_result}ï¼Œå»ºè­°åƒ…å‚³å…¥æª”åã€‚")
                    else:
                        # å˜—è©¦è‡ªå‹•å°‡ç›®å‰çš„æ´»èºåœ–è¡¨å„²å­˜è‡³é æœŸè·¯å¾‘
                        try:
                            import matplotlib.pyplot as _plt_autosave
                            if _plt_autosave.get_fignums():
                                _plt_autosave.gcf().savefig(final_plot_path, bbox_inches="tight")
                                st.info(f"âœ… åµæ¸¬åˆ°æœªå„²å­˜åœ–è¡¨ï¼Œå·²è‡ªå‹•å°‡ç•¶å‰åœ–è¡¨å­˜ç‚ºï¼š{final_plot_path}")
                            else:
                                raise FileNotFoundError
                        except Exception:
                            return {"type": "error", "message": f"æ‰¾ä¸åˆ°åœ–è¡¨æª”æ¡ˆ '{plot_filename}'ï¼Œ"
                                                           f"è«‹ç¢ºèª AI ä½¿ç”¨ `os.path.join(TEMP_DATA_STORAGE, 'æª”å')` "
                                                           f"å„²å­˜åœ–æª”ï¼Œä¸¦å°‡ `analysis_result` è¨­ç‚ºç´”æª”åã€‚"}

                # è‹¥æœ‰ç¹ªåœ–ç”¨è³‡æ–™è¡¨ï¼Œå¦è¡Œå„²å­˜
                if isinstance(plot_data_df, pd.DataFrame) and not plot_data_df.empty:
                    plot_data_filename = f"plot_data_for_{os.path.splitext(plot_filename)[0]}_{timestamp}.csv"
                    plot_data_df_saved_path = os.path.join(TEMP_DATA_STORAGE, plot_data_filename)
                    plot_data_df.to_csv(plot_data_df_saved_path, index=False)
                    st.info(f"ğŸ“‚ åœ–è¡¨å°æ‡‰è³‡æ–™å·²å„²å­˜æ–¼ï¼š{plot_data_df_saved_path}")
                elif plot_data_df is not None:
                    st.warning("âš ï¸ `plot_data_df` å·²å®šç¾©ä½†ä¸æ˜¯æœ‰æ•ˆçš„ DataFrameï¼Œç„¡æ³•å„²å­˜ã€‚")

                return {
                    "type": "plot",
                    "plot_path": final_plot_path,
                    "data_path": plot_data_df_saved_path
                }

            # è™•ç†è¡¨æ ¼çµæœï¼ˆè‹¥ç‚º DataFrame æˆ– Seriesï¼‰
            elif isinstance(analysis_result, (pd.DataFrame, pd.Series)):
                analysis_result_df = (analysis_result.to_frame()
                                      if isinstance(analysis_result, pd.Series)
                                      else analysis_result)
                if analysis_result_df.empty:
                    return {"type": "text", "value": "ğŸ“„ åˆ†æçµæœç‚ºç©ºè¡¨æ ¼ã€‚"}
                saved_csv_path = os.path.join(TEMP_DATA_STORAGE, f"table_result_{timestamp}.csv")
                analysis_result_df.to_csv(saved_csv_path, index=False)
                return {
                    "type": "table",
                    "data_path": saved_csv_path,
                    "dataframe_result": analysis_result_df
                }

            # è‹¥ç‚ºç´”æ–‡å­—è¼¸å‡º
            else:
                return {"type": "text", "value": str(analysis_result)}

        except Exception as e:
            import traceback
            error_message_for_user = f"âŒ ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}\nğŸ” è¿½è¹¤è¨˜éŒ„ï¼š\n{traceback.format_exc()}"
            current_analysis_res = local_scope.get('analysis_result', default_analysis_result_message)
            if current_analysis_res is None or (
                    isinstance(current_analysis_res, pd.DataFrame) and current_analysis_res.empty):
                local_scope['analysis_result'] = f"åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}"
            return {
                "type": "error",
                "message": error_message_for_user,
                "final_analysis_result_value": local_scope['analysis_result']
            }


# å»ºç«‹åŸ·è¡Œå™¨å¯¦ä¾‹ä¾›ä½¿ç”¨
code_executor = LocalCodeExecutionEngine()



# --- å°‡åˆ†æçµæœåŒ¯å‡ºç‚º PDF æ–‡ä»¶ ---
def export_analysis_to_pdf(artifacts, output_filename="analysis_report.pdf"):
    """
    å°‡åˆ†æçµæœï¼ˆæŸ¥è©¢ã€CDO å ±å‘Šã€åœ–è¡¨ã€æ•¸æ“šã€å ±å‘Šæ–‡å­—ã€è©•è«–ï¼‰å°å‡ºç‚º PDF æ–‡ä»¶ã€‚

    åƒæ•¸ï¼š
        artifacts (dict)ï¼šåŒ…å«åˆ†æè³‡æ–™çš„å­—å…¸ï¼Œå…§å«è·¯å¾‘èˆ‡å…§å®¹ã€‚
        output_filename (str)ï¼šè¼¸å‡ºçš„ PDF æ–‡ä»¶åç¨±ï¼ˆå«å‰¯æª”åï¼‰ã€‚
    å›å‚³ï¼š
        strï¼šæˆåŠŸç”Ÿæˆçš„ PDF æª”æ¡ˆè·¯å¾‘ï¼›è‹¥å¤±æ•—å‰‡å›å‚³ Noneã€‚
    """
    # è¨»å†Šä¸­æ–‡å­—é«” (å˜—è©¦å¤šç¨®å¸¸è¦‹ä¸­æ–‡å­—é«”)
    chinese_font_available = False
    chinese_font_name = ''
    
    # å˜—è©¦è¼‰å…¥ä¸åŒçš„ä¸­æ–‡å­—é«”ï¼ŒæŒ‰å„ªå…ˆé †åº
    font_options = [
        ("MicrosoftJhengHei", "c:/windows/fonts/msjh.ttc"),  # å¾®è»Ÿæ­£é»‘é«”
        ("MicrosoftJhengHei", "c:/windows/fonts/msjhbd.ttc"), # å¾®è»Ÿæ­£é»‘é«”ç²—é«”
        ("DFKaiShu", "c:/windows/fonts/kaiu.ttf"),           # æ¨™æ¥·é«”
        ("MingLiU", "c:/windows/fonts/mingliu.ttc"),         # ç´°æ˜é«”
        ("SimSun", "c:/windows/fonts/simsun.ttc"),           # æ–°å®‹é«”
        ("SimHei", "c:/windows/fonts/simhei.ttf")            # é»‘é«”
    ]
    
    for font_name, font_path in font_options:
        try:
            pdfmetrics.registerFont(TTFont(font_name, font_path))
            chinese_font_available = True
            chinese_font_name = font_name
            st.success(f"æˆåŠŸè¼‰å…¥ä¸­æ–‡å­—é«”: {font_name}")
            break
        except Exception:
            continue
    
    if not chinese_font_available:
        st.warning("ç„¡æ³•è¼‰å…¥ä»»ä½•ä¸­æ–‡å­—é«”ï¼ŒPDF ä¸­çš„ä¸­æ–‡å¯èƒ½æœƒé¡¯ç¤ºä¸æ­£ç¢ºã€‚è«‹ç¢ºä¿ç³»çµ±å®‰è£äº†ä¸­æ–‡å­—é«”ã€‚")
    
    pdf_path = os.path.join(TEMP_DATA_STORAGE, output_filename)
    doc = SimpleDocTemplate(pdf_path)  # å»ºç«‹ PDF æ–‡ä»¶æ¨¡æ¿
    styles = getSampleStyleSheet()     # å–å¾—é è¨­æ¨£å¼
    
    # å»ºç«‹ä¸­æ–‡æ¨£å¼
    if chinese_font_available:
        # è¤‡è£½ä¸¦ä¿®æ”¹ç¾æœ‰æ¨£å¼ä»¥æ”¯æ´ä¸­æ–‡
        styles['Title'] = ParagraphStyle('Title', parent=styles['Title'], fontName=chinese_font_name, leading=14)
        styles['h1'] = ParagraphStyle('h1', parent=styles['Heading1'], fontName=chinese_font_name, leading=14)
        styles['h2'] = ParagraphStyle('h2', parent=styles['Heading2'], fontName=chinese_font_name, leading=14)
        styles['Normal'] = ParagraphStyle('Normal', parent=styles['Normal'], fontName=chinese_font_name, leading=12)
        styles['Bullet'] = ParagraphStyle('Bullet', parent=styles['Bullet'], fontName=chinese_font_name, leading=12)
        styles['Italic'] = ParagraphStyle('Italic', parent=styles['Italic'], fontName=chinese_font_name, leading=12)
    
    story = []                         # ç”¨ä¾†å„²å­˜æ‰€æœ‰ PDF é é¢å…§å®¹çš„ list

    # --- æ¨™é¡Œ ---
    story.append(Paragraph("ğŸ“˜ å…¨é¢åˆ†æå ±å‘Š", styles['h1']))
    story.append(Spacer(1, 0.2 * inch))

    # --- 1. åˆ†æç›®æ¨™ï¼ˆä½¿ç”¨è€…æŸ¥è©¢ï¼‰---
    story.append(Paragraph("1. åˆ†æç›®æ¨™ï¼ˆç”¨æˆ¶æŸ¥è©¢ï¼‰", styles['h2']))
    analysis_goal = artifacts.get("original_user_query", "æœªæŒ‡å®šã€‚")
    story.append(Paragraph(html.escape(analysis_goal), styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # --- 2. CDO åˆå§‹å ±å‘Š ---
    story.append(Paragraph("2. CDO åˆå§‹æ•¸æ“šæè¿°èˆ‡å“è³ªè©•ä¼°", styles['h2']))
    cdo_report_text = st.session_state.get("cdo_initial_report_text", "âš ï¸ ç„¡æ³•å–å¾— CDO åˆå§‹å ±å‘Šã€‚")
    cdo_report_text_cleaned = html.escape(cdo_report_text.replace("**", ""))

    # æ¯æ®µè½åˆ†æ®µåŠ å…¥
    for para_text in cdo_report_text_cleaned.split('\n'):
        if para_text.strip().startswith("- "):
            story.append(Paragraph(para_text, styles['Bullet'], bulletText='-'))
        elif para_text.strip():
            story.append(Paragraph(para_text, styles['Normal']))
        else:
            story.append(Spacer(1, 0.1 * inch))
    story.append(Spacer(1, 0.2 * inch))
    story.append(PageBreak())  # æ›é 

    # --- 3. åœ–è¡¨åœ–ç‰‡ ---
    story.append(Paragraph("3. ç”Ÿæˆçš„åœ–è¡¨", styles['h2']))
    plot_image_path = artifacts.get("plot_image_path")
    if plot_image_path and os.path.exists(plot_image_path):
        try:
            img = Image(plot_image_path, width=6 * inch, height=4 * inch)
            img.hAlign = 'CENTER'
            story.append(img)
        except Exception as e:
            story.append(Paragraph(f"âŒ åœ–è¡¨åµŒå…¥éŒ¯èª¤ï¼š{html.escape(str(e))}", styles['Normal']))
    else:
        story.append(Paragraph("âš ï¸ æ‰¾ä¸åˆ°åœ–è¡¨æˆ–åœ–è¡¨è·¯å¾‘ç„¡æ•ˆã€‚", styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # --- 4. åœ–è¡¨å°æ‡‰æ•¸æ“š / åŸ·è¡Œå¾Œè³‡æ–™è¡¨ ---
    story.append(Paragraph("4. åœ–è¡¨å°æ‡‰æ•¸æ“šï¼ˆæˆ–åŸ·è¡Œçš„æ•¸æ“šè¡¨ï¼‰", styles['h2']))
    plot_data_csv_path = artifacts.get("executed_data_path")
    executed_df = artifacts.get("executed_dataframe_result")

    data_to_display_in_pdf = None
    if executed_df is not None and isinstance(executed_df, pd.DataFrame):
        data_to_display_in_pdf = executed_df
    elif plot_data_csv_path and os.path.exists(plot_data_csv_path) and plot_data_csv_path.endswith(".csv"):
        try:
            data_to_display_in_pdf = pd.read_csv(plot_data_csv_path)
        except Exception as e:
            story.append(Paragraph(f"âŒ ç„¡æ³•è®€å– CSVï¼š{html.escape(str(e))}", styles['Normal']))
            data_to_display_in_pdf = None

    # é¡¯ç¤ºè¡¨æ ¼
    if data_to_display_in_pdf is not None and not data_to_display_in_pdf.empty:
        data_for_table = [data_to_display_in_pdf.columns.astype(str).tolist()] + \
                         data_to_display_in_pdf.astype(str).values.tolist()

        if len(data_for_table) > 1:
            max_rows_in_pdf = 30
            if len(data_for_table) > max_rows_in_pdf:
                data_for_table = data_for_table[:max_rows_in_pdf]
                story.append(Paragraph(f"(ğŸ“Œ åƒ…é¡¯ç¤ºå‰ {max_rows_in_pdf - 1} è¡Œè³‡æ–™)", styles['Italic']))

            table = Table(data_for_table, repeatRows=1)
            table_style = [
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('WORDWRAP', (0, 0), (-1, -1), 'CJK')  # ä¸­æ–‡æ›è¡Œæ”¯æ´
            ]
            
            # å¦‚æœæœ‰ä¸­æ–‡å­—é«”å¯ç”¨ï¼Œå‰‡ä½¿ç”¨ä¸­æ–‡å­—é«”
            if chinese_font_available:
                table_style.append(('FONTNAME', (0, 0), (-1, -1), chinese_font_name))
                table_style.append(('FONTNAME', (0, 0), (-1, 0), chinese_font_name))
            else:
                table_style.append(('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'))
                
            table.setStyle(TableStyle(table_style))
            story.append(table)
        else:
            story.append(Paragraph("âš ï¸ è¡¨æ ¼åƒ…å«æ¨™é ­ï¼Œç„¡å¯é¡¯ç¤ºè³‡æ–™ã€‚", styles['Normal']))
    else:
        story.append(Paragraph("âš ï¸ ç„¡æ³•å–å¾—æˆ–é¡¯ç¤ºè¡¨æ ¼è³‡æ–™ã€‚", styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))
    story.append(PageBreak())

    # --- 5. ç”Ÿæˆçš„æ–‡å­—å ±å‘Š ---
    story.append(Paragraph("5. ç”Ÿæˆçš„æ–‡æœ¬å ±å‘Šï¼ˆç‰¹å®šåˆ†æï¼‰", styles['h2']))
    report_text_path = artifacts.get("generated_report_path")
    if report_text_path and os.path.exists(report_text_path):
        try:
            with open(report_text_path, 'r', encoding='utf-8') as f:
                report_text_content = f.read()
            report_text_content_cleaned = html.escape(report_text_content.replace("**", ""))
            for para_text in report_text_content_cleaned.split('\n'):
                story.append(Paragraph(para_text if para_text.strip() else "&nbsp;", styles['Normal']))
        except Exception as e:
            story.append(Paragraph(f"âŒ ç„¡æ³•è®€å–å ±å‘Šå…§å®¹ï¼š{html.escape(str(e))}", styles['Normal']))
    else:
        story.append(Paragraph("âš ï¸ æ‰¾ä¸åˆ°å ±å‘Šæª”æ¡ˆæˆ–è·¯å¾‘éŒ¯èª¤ã€‚", styles['Normal']))
    story.append(Spacer(1, 0.2 * inch))

    # --- 6. åˆ†æè©•è«– ---
    story.append(Paragraph("6. åˆ†æè©•è«–", styles['h2']))
    critique_text_path = artifacts.get("generated_critique_path")
    if critique_text_path and os.path.exists(critique_text_path):
        try:
            with open(critique_text_path, 'r', encoding='utf-8') as f:
                critique_text_content = f.read()
            critique_text_content_cleaned = html.escape(critique_text_content.replace("**", ""))
            for para_text in critique_text_content_cleaned.split('\n'):
                story.append(Paragraph(para_text if para_text.strip() else "&nbsp;", styles['Normal']))
        except Exception as e:
            story.append(Paragraph(f"âŒ ç„¡æ³•è®€å–è©•è«–æª”æ¡ˆï¼š{html.escape(str(e))}", styles['Normal']))
    else:
        story.append(Paragraph("âš ï¸ æ‰¾ä¸åˆ°è©•è«–æª”æ¡ˆæˆ–è·¯å¾‘éŒ¯èª¤ã€‚", styles['Normal']))

    # --- å»ºç«‹ PDF æ–‡ä»¶ ---
    try:
        # ä½¿ç”¨ try-except æ•æ‰å­—é«”ç›¸é—œéŒ¯èª¤
        doc.build(story)
        st.success(f"âœ… PDF å ±å‘Šå·²æˆåŠŸç”Ÿæˆ: {pdf_path}")
        return pdf_path
    except Exception as e:
        st.error(f"âŒ PDF ç”Ÿæˆå¤±æ•—ï¼š{e}")
        # å¦‚æœæ˜¯å­—é«”ç›¸é—œéŒ¯èª¤ï¼Œå˜—è©¦ä½¿ç”¨åŸºæœ¬å­—é«”é‡æ–°ç”Ÿæˆ
        if "font" in str(e).lower() and chinese_font_available:
            st.warning("å˜—è©¦ä½¿ç”¨åŸºæœ¬å­—é«”é‡æ–°ç”Ÿæˆ PDF...")
            try:
                # é‡ç½®æ¨£å¼ç‚ºåŸºæœ¬å­—é«”
                styles = getSampleStyleSheet()
                # é‡å»ºæ–‡æª”
                doc = SimpleDocTemplate(pdf_path)
                doc.build(story)
                st.success(f"âœ… PDF å ±å‘Šå·²ä½¿ç”¨åŸºæœ¬å­—é«”æˆåŠŸç”Ÿæˆ: {pdf_path}")
                return pdf_path
            except Exception as e2:
                st.error(f"âŒ ä½¿ç”¨åŸºæœ¬å­—é«”é‡æ–°ç”Ÿæˆ PDF ä¹Ÿå¤±æ•—ï¼š{e2}")
        return None
        return None



# --- HTML Bento å ±å‘Šç”Ÿæˆï¼ˆåŸºæ–¼ Pythonï¼‰---
def _generate_html_paragraphs(text_content):
    """å°‡ç´”æ–‡å­—å…§å®¹è½‰æ›ç‚º HTML æ®µè½ï¼Œä¸¦è½‰ç¾©ç‰¹æ®Šå­—å…ƒé¿å… HTML æ³¨å…¥"""
    if not text_content or text_content.strip() == "ç„¡å¯ç”¨å…§å®¹":
        return "<p><em>ç„¡å¯ç”¨å…§å®¹</em></p>"
    
    escaped_content = html.escape(text_content)
    paragraphs = "".join([f"<p>{line}</p>" for line in escaped_content.split('\n') if line.strip()])
    return paragraphs if paragraphs else "<p><em>æœªæä¾›å…§å®¹ã€‚</em></p>"

def generate_operation_diagnosis_pdf(data_summary, risk_summary, diagnosis_report_text, file_name="operation_diagnosis_report.pdf"):
    """
    ç‚ºé‹ç‡Ÿè¨ºæ–·å ±å‘Šç”Ÿæˆ PDF æ–‡ä»¶ã€‚
    """
    from reportlab.platypus import (
        SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle
    )
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.lib import colors
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    from reportlab.lib.pagesizes import letter  
    import io

    # è¨»å†Šä¸­æ–‡å­—é«” (é€™è£¡å¯ä»¥é‡ç”¨å·²åœ¨é ‚éƒ¨å®šç¾©çš„å­—é«”è¨»å†Šé‚è¼¯)
    chinese_font_available = False
    chinese_font_name = ''
    font_options = [
        ("MicrosoftJhengHei", "c:/windows/fonts/msjh.ttc"),
        ("MicrosoftJhengHei", "c:/windows/fonts/msjhbd.ttc"),
        ("DFKaiShu", "c:/windows/fonts/kaiu.ttf"),
        ("MingLiU", "c:/windows/fonts/mingliu.ttc"),
        ("SimSun", "c:/windows/fonts/simsun.ttc"),
        ("SimHei", "c:/windows/fonts/simhei.ttf")
    ]
    for font_name, font_path in font_options:
        try:
            pdfmetrics.registerFont(TTFont(font_name, font_path))
            chinese_font_available = True
            chinese_font_name = font_name
            break
        except Exception:
            continue

    if not chinese_font_available:
        # Fallback to a default font or show a warning
        st.warning("ç„¡æ³•è¼‰å…¥ä»»ä½•ä¸­æ–‡å­—é«”ï¼ŒPDF ä¸­çš„ä¸­æ–‡å¯èƒ½é¡¯ç¤ºä¸æ­£ç¢ºã€‚")
        default_font_name = 'Helvetica' # Fallback for PDF

    styles = getSampleStyleSheet()
    # å»ºç«‹ä¸­æ–‡æ¨£å¼
    if chinese_font_available:
        styles.add(ParagraphStyle(name='ChineseNormal', fontName=chinese_font_name, fontSize=10, leading=14))
        styles.add(ParagraphStyle(name='ChineseHeading1', fontName=chinese_font_name, fontSize=16, leading=20, spaceAfter=12))
        styles.add(ParagraphStyle(name='ChineseHeading2', fontName=chinese_font_name, fontSize=14, leading=18, spaceAfter=10))
        normal_style = styles['ChineseNormal']
        h1_style = styles['ChineseHeading1']
        h2_style = styles['ChineseHeading2']
    else:
        normal_style = styles['Normal']
        h1_style = styles['h1']
        h2_style = styles['h2']

    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=letter)
    story = []

    # Title
    story.append(Paragraph("ğŸ“Š é‹ç‡Ÿè¨ºæ–·å ±å‘Š", h1_style))
    story.append(Spacer(1, 0.2 * inch))

    # Data Summary
    story.append(Paragraph("## æ•¸æ“šæ¦‚æ³", h2_style))
    for line in data_summary.split('\n'):
        if line.strip():
            story.append(Paragraph(line.strip(), normal_style))
    story.append(Spacer(1, 0.2 * inch))

    # Risk Metrics
    story.append(Paragraph("## âš ï¸ é¢¨éšªæŒ‡æ¨™", h2_style))
    # å°‡ risk_summary è½‰æ›ç‚ºè¡¨æ ¼å½¢å¼ï¼Œæˆ–è€…ç›´æ¥æ®µè½å½¢å¼
    risk_data = [["æŒ‡æ¨™", "æ•¸å€¼"]]
    for line in risk_summary.split('\n'):
        if ':' in line:
            key, value = line.split(':', 1)
            risk_data.append([key.strip(), value.strip()])
    
    if len(risk_data) > 1:
        risk_table = Table(risk_data, colWidths=[2.5*inch, 3.5*inch])
        risk_table.setStyle(TableStyle([
            ('BACKGROUND', (0,0), (-1,0), colors.grey),
            ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),
            ('ALIGN', (0,0), (-1,-1), 'LEFT'),
            ('FONTNAME', (0,0), (-1,0), chinese_font_name if chinese_font_available else 'Helvetica-Bold'),
            ('FONTNAME', (0,1), (-1,-1), chinese_font_name if chinese_font_available else 'Helvetica'),
            ('BOTTOMPADDING', (0,0), (-1,0), 12),
            ('BACKGROUND', (0,1), (-1,-1), colors.beige),
            ('GRID', (0,0), (-1,-1), 1, colors.black),
        ]))
        story.append(risk_table)
    else:
        story.append(Paragraph("ç„¡é¢¨éšªæŒ‡æ¨™å¯ç”¨ã€‚", normal_style))
    story.append(Spacer(1, 0.2 * inch))


    # Diagnosis Report
    story.append(Paragraph("## ğŸ“‹ é‹ç‡Ÿè¨ºæ–·å ±å‘Š", h2_style))
    for line in diagnosis_report_text.split('\n'):
        if line.strip():
            story.append(Paragraph(line.strip(), normal_style))
    story.append(Spacer(1, 0.5 * inch))

    try:
        doc.build(story)
        return buffer.getvalue()
    except Exception as e:
        st.error(f"ç”Ÿæˆé‹ç‡Ÿè¨ºæ–· PDF å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None

# --- é‹ç‡Ÿè¨ºæ–·å ±å‘Š AI ç”Ÿæˆå‡½æ•¸ï¼ˆå…¨åŸŸå¯ç”¨ï¼‰---
def generate_operation_diagnosis(data_summary, risk_summary):
    """ç”Ÿæˆé‹ç‡Ÿè¨ºæ–·å ±å‘Š"""
    prompt = f"""ä½œç‚ºä¸€ä½è·¨é ˜åŸŸçš„ç­–ç•¥é¡§å•ï¼Œè«‹æ ¹æ“šä¸‹åˆ— CSV æ•¸æ“šåˆ†æçµæœï¼Œæ’°å¯«ä¸€ä»½**å…¨é¢çš„è¨ºæ–·å ±å‘Š**ï¼Œå…§å®¹é ˆçµåˆé‡åŒ–æŒ‡æ¨™èˆ‡è³ªåŒ–æ´å¯Ÿï¼Œä¸¦ä¸é™æ–¼æ¬„ä½å±¤é¢çš„æè¿°ï¼š

æ•¸æ“šæ¦‚æ³ï¼š
{data_summary}

é¢¨éšªåˆ†æï¼š
{risk_summary}

è«‹å¾ä»¥ä¸‹é¢å‘é€²è¡Œæ·±å…¥åˆ†æï¼š
1. ç‡Ÿé‹èˆ‡è²¡å‹™ç¾æ³
2. å¸‚å ´èˆ‡å®¢æˆ¶æ´å¯Ÿ
3. æŠ€è¡“èˆ‡ç³»çµ±ç¾æ³
4. ä¾›æ‡‰éˆèˆ‡æµç¨‹æ•ˆç‡
5. äººæ‰èˆ‡çµ„ç¹”è³‡æº
6. ä¸»è¦é¢¨éšªèˆ‡ç·©è§£ç­–ç•¥
7. å„ªåŒ–å»ºè­°ï¼ˆçŸ­ / ä¸­ / é•·æœŸè¡Œå‹•è¨ˆç•«ï¼‰
8. æœªä¾†ç™¼å±•æ–¹å‘èˆ‡æ©Ÿæœƒ

è«‹ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¿æŒå°ˆæ¥­ã€å®¢è§€çš„èªæ°£ã€‚"""
    try:
        response = client.chat.completions.create(
            model="gpt-4-0125-preview",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ“…é•·æ•´åˆå•†æ¥­ã€æŠ€è¡“èˆ‡ç®¡ç†è§€é»çš„ç­–ç•¥é¡§å•ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ç”Ÿæˆè¨ºæ–·å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"


def _generate_html_table(csv_text_or_df):
    """å¾ CSV å­—ä¸²æˆ– pandas DataFrame ç”Ÿæˆå¯åµŒå…¥ç¶²é çš„ HTML è¡¨æ ¼"""
    df = None

    # è‹¥ç‚º DataFrameï¼Œç›´æ¥ä½¿ç”¨
    if isinstance(csv_text_or_df, pd.DataFrame):
        df = csv_text_or_df

    # è‹¥ç‚º CSV å­—ä¸²ï¼Œå…ˆå»é™¤å‰ç¶´æç¤ºèªå¾Œè§£æç‚º DataFrame
    elif isinstance(csv_text_or_df, str):
        prefixes = [
            "Primary Table Data (CSV format for HTML table):\n",
            "Data for Chart (CSV format for Chart.js):\n",
            "DQ Column Assessment (CSV for table):\n",
            "DQ Correlation Matrix (CSV for table):\n"
        ]
        for p in prefixes:
            if csv_text_or_df.startswith(p):
                csv_text_or_df = csv_text_or_df.split(p, 1)[1]
                break

        # ç„¡æ•ˆè³‡æ–™è™•ç†
        if not csv_text_or_df.strip() or "Not available" in csv_text_or_df or "No specific data table" in csv_text_or_df:
            return "<p><em>âš ï¸ è³‡æ–™è¡¨ç„¡æ³•å–å¾—æˆ–ä¸é©ç”¨ã€‚</em></p>"
        
        try:
            csv_file = io.StringIO(csv_text_or_df)
            df = pd.read_csv(csv_file)
        except pd.errors.EmptyDataError:
            return "<p><em>âš ï¸ è¡¨æ ¼è³‡æ–™ç‚ºç©ºæˆ–ä¸æ˜¯æœ‰æ•ˆçš„ CSV æ ¼å¼ã€‚</em></p>"
        except Exception as e:
            return f"<p><em>âŒ CSV è§£æéŒ¯èª¤ï¼š{html.escape(str(e))}ã€‚</em></p>"
    else:
        return "<p><em>âŒ è³‡æ–™å‹åˆ¥éŒ¯èª¤ï¼Œç„¡æ³•ç”Ÿæˆè¡¨æ ¼ã€‚</em></p>"

    if df is None or df.empty:
        return "<p><em>âš ï¸ è¡¨æ ¼è³‡æ–™ç‚ºç©ºã€‚</em></p>"

    # è½‰æ›ç‚º HTML è¡¨æ ¼
    table_html = "<table>\n<thead>\n<tr>"
    for header in df.columns:
        table_html += f"<th>{html.escape(str(header))}</th>"
    table_html += "</tr>\n</thead>\n<tbody>\n"

    for _, row in df.iterrows():
        table_html += "<tr>"
        for cell in row:
            cell_str = str(cell)
            is_numeric = pd.api.types.is_number(cell) and not pd.isna(cell)
            class_attr = ' class="numeric"' if is_numeric else ''
            table_html += f"<td{class_attr}>{html.escape(cell_str)}</td>"
        table_html += "</tr>\n"

    table_html += "</tbody>\n</table>"
    return table_html



def _generate_html_image_embed(image_path_from_artifact):
    """åµŒå…¥åœ–ç‰‡çš„ HTMLï¼Œä¸¦æç¤ºä½¿ç”¨è€…åœ–ç‰‡éœ€èˆ‡ HTML åŒç›®éŒ„"""
    if not image_path_from_artifact or not os.path.exists(image_path_from_artifact):
        return "<p><em>âš ï¸ æ‰¾ä¸åˆ°åœ–ç‰‡æˆ–è·¯å¾‘ç„¡æ•ˆã€‚</em></p>"

    image_filename = os.path.basename(image_path_from_artifact)
    img_tag = f'<img src="{html.escape(image_filename)}" alt="è¦–è¦ºåŒ–" style="max-width: 100%; max-height: 100%; height: auto; display: block; margin: auto; border-radius: 8px;">'
    note = f'<p class="image-note"><strong>åœ–ç‰‡è¨»è§£ã€Œ{html.escape(image_filename)}ã€ï¼š</strong>è«‹å°‡æ­¤åœ–ç‰‡æ”¾åœ¨èˆ‡ HTML æª”æ¡ˆç›¸åŒçš„è³‡æ–™å¤¾ä¸­ã€‚</p>'
    return f'<div class="visualization-container">{img_tag}</div>{note}'


def _generate_chartjs_embed(csv_text_or_df, chart_id):
    """å°‡ CSV æˆ– DataFrame è½‰æ›ç‚º Chart.js åœ–è¡¨çš„ HTML + JS ç¨‹å¼ç¢¼"""
    df = None

    # CSV æˆ– DataFrame è™•ç†
    if isinstance(csv_text_or_df, pd.DataFrame):
        df = csv_text_or_df
    elif isinstance(csv_text_or_df, str):
        prefixes = [
            "Data for Chart (CSV format for Chart.js):\n",
            "DQ Numeric Dist. Example (CSV for Chart.js):\n",
            "DQ Categorical Dist. Example (CSV for Chart.js):\n"
        ]
        for p in prefixes:
            if csv_text_or_df.startswith(p):
                csv_text_or_df = csv_text_or_df.split(p, 1)[1]
                break

        if not csv_text_or_df.strip() or "Not available" in csv_text_or_df:
            return f"<div class='visualization-container'><p><em>âš ï¸ Chart.js è³‡æ–™ä¸å¯ç”¨æˆ–ä¸é©ç”¨ã€‚</em></p></div>"
        try:
            csv_file = io.StringIO(csv_text_or_df)
            df = pd.read_csv(csv_file)
        except pd.errors.EmptyDataError:
            return f"<div class='visualization-container'><p><em>âš ï¸ Chart.js è³‡æ–™ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤ã€‚</em></p></div>"
        except Exception as e:
            return f"<div class='visualization-container'><p><em>âŒ CSV è§£æéŒ¯èª¤ï¼š{html.escape(str(e))}</em></p></div>"
    else:
        return f"<div class='visualization-container'><p><em>âŒ ç„¡æ•ˆè³‡æ–™é¡å‹ï¼Œç„¡æ³•ç”Ÿæˆ Chart.js åœ–è¡¨ã€‚</em></p></div>"

    if df is None or df.empty or len(df.columns) < 2:
        return f"<div class='visualization-container'><p><em>âš ï¸ è‡³å°‘éœ€å…©æ¬„ä¸”éç©ºçš„è³‡æ–™æ‰èƒ½ç”Ÿæˆåœ–è¡¨ã€‚</em></p></div>"

    # æŠ½å– labels èˆ‡æ•¸å€¼è³‡æ–™
    labels = df.iloc[:, 0].astype(str).tolist()
    data_values = df.iloc[:, 1].tolist()

    numeric_data_values = []
    for val in data_values:
        try:
            numeric_data_values.append(float(val))
        except (ValueError, TypeError):
            return f"<div class='visualization-container'><p><em>âŒ ç¬¬äºŒæ¬„æ‡‰ç‚ºæ•¸å€¼ï¼Œç™¼ç¾éæ•¸å€¼ï¼š'{html.escape(str(val))}'</em></p></div>"

    chart_type = 'bar'
    chart_label = html.escape(str(df.columns[1]))
    if all("-" in str(l) for l in labels[:3]) and len(labels) > 1:
        chart_label = "é »ç‡åˆ†ä½ˆ"

    # HTML èˆ‡ JavaScript çµåˆ Chart.js
    canvas_html = f'<div class="visualization-container"><canvas id="{html.escape(chart_id)}"></canvas></div>'
    script_html = f"""
<script>
    const ctx_{html.escape(chart_id)} = document.getElementById('{html.escape(chart_id)}').getContext('2d');
    new Chart(ctx_{html.escape(chart_id)}, {{
        type: '{chart_type}',
        data: {{
            labels: {json.dumps(labels)},
            datasets: [{{
                label: '{chart_label}',
                data: {json.dumps(numeric_data_values)},
                backgroundColor: 'rgba(199, 120, 221, 0.6)',
                borderColor: 'rgba(74, 35, 90, 1)',
                borderWidth: 1
            }}]
        }},
        options: {{
            responsive: true,
            maintainAspectRatio: false,
            scales: {{
                y: {{
                    beginAtZero: true,
                    ticks: {{ color: '#E0E0E0', font: {{ family: "Inter" }} }},
                    grid: {{ color: '#3A3D4E' }}
                }},
                x: {{
                    ticks: {{ color: '#E0E0E0', font: {{ family: "Inter" }} }},
                    grid: {{ color: '#3A3D4E' }}
                }}
            }},
            plugins: {{
                legend: {{
                    labels: {{ color: '#E0E0E0', font: {{ family: "Inter" }} }}
                }},
                tooltip: {{
                    backgroundColor: '#2A2D3E',
                    titleColor: '#FFFFFF',
                    bodyColor: '#FFFFFF',
                    borderColor: '#7DF9FF',
                    borderWidth: 1,
                    titleFont: {{ family: "Inter" }},
                    bodyFont: {{ family: "Inter" }}
                }}
            }}
        }}
    }});
</script>
"""
    return canvas_html + script_html


def generate_bento_html_report_python(artifacts, cdo_initial_report, data_summary_dict, main_df):
    """
    ä½¿ç”¨ Python å­—ä¸²æ ¼å¼åŒ–çš„æ–¹å¼ï¼Œç”¢ç”Ÿ Bento æ ¼å¼çš„ HTML åˆ†æå ±å‘Šã€‚

    å ±å‘Šå…§å®¹åŒ…å«ï¼š
    - æ•¸æ“šå“è³ªå„€è¡¨æ¿å€å¡Šï¼ˆData Quality Dashboardï¼‰
    - CDO åˆæ­¥å ±å‘Šæ–‡å­—
    - ä½¿ç”¨è€…æä¾›çš„æ¬„ä½èªªæ˜ï¼ˆè‹¥æœ‰ï¼‰
    - åœ–è¡¨èˆ‡è³‡æ–™è¡¨çš„åµŒå…¥å€å¡Šï¼ˆæ ¹æ“š artifacts è‡ªå‹•è¼‰å…¥ï¼‰
    
    åƒæ•¸ï¼š
    artifacts (dict): å­˜æ”¾åœ–è¡¨è·¯å¾‘ã€å ±å‘Šæ–‡å­—ç­‰åˆ†ææˆå“çš„å­—å…¸ã€‚
    cdo_initial_report (str): CDO æä¾›çš„åˆå§‹æ•¸æ“šåˆ†ææ–‡å­—ã€‚
    data_summary_dict (dict): æ•´ç†å¾Œçš„è³‡æ–™æ‘˜è¦ï¼ŒåŒ…å«è¡Œåˆ—æ•¸ã€ç¼ºå¤±å€¼ã€æ¬„ä½æè¿°ç­‰ã€‚
    main_df (DataFrame): ä¸»è¦çš„åŸå§‹è³‡æ–™è¡¨ï¼Œç”¨æ–¼è³‡æ–™å“è³ªæª¢æŸ¥èˆ‡é¡¯ç¤ºã€‚

    å›å‚³ï¼š
    str: åŒ…å«æ‰€æœ‰çµ„ä»¶çš„ HTML å­—ä¸²ã€‚
    """

    # å‚³å…¥ä¸»è³‡æ–™è¡¨ main_dfï¼Œè®“ compile_report_text_for_html_generation å¯æ ¹æ“šå…¶å…§å®¹ç”Ÿæˆ DQ åˆ†æå€æ®µ
    report_parts_dict = compile_report_text_for_html_generation(
        artifacts, 
        cdo_initial_report, 
        data_summary_dict, 
        main_df
    )


    html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI ç”Ÿæˆçš„ Bento å ±å‘Š - {html.escape(artifacts.get("original_user_query", "åˆ†æ")[:50])}</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {{ font-family: 'Inter', 'å¾®è»Ÿæ­£é»‘é«”', sans-serif; background-color: #1A1B26; color: #E0E0E0; margin: 0; padding: 20px; line-height: 1.6; }}
        .bento-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(320px, 1fr)); gap: 1.5rem; max-width: 1600px; margin: 20px auto; padding:0; }}
        .bento-item {{ 
            background-color: #2A2D3E; 
            border: 1px solid #3A3D4E; 
            border-radius: 16px; 
            padding: 25px; 
            transition: transform 0.3s ease, border-color 0.3s ease, box-shadow 0.3s ease;
            overflow-wrap: break-word; 
            word-wrap: break-word; 
            overflow: hidden; /* é˜²æ­¢å…§å®¹æº¢å‡º */
            display: flex; /* ç‚ºäº†æ›´å¥½çš„å…§éƒ¨å°é½Š */
            flex-direction: column; /* å †ç–Šæ¨™é¡Œå’Œå…§å®¹ */
        }}
        .bento-item:hover {{ 
            transform: translateY(-5px); 
            border-color: #7DF9FF; 
            box-shadow: 0 10px 20px rgba(0,0,0,0.25);
        }}
        .bento-item h2 {{ 
            color: #FFFFFF; 
            font-size: 1.5rem; /* ç•¥å¾®ç¸®å°ä»¥ä¿æŒå¹³è¡¡ */
            font-weight: 600;
            border-bottom: 2px solid #7DF9FF; 
            padding-bottom: 10px; 
            margin-top:0; 
            margin-bottom: 15px; /* ä¿æŒä¸€è‡´çš„é–“è· */
        }}
        .bento-item .content-wrapper {{ flex-grow: 1; overflow-y: auto; }} /* å…è¨±å…§å®¹åœ¨éé•·æ™‚æ»¾å‹• */
        .bento-item p {{ color: #C0C0C0; margin-bottom: 10px; }}
        .bento-item p:last-child {{ margin-bottom: 0; }}
        .bento-item strong {{ color: #7DF9FF; font-weight: 600; }}
        .bento-item ul {{ list-style-position: inside; padding-left: 5px; color: #C0C0C0; margin-bottom:10px; }}
        .bento-item li {{ margin-bottom: 6px; }}

        /* ä¸åŒå±å¹•å°ºå¯¸çš„è·¨åº¦è¦å‰‡ */
        .bento-item.large {{ grid-column: span 1; }} /* å°å±å¹•çš„é è¨­å€¼ */
        @media (min-width: 768px) {{ /* å¹³æ¿é›»è…¦åŠä»¥ä¸Š */
            .bento-item.large {{ grid-column: span 2; }}
        }}
        /* å°æ–¼éå¸¸å¯¬çš„å±å¹•ï¼Œå¯ä»¥è€ƒæ…®è®“æŸäº›é …ç›®è·¨è¶Š 3 æ ¼ï¼ˆå¦‚æœéœ€è¦ï¼‰ */
        @media (min-width: 1200px) {{ 
             .bento-grid {{ grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); }}
        }}

        .bento-item.accent-box {{ background-color: #4A235A; border-color: #C778DD; }}
        .bento-item.accent-box h2 {{ border-bottom-color: #C778DD; }}
        .bento-item.accent-box strong {{ color: #FFA6FC; }}


        table {{ width: 100%; border-collapse: collapse; margin-top: 15px; table-layout: auto; /* æ”¹ç‚ºè‡ªå‹•ä»¥ç²å¾—æ›´å¥½çš„é©é… */ }}
        th, td {{ border: 1px solid #4A4D5E; padding: 10px; text-align: left; word-wrap: break-word; font-size: 0.9rem; }}
        th {{ background-color: #3A3D4E; color: #FFFFFF; font-weight: 600; }}
        td.numeric {{ text-align: right; color: #7DF9FF; }}

        .visualization-container {{ 
            min-height: 300px; /* èª¿æ•´æœ€å°é«˜åº¦ */
            max-height: 400px; /* èª¿æ•´æœ€å¤§é«˜åº¦ */
            min-height: 300px; /* Adjusted min-height */
            max-height: 400px; /* Adjusted max-height */
            width: 100%; 
            display: flex; 
            align-items: center; 
            justify-content: center; 
            overflow: hidden; 
            background-color: #202230; 
            border-radius: 8px; 
            padding:10px; 
            margin-bottom: 10px; /* Space before image note */
        }}
        .visualization-container img, .visualization-container canvas {{ 
            max-width: 100%; 
            max-height: 100%; 
            object-fit: contain; /* Ensures entire image/chart is visible */
            display: block; 
            margin: auto; 
            border-radius: 8px;
        }}
        .image-note {{ font-size: 0.8rem; color: #A0A0A0; text-align: center; margin-top: 5px; }}
        .placeholder-text {{ color: #888; font-style: italic; }}
        .user-descriptions {{ border-left: 3px solid #7DF9FF; padding-left: 15px; margin-top:10px; background-color: rgba(125, 249, 255, 0.05); border-radius: 4px; }}
        .user-descriptions p {{ color: #D0D0D0; }}
    </style>
</head>
<body>
    <div class="bento-grid">
"""

   # æŒ‡å®šå ±å‘Šå„éƒ¨åˆ†çš„å‘ˆç¾é †åºï¼ˆç§»é™¤ä¸å†ä½¿ç”¨çš„åœ–è¡¨éƒ¨åˆ†ï¼‰
    ordered_keys = [
        "ANALYSIS_GOAL",                         # åˆ†æç›®æ¨™
        "DATA_SNAPSHOT_CDO_REPORT",              # CDO æ•¸æ“šæ‘˜è¦å ±å‘Š
        "USER_PROVIDED_DESCRIPTIONS",            # ç”¨æˆ¶æ¬„ä½èªªæ˜
        "KEY_DATA_QUALITY_ALERT",                # é‡è¦æ•¸æ“šå“è³ªè­¦å‘Š
        "DATA_PREPROCESSING_NOTE",               # æ•¸æ“šå‰è™•ç†èªªæ˜
        "DQ_COLUMN_ASSESSMENT_TABLE",            # æ¬„ä½æ•¸æ“šå“è³ªè©•ä¼°
        "DQ_CORRELATION_MATRIX_TABLE",           # æ•¸æ“šç›¸é—œæ€§çŸ©é™£
        "VISUALIZATION_CHART_OR_IMAGE",          # ä¸»è¦è¦–è¦ºåœ–è¡¨
        "PRIMARY_ANALYSIS_TABLE",                # ä¸»è¦åˆ†æçµæœè¡¨æ ¼
        "ACTIONABLE_INSIGHTS_FROM_REPORT",       # å¯è¡Œå»ºè­°èˆ‡æ´å¯Ÿ
        "CRITIQUE_SUMMARY",                      # æ‰¹åˆ¤æ‘˜è¦
        "SUGGESTIONS_FOR_ENHANCED_ANALYSIS"      # æ·±åŒ–åˆ†æçš„å»ºè­°
    ]

    # å°æ¯ä¸€å€å¡Šä¾åºé€²è¡Œ HTML çµ„è£
    for key_index, key in enumerate(ordered_keys):
        if key not in report_parts_dict:
            continue  # è‹¥æŸå€å¡Šç¼ºå°‘ï¼Œç•¥é

        # æ¯å€å¡Šçš„æ¨™é¡Œå°ç…§è¡¨ï¼ˆè‹±æ–‡è½‰ä¸­æ–‡å¯åƒè€ƒä¸‹æ–¹ï¼‰
        title_map = {
            "ANALYSIS_GOAL": "åˆ†æç›®æ¨™",
            "DATA_SNAPSHOT_CDO_REPORT": "CDO æ•¸æ“šæ‘˜è¦å ±å‘Š",
            "USER_PROVIDED_DESCRIPTIONS": "ç”¨æˆ¶æä¾›çš„æ¬„ä½èªªæ˜",
            "KEY_DATA_QUALITY_ALERT": "é—œéµæ•¸æ“šå“è³ªè­¦ç¤º",
            "DATA_PREPROCESSING_NOTE": "è³‡æ–™å‰è™•ç†èªªæ˜",
            "DQ_COLUMN_ASSESSMENT_TABLE": "æ•¸æ“šå“è³ªï¼šæ¬„ä½è©•ä¼°è¡¨",
            "DQ_CORRELATION_MATRIX_TABLE": "æ•¸æ“šå“è³ªï¼šç›¸é—œæ€§çŸ©é™£",
            "VISUALIZATION_CHART_OR_IMAGE": "ä¸»è¦è¦–è¦ºåŒ–åœ–è¡¨",
            "PRIMARY_ANALYSIS_TABLE": "ä¸»è¦åˆ†æè¡¨æ ¼",
            "ACTIONABLE_INSIGHTS_FROM_REPORT": "å¯è¡Œæ´å¯Ÿå»ºè­°",
            "CRITIQUE_SUMMARY": "æ‰¹åˆ¤ç¸½çµ",
            "SUGGESTIONS_FOR_ENHANCED_ANALYSIS": "å¼·åŒ–åˆ†æå»ºè­°"
        }

        title = title_map.get(key, key.replace('_', ' ').title())  # è‹¥ç„¡å°æ‡‰å‰‡è½‰ç‚ºæ¨™é¡Œæ ¼å¼
        raw_content_obj = report_parts_dict.get(key)

        item_classes = ["bento-item"]
        item_content_html = ""

        # åˆ¤æ–·å“ªäº›å€å¡Šéœ€è¦ä½¿ç”¨å¤§å€å¡Šç‰ˆé¢ï¼ˆ.large æ¨£å¼ï¼‰
        large_keys = [
            "KEY_DATA_QUALITY_ALERT", "VISUALIZATION_CHART_OR_IMAGE", "PRIMARY_ANALYSIS_TABLE",
            "DQ_COLUMN_ASSESSMENT_TABLE", "DQ_CORRELATION_MATRIX_TABLE", "ACTIONABLE_INSIGHTS_FROM_REPORT",
            "DATA_SNAPSHOT_CDO_REPORT", "USER_PROVIDED_DESCRIPTIONS"
        ]
        if key in large_keys:
            item_classes.append("large")

        # ç‰¹æ®Šè™•ç†ï¼šã€Œæ•¸æ“šå“è³ªè­¦å‘Šã€ä»¥æ¢åˆ—é¡¯ç¤º
        if key == "KEY_DATA_QUALITY_ALERT":
            item_classes.append("accent-box")  # é¡¯çœ¼æ¨£å¼
            if isinstance(raw_content_obj, str) and raw_content_obj.strip():
                alert_lines = [f"<li>{html.escape(line.strip())}</li>" for line in raw_content_obj.split('\n') if line.strip().startswith("- ")]
                if alert_lines:
                    item_content_html = f"<ul>{''.join(alert_lines)}</ul>"
                else:
                    item_content_html = _generate_html_paragraphs(raw_content_obj)
            else:
                item_content_html = "<p><em>å°šæœªæ¨™è¨»å…·é«”çš„æ•¸æ“šå“è³ªè­¦å‘Šï¼Œå¯èƒ½å·²æ•´åˆåœ¨ CDO å ±å‘Šä¸­ã€‚</em></p>"

        # ç”¨æˆ¶æä¾›çš„æ¬„ä½èªªæ˜
        elif key == "USER_PROVIDED_DESCRIPTIONS":
            if isinstance(raw_content_obj, str) and raw_content_obj.strip() and raw_content_obj != "User descriptions not provided.":
                item_content_html = f"<div class='user-descriptions'>{_generate_html_paragraphs(raw_content_obj)}</div>"
            else:
                item_content_html = "<p><em>ä½¿ç”¨è€…æœªæä¾›é¡å¤–æ¬„ä½èªªæ˜ã€‚</em></p>"

        # åœ–ç‰‡æˆ– Chart.js åœ–è¡¨
        elif key == "VISUALIZATION_CHART_OR_IMAGE":
            plot_image_path = artifacts.get("plot_image_path")
            executed_data_for_chart = raw_content_obj
            if plot_image_path and os.path.exists(plot_image_path):
                item_content_html = _generate_html_image_embed(plot_image_path)
            elif isinstance(executed_data_for_chart, pd.DataFrame) and not executed_data_for_chart.empty:
                item_content_html = _generate_chartjs_embed(executed_data_for_chart, f"bentoChartAiAnalysis{key_index}")
            else:
                item_content_html = "<p><em>æœªæä¾›åœ–è¡¨æˆ–è³‡æ–™ç„¡æ³•ç”¨æ–¼å¯è¦–åŒ–ã€‚</em></p>"

        # ä¸‰ç¨®è¡¨æ ¼è™•ç†ï¼ˆæ¬„ä½å“è³ªã€ç›¸é—œçŸ©é™£ã€ä¸»è¦åˆ†æï¼‰
        elif key in ["PRIMARY_ANALYSIS_TABLE", "DQ_COLUMN_ASSESSMENT_TABLE", "DQ_CORRELATION_MATRIX_TABLE"]:
            if isinstance(raw_content_obj, pd.DataFrame):
                item_content_html = _generate_html_table(raw_content_obj)
            else:
                item_content_html = _generate_html_paragraphs(str(raw_content_obj))

        # å…¶ä»–ç´”æ–‡å­—å…§å®¹
        elif isinstance(raw_content_obj, str):
            item_content_html = _generate_html_paragraphs(raw_content_obj)
        else:
            item_content_html = "<p><em>æ­¤å€å¡Šçš„å…§å®¹æ ¼å¼ç„¡æ³•è¾¨è­˜ã€‚</em></p>"

        # HTML çµ„åˆå€å¡Š
        html_content += f'<div class="{" ".join(item_classes)}">\n'
        html_content += f'<h2>{html.escape(title)}</h2>\n'
        html_content += f'<div class="content-wrapper">{item_content_html}</div>\n'
        html_content += '</div>\n'

    # æ”¶å°¾ HTML
    html_content += """
    </div>
</body>
</html>
"""

    # è¨­å®šæª”åï¼šåŒ…å«æŸ¥è©¢é—œéµå­—èˆ‡æ™‚é–“æˆ³è¨˜
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_query_part = "".join(c if c.isalnum() else "_" for c in artifacts.get("original_user_query", "report")[:30])
    html_filename = f"bento_report_{safe_query_part}_{timestamp}.html"
    html_filepath = os.path.join(TEMP_DATA_STORAGE, html_filename)

    # å¯«å…¥ HTML æª”æ¡ˆ
    try:
        with open(html_filepath, "w", encoding='utf-8') as f:
            f.write(html_content)
        return html_filepath
    except Exception as e:
        st.error(f"å¯«å…¥ HTML å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None



def get_content_from_path_helper(file_path, default_message="Not available."):
    """
    å®‰å…¨åœ°è®€å–æŒ‡å®šè·¯å¾‘çš„æª”æ¡ˆå…§å®¹ã€‚
    å›å‚³æª”æ¡ˆæ–‡å­—å…§å®¹ï¼›è‹¥æª”æ¡ˆä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—ï¼Œå‰‡å›å‚³é è¨­è¨Šæ¯ã€‚
    """
    if file_path and os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            return f"è®€å–æª”æ¡ˆéŒ¯èª¤ï¼š{str(e)}"
    return default_message  # é è¨­è¨Šæ¯



def compile_report_text_for_html_generation(artifacts, cdo_initial_report, data_summary_dict, main_df=None):
    """
    å°‡æ‰€æœ‰å ±å‘Šç›¸é—œæ–‡å­—èˆ‡è³‡æ–™æ•´ç†ç‚ºä¸€å€‹å­—å…¸ï¼Œä¾› HTML ç”Ÿæˆä½¿ç”¨ã€‚
    åŒ…å« AI åˆ†æè¡¨æ ¼ã€è¦–è¦ºåœ–è¡¨ã€ä½¿ç”¨è€…æè¿°èˆ‡æ•¸æ“šå“è³ªå„€è¡¨æ¿ç­‰å…§å®¹ã€‚
    """
    # å–å¾— AI æ‰¹åˆ¤æ‘˜è¦èˆ‡åˆ†æå ±å‘Š
    critique_text = get_content_from_path_helper(
        artifacts.get("generated_critique_path"),
        "ç„¡æ³•å–å¾—æ‰¹åˆ¤å…§å®¹ã€‚"
    )
    generated_report_text = get_content_from_path_helper(
        artifacts.get("generated_report_path"),
        "ç„¡æ³•å–å¾—ç”Ÿæˆçš„æ–‡å­—å ±å‘Šã€‚"
    )

    # ğŸ” ä¸»è¦åˆ†æè¡¨æ ¼
    primary_analysis_table_obj = "AI åˆ†æå°šæœªç”¢ç”Ÿä¸»è¦è¡¨æ ¼è³‡æ–™ã€‚"
    if isinstance(artifacts.get("executed_dataframe_result"), pd.DataFrame):
        primary_analysis_table_obj = artifacts["executed_dataframe_result"]
    elif artifacts.get("executed_data_path") and "table_result" in os.path.basename(artifacts["executed_data_path"]):
        try:
            primary_analysis_table_obj = pd.read_csv(artifacts["executed_data_path"])
        except:
            primary_analysis_table_obj = get_content_from_path_helper(artifacts["executed_data_path"])

    # ğŸ“Š åœ–è¡¨è³‡æ–™ï¼ˆChart.js æˆ–åœ–ç‰‡ï¼‰
    visualization_data_obj = "AI æœªæä¾›åœ–è¡¨è³‡æ–™ã€‚"
    if isinstance(artifacts.get("plot_specific_data_df"), pd.DataFrame):
        visualization_data_obj = artifacts["plot_specific_data_df"]
    elif artifacts.get("executed_data_path") and "plot_data_for" in os.path.basename(artifacts["executed_data_path"]):
        try:
            visualization_data_obj = pd.read_csv(artifacts["executed_data_path"])
        except:
            visualization_data_obj = get_content_from_path_helper(artifacts["executed_data_path"])

    # ğŸ“˜ çµ„åˆå ±å‘Šå„éƒ¨åˆ†
    report_parts = {
        "ANALYSIS_GOAL": artifacts.get("original_user_query", "æœªæŒ‡å®šåˆ†æç›®æ¨™"),
        "DATA_SNAPSHOT_CDO_REPORT": cdo_initial_report if cdo_initial_report else "ç„¡æ³•å–å¾— CDO æ•¸æ“šæ‘˜è¦ã€‚",
        "USER_PROVIDED_DESCRIPTIONS": data_summary_dict.get("user_provided_column_descriptions", "ä½¿ç”¨è€…æœªæä¾›æ¬„ä½èªªæ˜"),
        "KEY_DATA_QUALITY_ALERT": "",  # ä¸‹é¢æœƒè‡ªå‹•ç”¢å‡º
        "DATA_PREPROCESSING_NOTE": "è³‡æ–™ä½¿ç”¨åŸå§‹æ ¼å¼é€²è¡Œåˆ†æï¼Œè‹¥æœ‰å‰è™•ç†ï¼Œå·²åŒ…å«æ–¼æŸ¥è©¢ä¸­æˆ– CDO å ±å‘Šå…§ã€‚å·²åŸ·è¡Œæ¨™æº–æ ¼å¼åˆ¤æ–·èˆ‡è¼‰å…¥ã€‚",
        "VISUALIZATION_CHART_OR_IMAGE": visualization_data_obj,
        "PRIMARY_ANALYSIS_TABLE": primary_analysis_table_obj,
        "ACTIONABLE_INSIGHTS_FROM_REPORT": generated_report_text,
        "CRITIQUE_SUMMARY": critique_text,
        "SUGGESTIONS_FOR_ENHANCED_ANALYSIS": critique_text  # ç›®å‰ä½¿ç”¨ç›¸åŒå…§å®¹ï¼Œå¯æ”¹ç‚ºå‘¼å«å¦ä¸€æ¨¡å‹
    }

    # ğŸš¨ ç”¢å‡ºç¼ºæ¼å€¼è­¦ç¤ºæ–‡å­—
    missing_data_alerts_text = []
    if data_summary_dict and data_summary_dict.get("missing_values_per_column"):
        for col, count in data_summary_dict["missing_values_per_column"].items():
            if count > 0:
                num_rows = data_summary_dict.get("num_rows", 1)
                percentage = (count / num_rows) * 100 if num_rows > 0 else 0
                missing_data_alerts_text.append(
                    f"- æ¬„ä½ '{html.escape(col)}'ï¼šå…± {count} ç­†ç¼ºæ¼å€¼ï¼ˆ{percentage:.2f}%ï¼‰"
                )
    if not missing_data_alerts_text:
        missing_data_alerts_text.append(
            "- è‡ªå‹•æ‘˜è¦ä¸­æœªç™¼ç¾æ˜é¡¯ç¼ºæ¼å€¼ï¼Œæˆ–æ•¸æ“šå“è³ªå•é¡Œå·²æ•´åˆåœ¨ CDO å ±å‘Šä¸­ã€‚"
        )
    report_parts["KEY_DATA_QUALITY_ALERT"] = "\n".join(missing_data_alerts_text)

    # ğŸ“Š åŠ å…¥è³‡æ–™å“è³ªå ±è¡¨ï¼ˆæ¬„ä½è©•ä¼°ã€ç›´æ–¹åœ–ã€åˆ†é¡åˆ†ä½ˆã€ç›¸é—œçŸ©é™£ï¼‰
    if main_df is not None and not main_df.empty:
        quality_assessment_df = get_column_quality_assessment(main_df.copy())
        report_parts["DQ_COLUMN_ASSESSMENT_TABLE"] = (
            quality_assessment_df if not quality_assessment_df.empty else "æ¬„ä½è©•ä¼°ç„¡è³‡æ–™"
        )

        # æ•¸å€¼æ¬„ä½çš„åˆ†ä½ˆåœ–è¡¨
        numeric_cols_dq = main_df.select_dtypes(include=np.number).columns.tolist()
        if numeric_cols_dq:
            first_numeric_col = numeric_cols_dq[0]
            col_data_numeric = main_df[first_numeric_col].dropna()
            if not col_data_numeric.empty:
                counts, bins = np.histogram(col_data_numeric, bins=10)
                hist_df = pd.DataFrame({
                    'Bin': [f"{bins[i]:.1f}-{bins[i + 1]:.1f}" for i in range(len(bins) - 1)],
                    'Frequency': counts
                })
                report_parts["DQ_NUMERIC_DIST_CHART"] = hist_df
            else:
                report_parts["DQ_NUMERIC_DIST_CHART"] = f"æ¬„ä½ '{html.escape(first_numeric_col)}' ç„¡è¶³å¤ è³‡æ–™ç”¢ç”Ÿåˆ†ä½ˆåœ–ã€‚"
        else:
            report_parts["DQ_NUMERIC_DIST_CHART"] = "è³‡æ–™ä¸­æœªåŒ…å«æ•¸å€¼æ¬„ä½ã€‚"

        # é¡åˆ¥æ¬„ä½çš„åˆ†ä½ˆé•·æ¢åœ–
        categorical_cols_dq = main_df.select_dtypes(include=['object', 'category']).columns.tolist()
        if categorical_cols_dq:
            first_cat_col = categorical_cols_dq[0]
            col_data_cat = main_df[first_cat_col].dropna()
            if not col_data_cat.empty:
                cat_counts = col_data_cat.value_counts().head(10)
                cat_df = cat_counts.reset_index()
                cat_df.columns = [first_cat_col, 'Count']
                report_parts["DQ_CATEGORICAL_DIST_CHART"] = cat_df
            else:
                report_parts["DQ_CATEGORICAL_DIST_CHART"] = f"æ¬„ä½ '{html.escape(first_cat_col)}' ç„¡è³‡æ–™å¯è£½åœ–ã€‚"
        else:
            report_parts["DQ_CATEGORICAL_DIST_CHART"] = "è³‡æ–™ä¸­æœªåŒ…å«é¡åˆ¥æ¬„ä½ã€‚"

        # æ•¸å€¼æ¬„ä½ç›¸é—œä¿‚æ•¸çŸ©é™£
        if len(numeric_cols_dq) >= 2:
            corr_matrix = main_df[numeric_cols_dq].corr().round(2)
            report_parts["DQ_CORRELATION_MATRIX_TABLE"] = corr_matrix
        else:
            report_parts["DQ_CORRELATION_MATRIX_TABLE"] = "æ•¸å€¼æ¬„ä½ä¸è¶³ï¼Œç„¡æ³•ç”¢ç”Ÿç›¸é—œä¿‚æ•¸çŸ©é™£ã€‚"
    else:
        # è‹¥ä¸»è³‡æ–™æœªæä¾›
        default_msg = "ä¸»è³‡æ–™ç¼ºå¤±ï¼Œç„¡æ³•ç”Ÿæˆæ­¤å€å¡Šã€‚"
        report_parts["DQ_COLUMN_ASSESSMENT_TABLE"] = default_msg
        report_parts["DQ_NUMERIC_DIST_CHART"] = default_msg
        report_parts["DQ_CATEGORICAL_DIST_CHART"] = default_msg
        report_parts["DQ_CORRELATION_MATRIX_TABLE"] = default_msg

    return report_parts



# --- Streamlit App UI ---
st.title("ğŸ¤– éŠ·è²¨æç›Šåˆ†æå°å¹«æ‰‹")  
# è¨­å®šæ‡‰ç”¨ç¨‹å¼æ¨™é¡Œï¼Œé¡¯ç¤ºåœ¨ç¶²é æœ€ä¸Šæ–¹

st.caption(
    "ä¸Šå‚³ CSV å’Œå¯é¸åˆ—æè¿° (.txt)ï¼Œå¯©æŸ¥è³‡æ–™è³ªé‡ï¼Œé€²è¡Œæ¢ç´¢ï¼Œç„¶å¾Œé¸æ“‡æ€§åœ°åŸ·è¡Œ CDO å·¥ä½œæµç¨‹é€²è¡Œ AI åˆ†æã€‚"
)
# é¡¯ç¤ºå‰¯æ¨™èªªæ˜ï¼Œå¼•å°ä½¿ç”¨è€…å¦‚ä½•ä½¿ç”¨æ­¤åˆ†æå°å¹«æ‰‹



# åˆå§‹åŒ– Session State ç‹€æ…‹è®Šæ•¸
# åŸºæœ¬æ‡‰ç”¨ç‹€æ…‹ç®¡ç†
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant",
                                  "content": "æ‚¨å¥½ï¼é¸æ“‡æ¨¡å‹ï¼Œä¸Šå‚³ CSV æª”æ¡ˆï¼ˆä¹Ÿå¯ä»¥ä¸Šå‚³åŒ…å«åˆ—æè¿°çš„ .txt æª”æ¡ˆï¼‰å³å¯é–‹å§‹ã€‚"}]
# åˆå§‹å°è©±è¨Šæ¯ï¼Œæä¾›æ­¡è¿æç¤º

if "current_dataframe" not in st.session_state:
    st.session_state.current_dataframe = None
# ä½¿ç”¨è€…ä¸Šå‚³çš„ CSV æª”æ¡ˆæ‰€è½‰æ›çš„ DataFrame å°‡å„²å­˜åœ¨æ­¤è®Šæ•¸

if "data_summary" not in st.session_state:
    st.session_state.data_summary = None
# å„²å­˜è³‡æ–™çš„ç¶œåˆæ‘˜è¦ï¼ˆæ¬„ä½çµ±è¨ˆã€ç¼ºå€¼è³‡è¨Šç­‰ï¼‰

if "data_source_name" not in st.session_state:
    st.session_state.data_source_name = None
# å„²å­˜ä¸Šå‚³çš„ CSV æª”æ¡ˆåç¨±

if "desc_file_name" not in st.session_state:
    st.session_state.desc_file_name = None
# å„²å­˜ä½¿ç”¨è€…ä¸Šå‚³çš„åˆ—æè¿°ï¼ˆTXTï¼‰æª”æ¡ˆåç¨±

if "current_analysis_artifacts" not in st.session_state:
    st.session_state.current_analysis_artifacts = {}
# ç”¨ä¾†å„²å­˜æ•´é«”åˆ†æçµæœï¼ˆåŒ…å«åœ–è¡¨è·¯å¾‘ã€å ±å‘Šå…§å®¹ç­‰ï¼‰çš„å­—å…¸



# æ¨¡å‹é¸æ“‡ç‹€æ…‹
# å·¥ä½œæ¨¡å‹ï¼ˆåŸ·è¡Œåˆ†æçš„ AI æ¨¡å‹ï¼‰
if "selected_worker_model" not in st.session_state:
    st.session_state.selected_worker_model = DEFAULT_WORKER_MODEL

# è©•ä¼°æ¨¡å‹ï¼ˆç”¨ä¾†è©•ä¼°åˆ†æå“è³ªï¼‰
if "selected_judge_model" not in st.session_state:
    st.session_state.selected_judge_model = DEFAULT_JUDGE_MODEL



# LangChain è¨˜æ†¶æ¨¡çµ„ï¼ˆå¯é¸ï¼‰
# LangChain è¨˜æ†¶é«”ï¼Œç´€éŒ„èŠå¤©æ­·ç¨‹ï¼Œè®“æ¨¡å‹èƒ½è¨˜ä½ä½¿ç”¨è€…æå•è„ˆçµ¡
if "lc_memory" not in st.session_state:
    st.session_state.lc_memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=False,
        input_key="user_query"
    )

# CDO å·¥ä½œæµç¨‹ç‹€æ…‹ç®¡ç†ï¼ˆåˆ†æçš„ä¸åŒéšæ®µï¼‰
if "cdo_initial_report_text" not in st.session_state:
    st.session_state.cdo_initial_report_text = None
# å„²å­˜ç”± CDO ç”¢ç”Ÿçš„åˆå§‹å ±å‘Šæ–‡å­—

if "other_perspectives_text" not in st.session_state:
    st.session_state.other_perspectives_text = None
# å„²å­˜å…¶ä»– AI åˆ†æè¦–è§’

if "strategy_text" not in st.session_state:
    st.session_state.strategy_text = None
# å„²å­˜ AI çµ¦å‡ºçš„å•†æ¥­ç­–ç•¥å»ºè­°

if "cdo_workflow_stage" not in st.session_state:
    st.session_state.cdo_workflow_stage = None
# è¿½è¹¤ç›®å‰åˆ†ææµç¨‹é€²è¡Œåˆ°å“ªä¸€å€‹éšæ®µ



# åˆ†æè§¸ç™¼æ——æ¨™ï¼ˆä¾›å¾Œç«¯ç¨‹å¼åˆ¤æ–·æ˜¯å¦åŸ·è¡Œï¼‰
if "trigger_code_generation" not in st.session_state:
    st.session_state.trigger_code_generation = False
# æ˜¯å¦å•Ÿå‹•ç¨‹å¼ç¢¼ç”Ÿæˆï¼ˆå¯èƒ½æ˜¯ç”¨ LLM æ ¹æ“šåˆ†æç”¢ç”Ÿç¨‹å¼ç¢¼ï¼‰

if "trigger_report_generation" not in st.session_state:
    st.session_state.trigger_report_generation = False
# æ˜¯å¦è§¸ç™¼ç”¢ç”Ÿåˆ†æå ±å‘Šï¼ˆPDFï¼‰

if "trigger_judging" not in st.session_state:
    st.session_state.trigger_judging = False
# æ˜¯å¦è§¸ç™¼ AI å°çµæœé€²è¡Œåˆ¤æ–·ï¼ˆè‡ªè©•æˆ–å¤šæ¨¡å‹å¯©æ ¸ï¼‰

if "trigger_html_export" not in st.session_state:
    st.session_state.trigger_html_export = False
# æ˜¯å¦è§¸ç™¼ç”¢å‡º HTML ç‰ˆæœ¬å ±å‘Š



# ç”¢å‡ºå ±å‘Šæ™‚ä½¿ç”¨çš„ç›®æ¨™è³‡æ–™èˆ‡å…§å®¹è¨­å®š
if "report_target_data_path" not in st.session_state:
    st.session_state.report_target_data_path = None
# ç”¨æ–¼å ±å‘Šç”¢å‡ºçš„ CSV è³‡æ–™æª”æ¡ˆè·¯å¾‘

if "report_target_plot_path" not in st.session_state:
    st.session_state.report_target_plot_path = None
# åœ–è¡¨åœ–åƒæª”æ¡ˆçš„è·¯å¾‘

if "report_target_query" not in st.session_state:
    st.session_state.report_target_query = None
# åŸå§‹çš„ç”¨æˆ¶æŸ¥è©¢ï¼ˆè‡ªç„¶èªè¨€å•é¡Œï¼‰ä½œç‚ºåˆ†æèµ·é»



# --- æç¤ºæ¨¡æ¿ ---
# æ³¨æ„ï¼šæç¤ºç¾åœ¨æœƒè‡ªå‹•åŒ…å« user_provided_column_descriptions ä½œç‚º {data_summary} çš„ä¸€éƒ¨åˆ†
cdo_initial_data_description_prompt_template = PromptTemplate(input_variables=["data_summary", "chat_history"],
                                                              template="""ä½ æ˜¯é¦–å¸­è³‡æ–™é•·ï¼ˆCDOï¼‰ã€‚ä½¿ç”¨è€…å·²ä¸Šå‚³ä¸€å€‹ CSV æª”æ¡ˆï¼Œä¸¦å¯èƒ½é™„ä¸Šæ¬„ä½èªªæ˜çš„æ–‡å­—æª”ã€‚
è³‡æ–™æ‘˜è¦ï¼ˆæä¾›èƒŒæ™¯è³‡è¨Šï¼Œå¯èƒ½åŒ…å« 'user_provided_column_descriptions'ï¼‰ï¼š
{data_summary}

CDOï¼Œä½ çš„ç¬¬ä¸€é …ä»»å‹™æ˜¯å°è³‡æ–™é›†é€²è¡Œåˆæ­¥èªªæ˜ï¼Œå…§å®¹åŒ…æ‹¬ï¼š
1. é¡ä¼¼ `df.info()` çš„æ¦‚è¦½ï¼ˆæ¬„ä½åç¨±ã€éç©ºå€¼æ•¸é‡ã€è³‡æ–™å‹åˆ¥ï¼‰ã€‚
2. è‹¥è³‡æ–™æ‘˜è¦ä¸­åŒ…å« 'user_provided_column_descriptions'ï¼Œè«‹å¼•ç”¨ä¸¦æ•´åˆè‡³èªªæ˜ä¸­ï¼Œèªªæ˜é€™äº›æè¿°å¦‚ä½•é‡æ¸…è³‡æ–™å…§å®¹ã€‚
3. å°æ¯å€‹è®Šæ•¸ï¼æ¬„ä½æä¾›ä½ çš„æ¨æ¸¬å«ç¾©æˆ–å¸¸è¦‹è§£é‡‹ï¼Œç‰¹åˆ¥æ˜¯æœªç”±ä½¿ç”¨è€…èªªæ˜è€…ã€‚
4. åˆæ­¥è³‡æ–™å“è³ªè©•ä¼°ï¼ˆä¾‹å¦‚æ˜é¡¯çš„ç¼ºå¤±è³‡æ–™æ¨¡å¼ã€æ½›åœ¨é›¢ç¾¤å€¼ã€è³‡æ–™å‹åˆ¥ä¸€è‡´æ€§ç­‰ï¼‰ï¼Œä¸¦è€ƒé‡ä½¿ç”¨è€…æä¾›çš„æè¿°ã€‚
é€™ä»½èªªæ˜å°‡æä¾›çµ¦å…¶ä»–éƒ¨é–€ä¸»ç®¡åƒè€ƒã€‚
å°è©±æ­·å²ï¼ˆä¾›åƒè€ƒï¼‰ï¼š
{chat_history}

CDO æ’°å¯«çš„è©³ç´°åˆæ­¥èªªæ˜ï¼ˆç´å…¥ä½¿ç”¨è€…èªªæ˜ï¼Œå¦‚æœ‰ï¼‰ï¼š""")

individual_perspectives_prompt_template = PromptTemplate(
    input_variables=["data_summary", "chat_history", "cdo_initial_report"], template="""ä½ æ˜¯ä¸€çµ„ç”±å„éƒ¨é–€ä¸»ç®¡ï¼ˆåŒ…å« CDOï¼‰çµ„æˆçš„å°ˆå®¶å°çµ„ã€‚
ä½¿ç”¨è€…å·²ä¸Šå‚³ CSV æª”æ¡ˆï¼ˆä»¥åŠå¯èƒ½çš„æ¬„ä½èªªæ˜ï¼‰ï¼Œè€Œ CDO å·²æä¾›åˆæ­¥çš„è³‡æ–™èªªæ˜èˆ‡å“è³ªè©•ä¼°ã€‚
è³‡æ–™æ‘˜è¦ï¼ˆåŸå§‹å…§å®¹ï¼Œå¯èƒ½åŒ…å« 'user_provided_column_descriptions'ï¼‰ï¼š
{data_summary}

CDO åˆæ­¥èªªæ˜èˆ‡å“è³ªå ±å‘Šï¼ˆå·²æ•´åˆä½¿ç”¨è€…èªªæ˜ï¼‰ï¼š
--- CDO å ±å‘Šé–‹å§‹ ---
{cdo_initial_report}
--- CDO å ±å‘ŠçµæŸ ---

è«‹æ ¹æ“šåŸå§‹è³‡æ–™æ‘˜è¦èˆ‡ CDO å ±å‘Šï¼Œåˆ†åˆ¥æä¾›ä»¥ä¸‹è§’è‰²çš„è©³ç´°è§€é»ï¼š
æ¯ä½ä¸»ç®¡éœ€æå‡º 2 è‡³ 3 é …å…·é«”å•é¡Œã€å¸Œæœ›åŸ·è¡Œçš„åˆ†ææˆ–è§€å¯Ÿå…§å®¹ï¼Œä¸¦åƒè€ƒ CDO ç™¼ç¾èˆ‡ä½¿ç”¨è€…æä¾›çš„æ¬„ä½æ„ç¾©ã€‚

è«‹ä½¿ç”¨ä¸‹åˆ—çµæ§‹å‘ˆç¾ï¼š
* **CEOï¼ˆåŸ·è¡Œé•·ï¼‰è§€é»ï¼š**
* **CFOï¼ˆè²¡å‹™é•·ï¼‰è§€é»ï¼š**
* **CTOï¼ˆæŠ€è¡“é•·ï¼‰è§€é»ï¼š**
* **COOï¼ˆç‡Ÿé‹é•·ï¼‰è§€é»ï¼š**
* **CMOï¼ˆè¡ŒéŠ·é•·ï¼‰è§€é»ï¼š**
* **CDOï¼ˆé‡ç”³é—œéµé»ï¼Œä¸¦å¼•ç”¨ä½¿ç”¨è€…èªªæ˜ï¼‰ï¼š**

å°è©±æ­·å²ï¼ˆä¾›åƒè€ƒï¼‰ï¼š
{chat_history}

å„éƒ¨é–€ä¸»ç®¡çš„è©³ç´°è§€é»ï¼ˆåƒè€ƒ CDO å ±å‘Šèˆ‡ä½¿ç”¨è€…æ¬„ä½èªªæ˜ï¼‰ï¼š""")

synthesize_analysis_suggestions_prompt_template = PromptTemplate(
    input_variables=["data_summary", "chat_history", "cdo_initial_report", "generated_perspectives_from_others"],
    template="""ä½ æ˜¯æœ¬å…¬å¸çš„é¦–å¸­è³‡æ–™é•·ï¼ˆCDOï¼‰ã€‚
ä½¿ç”¨è€…å·²ä¸Šå‚³ä¸€ä»½ CSV æª”æ¡ˆã€‚ä½ å·²å®Œæˆåˆæ­¥çš„è³‡æ–™èªªæ˜ï¼ˆè‹¥æœ‰ä½¿ç”¨è€…æä¾›æ¬„ä½èªªæ˜äº¦å·²ç´å…¥ï¼‰ã€‚
æ¥è‘—ï¼Œå…¶å®ƒéƒ¨é–€ä¸»ç®¡ä¹Ÿæ ¹æ“šä½ çš„åˆæ­¥ç™¼ç¾èˆ‡è³‡æ–™æ‘˜è¦æä¾›äº†å›é¥‹ã€‚

åŸå§‹è³‡æ–™æ‘˜è¦ï¼ˆå¯èƒ½åŒ…å« 'user_provided_column_descriptions'ï¼‰ï¼š
{data_summary}

ä½ çš„åˆæ­¥è³‡æ–™èªªæ˜èˆ‡å“è³ªè©•ä¼°ï¼š
--- CDO åˆæ­¥å ±å‘Šé–‹å§‹ ---
{cdo_initial_report}
--- CDO åˆæ­¥å ±å‘ŠçµæŸ ---

å„éƒ¨é–€ä¸»ç®¡çš„è§€é»ï¼ˆCEOã€CFOã€CTOã€COOã€CMOï¼‰ï¼š
--- ä¸»ç®¡è§€é»é–‹å§‹ ---
{generated_perspectives_from_others}
--- ä¸»ç®¡è§€é»çµæŸ ---

ä½ çš„ä»»å‹™æ˜¯ç¶œåˆä¸Šè¿°è³‡è¨Šï¼Œæå‡º **5 é …æ¸…æ™°ä¸”å¯åŸ·è¡Œçš„åˆ†æç­–ç•¥å»ºè­°**ã€‚
é€™äº›å»ºè­°æ‡‰å„ªå…ˆèšç„¦åœ¨èƒ½ç”¢å‡ºæ˜ç¢ºåœ–è¡¨ã€æ ¼å¼è‰¯å¥½çš„è¡¨æ ¼ï¼Œæˆ–ç°¡æ½”æè¿°æ€§æ‘˜è¦çš„åˆ†ææ–¹å¼ã€‚
è‹¥æœ‰ä½¿ç”¨è€…æä¾›æ¬„ä½èªªæ˜ï¼Œè«‹å……åˆ†åŠ ä»¥æ‡‰ç”¨ã€‚

è«‹ä»¥ç·¨è™Ÿæ¢åˆ—æ–¹å¼å‘ˆç¾é€™ 5 é …å»ºè­°ã€‚æ¯é …å»ºè­°éœ€æ¸…æ¥šèªªæ˜åˆ†æé¡å‹ã€‚

å°è©±æ­·å²ï¼ˆä¾›åƒè€ƒï¼‰ï¼š
{chat_history}

CDO çµ±æ•´å¾Œçš„ 5 é …åˆ†æç­–ç•¥å»ºè­°ï¼ˆè‘—é‡åœ–è¡¨ã€è¡¨æ ¼ã€æè¿°æ–¹å¼ï¼Œæ•´åˆæ‰€æœ‰å…ˆå‰è¼¸å…¥èˆ‡ä½¿ç”¨è€…èªªæ˜ï¼‰ï¼š""")

# TEMP_DATA_STORAGE_PROMPT will be replaced with the actual path
TEMP_DATA_STORAGE_PROMPT_PLACEHOLDER = "{TEMP_DATA_STORAGE_PATH_FOR_PROMPT}"

code_generation_prompt_template = PromptTemplate(input_variables=["data_summary", "user_query", "chat_history"],
                                                 template=f"""ä½ æ˜¯ä½å°ˆæ¥­çš„ Python è³‡æ–™åˆ†æåŠ©æ‰‹ã€‚
è³‡æ–™æ‘˜è¦ï¼ˆå¯èƒ½åŒ…å« 'user_provided_column_descriptions'ï¼‰ï¼š
{'{data_summary}'}
ä½¿ç”¨è€…æå•ï¼š"{{user_query}}"
å…ˆå‰å°è©±ç´€éŒ„ï¼ˆæä¾›èƒŒæ™¯åƒè€ƒï¼‰ï¼š
{'{chat_history}'}

ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šåç‚º `df` çš„ pandas DataFrameï¼Œæ’°å¯«ä¸€æ®µ Python ç¨‹å¼ç¢¼ä¾†åŸ·è¡ŒæŒ‡å®šåˆ†æã€‚
**é—œæ–¼ `analysis_result` èˆ‡ `plot_data_df` çš„é—œéµæŒ‡å¼•ï¼š**
1.  **å¿…é ˆè¨­å®š `analysis_result`**ï¼šä¸»åˆ†æçµæœéœ€æŒ‡å®šç‚º `analysis_result`ã€‚
2.  **è‹¥ç‚ºåœ–è¡¨é¡è¼¸å‡ºï¼š**
    a.  å°‡åœ–è¡¨å„²å­˜æ–¼æŒ‡å®šæš«å­˜è³‡æ–™å¤¾ï¼Œä½¿ç”¨è·¯å¾‘ï¼š`os.path.join(TEMP_DATA_STORAGE, 'your_plot_filename.png')`ã€‚ä½ å¯ä½¿ç”¨ `TEMP_DATA_STORAGE` è®Šæ•¸ã€‚
    b.  `analysis_result` åƒ…éœ€è¨­å®šç‚ºåœ–æª”åç¨±å­—ä¸²ï¼ˆä¾‹å¦‚ 'my_plot.png'ï¼Œä¸è¦åŠ ä¸Šå®Œæ•´è·¯å¾‘ï¼‰ã€‚
    c.  å»ºç«‹åç‚º `plot_data_df` çš„ pandas DataFrameï¼ŒåªåŒ…å«å¯¦éš›è¦–è¦ºåŒ–çš„è³‡æ–™ã€‚è‹¥ç‚ºæ•´å€‹ `df`ï¼Œå‰‡ä½¿ç”¨ `df.copy()`ï¼›è‹¥åœ–è¡¨ä¾†è‡ªå½™ç¸½ï¼Œå‰‡è¨­å®šç‚º `None`ã€‚
3.  **è‹¥ç‚ºè¡¨æ ¼é¡è¼¸å‡ºï¼ˆDataFrame / Seriesï¼‰ï¼š**
    a.  å°‡ç”¢å‡ºçµæœæŒ‡å®šç‚º `analysis_result`ã€‚
    b.  å°‡ `plot_data_df = analysis_result.copy()`ï¼ˆè‹¥ç‚º DataFrame/Seriesï¼‰æˆ–è¨­ç‚º `None`ï¼ˆè‹¥ä¸é©ç”¨ï¼‰ã€‚
4.  **è‹¥ç‚ºæ–‡å­—è¼¸å‡ºçµæœï¼š**
    a.  å°‡æ–‡å­—çµæœè¨­ç‚º `analysis_result`ï¼ˆå­—ä¸²ï¼‰ã€‚
    b.  è¨­å®š `plot_data_df = None`ã€‚
5.  **é è¨­è™•ç†ï¼š** è‹¥æå•å…§å®¹è¼ƒå»£æ³›æˆ–æœªæ˜ç¢ºè¦æ±‚ç‰¹å®šè¼¸å‡ºï¼Œ`analysis_result` å¯ç‚º `df.head()` æˆ–ç°¡è¦æè¿°ï¼›æ­¤æ™‚ `plot_data_df = df.head().copy()` æˆ– `None`ã€‚
6.  **å¿…è¦åŒ¯å…¥ï¼š** è«‹å‹™å¿…å¼•å…¥å¿…è¦å¥—ä»¶ï¼ˆå¦‚ `pandas`, `matplotlib.pyplot`, `seaborn`, `numpy`, `os`ï¼‰ã€‚`TEMP_DATA_STORAGE` è·¯å¾‘è®Šæ•¸å¯ç›´æ¥ä½¿ç”¨ã€‚
7.  **è³‡æ–™å¤¾å»ºç«‹ï¼š** åœ¨å„²å­˜åœ–è¡¨å‰ï¼Œè«‹ç¢ºèªç›®éŒ„å­˜åœ¨ï¼Œå¯ç”¨ `os.makedirs(TEMP_DATA_STORAGE, exist_ok=True)` ç¢ºä¿ã€‚

**ä¿éšªæ©Ÿåˆ¶ - è«‹åœ¨è…³æœ¬çµå°¾åŠ å…¥ä»¥ä¸‹é‚è¼¯ä»¥é˜²éºæ¼è¨­å®šï¼š**
```python
# --- ä¿éšªæ©Ÿåˆ¶ ---
if 'analysis_result' not in locals():
    if 'df' in locals() and isinstance(df, pd.DataFrame) and not df.empty:
        analysis_result = "è…³æœ¬åŸ·è¡Œå®Œç•¢ï¼ŒAI ä¸»é‚è¼¯æœªæŒ‡å®š 'analysis_result'ï¼Œé è¨­é¡¯ç¤º df.head() çµæœã€‚"
        plot_data_df = df.head().copy()
    else:
        analysis_result = "è…³æœ¬åŸ·è¡Œå®Œç•¢ï¼Œæœªè¨­å®š 'analysis_result'ï¼Œä¸”æœªåµæ¸¬åˆ°æœ‰æ•ˆçš„ 'df'ã€‚"
if 'plot_data_df' not in locals():
    plot_data_df = None
```
è«‹åƒ…è¼¸å‡ºç´” Python ç¨‹å¼ç¢¼ï¼Œå‹¿åŒ…å«å…¶ä»–æ–‡å­—æˆ– markdown èªæ³•ã€‚
Python ç¨‹å¼ç¢¼ï¼š""")

report_generation_prompt_template = PromptTemplate(
    input_variables=["table_data_csv_or_text", "original_data_summary", "user_query_that_led_to_data", "chat_history",
                     "plot_info_if_any"],
    template="""ä½ æ˜¯ä¸€ä½å…·æœ‰æ´å¯ŸåŠ›çš„è³‡æ–™åˆ†æå¸«ã€‚è«‹æ ¹æ“šæä¾›çš„è³‡æ–™èˆ‡èƒŒæ™¯å…§å®¹æ’°å¯«æ–‡å­—å ±å‘Šã€‚
åŸå§‹è³‡æ–™æ‘˜è¦ï¼ˆå¯èƒ½åŒ…å« 'user_provided_column_descriptions'ï¼‰ï¼š
{original_data_summary}

ä½¿ç”¨è€…çš„æå•ï¼ˆå°è‡´æ­¤è³‡æ–™æˆ–åœ–è¡¨ç”¢å‡ºï¼‰ï¼š"{user_query_that_led_to_data}"
åœ–è¡¨è³‡è¨Šï¼ˆè‹¥é©ç”¨ï¼Œå¦å‰‡è«‹å¯« 'N/A'ï¼‰ï¼š{plot_info_if_any}
å°è©±æ­·å²ï¼ˆæä¾›èƒŒæ™¯ï¼‰ï¼š
{chat_history}

åˆ†æçµæœè³‡æ–™ï¼ˆCSV å…§å®¹ã€æ–‡å­—è¼¸å‡ºï¼Œæˆ–ç‚ºåœ–è¡¨å ±å‘Šæ™‚å¯« 'N/A'ï¼‰ï¼š
```
{table_data_csv_or_text}
```

**å ±å‘Šæ¶æ§‹ï¼š**
* **1. é‡é»æ‘˜è¦ï¼ˆ1-2 å¥ï¼‰ï¼š** æ¿ƒç¸®æ•˜è¿°ä¾†è‡ª `åˆ†æçµæœè³‡æ–™` æˆ–åœ–è¡¨çš„ä¸»è¦çµè«–ã€‚
* **2. åˆ†æç›®çš„ï¼ˆ1 å¥ï¼‰ï¼š** ç°¡è¦é‡è¿°ä½¿ç”¨è€…æå•æ‰€åæ˜ çš„ç›®æ¨™ã€‚
* **3. ä¸»è¦è§€å¯Ÿï¼ˆæ¢åˆ— 2-4 é»ï¼‰ï¼š** å…·é«”èªªæ˜ `åˆ†æçµæœè³‡æ–™` æˆ–åœ–è¡¨ä¸­æ˜ç¢ºçš„æ•¸æ“šè¶¨å‹¢æˆ–è§€å¯Ÿï¼Œè‹¥æœ‰æ¬„ä½èªªæ˜å¹«åŠ©ç†è§£ï¼Œè«‹æåŠã€‚
* **4. å¯è¡Œæ´å¯Ÿï¼ˆ1-2 é»ï¼‰ï¼š** é€™äº›ç™¼ç¾ä»£è¡¨ä»€éº¼æ„ç¾©ï¼Ÿå¯æ“šä»¥æ¡å–å“ªäº›è¡Œå‹•ï¼Ÿ
* **5. è³‡æ–™ä¾·é™èˆ‡ç„¦é»ï¼š** æ¸…æ¥šèªªæ˜æœ¬å ±å‘Š *åƒ…æ ¹æ“š* ä¸Šè¿°çš„ã€Œåˆ†æçµæœè³‡æ–™ã€èˆ‡ï¼æˆ–ã€Œåœ–è¡¨è³‡è¨Šã€æ’°å¯«ã€‚è‹¥è³‡æ–™ç‚ºæ¨£æœ¬ã€å½™ç¸½ç­‰ä¹Ÿè«‹æŒ‡å‡ºã€‚

**èªæ°£é¢¨æ ¼ï¼š** å°ˆæ¥­ã€æ¸…æ™°ã€å®¢è§€ã€‚**è«‹å‹¿**ä½¿ç”¨ã€Œå¾åœ–è¡¨å¯ä»¥çœ‹å‡ºâ€¦ã€æˆ–ã€ŒCSV é¡¯ç¤ºäº†â€¦ã€ç­‰èªå¥ï¼Œè«‹ç›´æ¥é™³è¿°è§€å¯Ÿçµæœã€‚
å ±å‘Šï¼š""")

judging_prompt_template = PromptTemplate(
    input_variables=["python_code", "data_content_for_judge", "report_text_content", "original_user_query",
                     "data_summary",
                     "plot_image_path", "plot_info_for_judge"], template="""ä½ æ˜¯ä¸€ä½è³‡æ·±çš„è³‡æ–™ç§‘å­¸å¯©æŸ¥å“¡ã€‚è«‹ä¾æ“šä½¿ç”¨è€…çš„æå•èˆ‡è³‡æ–™ä¸Šä¸‹æ–‡ï¼Œè©•ä¼° AI åŠ©æ‰‹ç”¢å‡ºçš„å„é …æˆæœã€‚
ä½¿ç”¨è€…åŸå§‹å•é¡Œï¼š"{original_user_query}"
åŸå§‹è³‡æ–™æ‘˜è¦ï¼ˆå¯èƒ½åŒ…å« 'user_provided_column_descriptions'ï¼‰ï¼š
{data_summary}

--- è©•ä¼°é …ç›® ---
1.  **AI åŠ©æ‰‹ç”¢å‡ºçš„ Python ç¨‹å¼ç¢¼ï¼š**
    ```python
{python_code}
    ```
2.  **åŸ·è¡Œç¨‹å¼ç¢¼å¾Œç”¢ç”Ÿçš„è³‡æ–™ï¼ˆCSV å…§å®¹æˆ– `analysis_result` çš„æ–‡å­—è¼¸å‡ºï¼‰ï¼š**
    ```
{data_content_for_judge}
    ```
    **åœ–è¡¨è³‡è¨Šï¼š** {plot_info_for_judge}ï¼ˆå¯èƒ½æŒ‡å‡ºæ˜¯å¦ç”¢å‡ºåœ–æª” '{plot_image_path}'ï¼Œæˆ–æ˜¯å¦ç”¢å‡º `plot_data_df`ï¼‰

3.  **AI åŠ©æ‰‹æ’°å¯«çš„æ–‡å­—å ±å‘Šï¼ˆè‹¥æœ‰ï¼‰ï¼š**
    ```text
{report_text_content}
    ```
--- çµæŸè©•ä¼°é …ç›® ---

**è©•ä¼°æº–å‰‡ï¼š**
1.  **ç¨‹å¼ç¢¼å“è³ªèˆ‡åˆè¦æ€§ï¼ˆæ ¸å¿ƒæŒ‡æ¨™ï¼‰ï¼š**
    * æ­£ç¢ºæ€§ï¼šç¨‹å¼ç¢¼æ˜¯å¦èƒ½åŸ·è¡Œä¸”é‚è¼¯æ­£ç¢ºï¼Ÿ
    * æ•ˆç‡èˆ‡å¯è®€æ€§ï¼šæ˜¯å¦å…·å‚™åˆç†æ•ˆç‡èˆ‡æ¸…æ™°çµæ§‹ï¼Ÿ
    * å¯¦å‹™æº–å‰‡ï¼šæ˜¯å¦ä½¿ç”¨åˆé©çš„å¥—ä»¶èˆ‡æ–¹æ³•ï¼Ÿ
    * **`analysis_result` èˆ‡ `plot_data_df` ä½¿ç”¨æƒ…å½¢ï¼š**
        * æ˜¯å¦æ­£ç¢ºè¨­å®š `analysis_result` ç‚ºåœ–æª”æª”åï¼ˆè‹¥ç‚ºåœ–ï¼‰ï¼Ÿ
        * æ˜¯å¦æ­£ç¢ºè¨­å®šç‚º DataFrame / Series æˆ–æ–‡å­—ï¼ˆè¦–çµæœé¡å‹è€Œå®šï¼‰ï¼Ÿ
        * `plot_data_df` æ˜¯å¦å°æ‡‰åœ–è¡¨è³‡æ–™ï¼ˆæˆ–ç‚ºåˆ†æçµæœçš„è¤‡æœ¬ï¼‰ï¼Ÿ
    * åœ–è¡¨å„²å­˜ï¼šæ˜¯å¦å°‡åœ–è¡¨å„²å­˜åœ¨ `TEMP_DATA_STORAGE` è³‡æ–™å¤¾ï¼Ÿï¼ˆä½¿ç”¨ os.path.joinï¼‰

2.  **è³‡æ–™åˆ†æå“è³ªï¼š**
    * ç›¸é—œæ€§ï¼šåˆ†ææ˜¯å¦ç¬¦åˆä½¿ç”¨è€…å•é¡Œèˆ‡è³‡æ–™å±¬æ€§ï¼Ÿ
    * æº–ç¢ºæ€§ï¼šè¨ˆç®—èˆ‡é‚è¼¯æ˜¯å¦å¯èƒ½æ­£ç¢ºï¼Ÿ
    * æ–¹æ³•é¸æ“‡ï¼šæ˜¯å¦é¸ç”¨åˆé©çš„åˆ†ææˆ–è¦–è¦ºåŒ–æ–¹æ³•ï¼Ÿ
    * `plot_data_df` å…§å®¹ï¼šæ˜¯å¦åˆç†å°æ‡‰åœ–è¡¨è³‡æ–™ï¼Ÿ

3.  **åœ–è¡¨å“è³ªï¼ˆè‹¥æœ‰åœ–ï¼‰ï¼š**
    * åˆé©æ€§ï¼šåœ–è¡¨é¡å‹æ˜¯å¦é©åˆè³‡æ–™èˆ‡æå•ï¼Ÿ
    * æ¸…æ™°æ€§ï¼šæ˜¯å¦æœ‰æ¨™é¡Œã€åº§æ¨™è»¸ã€åœ–ä¾‹ï¼Ÿæ˜¯å¦æ˜“æ–¼ç†è§£ï¼Ÿ

4.  **å ±å‘Šå“è³ªï¼ˆè‹¥æœ‰å ±å‘Šï¼‰ï¼š**
    * æ¸…æ™°èˆ‡ç°¡æ½”ï¼šå ±å‘Šæ˜¯å¦æ¸…æ¥šæ˜“æ‡‚ï¼Ÿ
    * æ´å¯ŸåŠ›ï¼šæ˜¯å¦å¾ `data_content_for_judge` æˆ–åœ–è¡¨ä¸­èƒå–å‡ºæœ‰åƒ¹å€¼çš„ç™¼ç¾ï¼Ÿ
    * å›æ‡‰æŸ¥è©¢ï¼šæ˜¯å¦å›æ‡‰ä½¿ç”¨è€…åŸå§‹å•é¡Œï¼Ÿ
    * å®¢è§€æ€§ï¼šæ˜¯å¦ä»¥æ•¸æ“šç‚ºä¾æ“šï¼Œé¿å…ä¸»è§€æ¨è«–ï¼Ÿ

5.  **æ•´é«”è¡¨ç¾èˆ‡å»ºè­°ï¼š**
    * è«‹ç‚º AI åŠ©æ‰‹çš„å›æ‡‰æ‰“åˆ†ï¼ˆ1-10 åˆ†ï¼Œ10 ç‚ºå„ªç§€ï¼‰
    * çµ¦äºˆ 1-2 é …å…·é«”å»ºè­°ï¼Œä»¥æå‡å°æ­¤é¡å•é¡Œçš„æœªä¾†å›æ‡‰å“è³ª

å¯©æŸ¥æ„è¦‹ï¼š""")

with st.sidebar:
    st.header("ğŸ”‘ API é‡‘é‘°è¨­å®š")

    st.markdown("è«‹è¼¸å…¥æ‚¨çš„ **Google AI Studio API é‡‘é‘°**ã€‚")
    google_user_api_key = st.text_input("è¼¸å…¥æ‚¨çš„ Google API é‡‘é‘°", 
                                 type="password", 
                                 value=st.session_state.get("google_api_key", ""),
                                 key="google_api_key_input")
    
    if google_user_api_key:
        st.session_state.google_api_key = google_user_api_key

    st.markdown("è«‹è¼¸å…¥æ‚¨çš„ **OpenAI (GPT) API é‡‘é‘°**ã€‚")
    openai_user_api_key = st.text_input("è¼¸å…¥æ‚¨çš„ OpenAI (GPT) API é‡‘é‘°",
                                 type="password",
                                 value=st.session_state.get("openai_api_key", ""),
                                 key="openai_api_key_input")

    if openai_user_api_key:
        st.session_state.openai_api_key = openai_user_api_key

    if google_user_api_key and openai_user_api_key:
        st.success("Google èˆ‡ OpenAI é‡‘é‘°å‡å·²è¨­å®šã€‚")
    elif not google_user_api_key and not openai_user_api_key:
        st.warning("è«‹è¼¸å…¥æ‚¨çš„ API é‡‘é‘°ä»¥å•Ÿç”¨ AI åŠŸèƒ½ã€‚")

    st.markdown("---") # åˆ†éš”ç·š
    # é¡¯ç¤ºæ¨™é¡Œã€Œæ¨¡å‹é¸æ“‡ã€
    st.header("âš™ï¸ æ¨¡å‹é¸æ“‡")
    st.markdown("""
    **æ¨¡å‹é¸æ“‡**  
    1.    **å·¥ä½œè€…æ¨¡å¼**ï¼šé‡å°ä½¿ç”¨AI åˆ†æå°è©±æ™‚çš„å›è¦†æ¨¡å‹
    2.    **è©•åˆ¤æ¨¡å¼**ï¼šé‡å°ä½¿ç”¨AI åˆ†æå°è©±æ™‚å·¥ä½œè€…æ¨¡å¼é€²è¡Œè©•åˆ¤
    """)
    # æ¨¡å‹é¸æ“‡ä¸‹æ‹‰é¸å–®ï¼ˆå·¥ä½œæ¨¡å‹ï¼‰
    # ä½¿ç”¨è€…å¯ä»¥é¸æ“‡ç”¨æ–¼åˆ†æçš„ LLM æ¨¡å‹
    st.session_state.selected_worker_model = st.selectbox("é¸æ“‡å·¥ä½œè€…æ¨¡å¼ï¼š", AVAILABLE_MODELS,
                                                          index=AVAILABLE_MODELS.index(
                                                              st.session_state.selected_worker_model))

    # æ¨¡å‹é¸æ“‡ä¸‹æ‹‰é¸å–®ï¼ˆè©•åˆ¤æ¨¡å‹ï¼‰
    # ä½¿ç”¨è€…å¯ä»¥é¸æ“‡ç”¨æ–¼è©•ä¼°åˆ†æçµæœçš„ LLM æ¨¡å‹
    st.session_state.selected_judge_model = st.selectbox("é¸æ“‡è©•åˆ¤æ¨¡å‹ï¼š", AVAILABLE_MODELS,
                                                         index=AVAILABLE_MODELS.index(
                                                             st.session_state.selected_judge_model))

    # é¡¯ç¤ºæ¨™é¡Œã€Œä¸Šå‚³æ•¸æ“šã€
    st.header("ğŸ“¤ ä¸Šå‚³æ•¸æ“š")

    # CSV æª”æ¡ˆä¸Šå‚³åŠŸèƒ½ï¼ˆåªå…è¨±ä¸Šå‚³ .csv æª”ï¼‰
    uploaded_csv_file = st.file_uploader("ä¸Šå‚³æ‚¨çš„ CSV æª”æ¡ˆï¼š", type="csv", key="csv_uploader")

    # å¯é¸çš„æ–‡å­—æª”æè¿°æ¬„ä½ä¸Šå‚³ï¼ˆåªå…è¨±ä¸Šå‚³ .txt æª”ï¼‰
    uploaded_desc_file = st.file_uploader("å¯é¸ï¼šä¸Šå‚³åˆ—æè¿°ï¼ˆ.txtï¼‰ï¼š", type="txt", key="desc_uploader")

    # å¦‚æœä½¿ç”¨è€…ä¸Šå‚³äº† CSV æª”æ¡ˆ
    if uploaded_csv_file is not None:
        reprocess = False  # æ˜¯å¦éœ€è¦é‡æ–°è™•ç†æª”æ¡ˆçš„æ——æ¨™

        # å¦‚æœä¸Šå‚³çš„ CSV æª”èˆ‡ä¹‹å‰çš„ä¸ä¸€æ¨£
        if st.session_state.get("data_source_name") != uploaded_csv_file.name:
            reprocess = True

        # å¦‚æœä¸Šå‚³çš„æè¿°æª”ä¸åŒï¼Œä¹Ÿéœ€è¦é‡æ–°è™•ç†
        if uploaded_desc_file and st.session_state.get("desc_file_name") != uploaded_desc_file.name:
            reprocess = True

        # å¦‚æœå…ˆå‰æœ‰æè¿°æª”ï¼Œä½†ç¾åœ¨æ²’ä¸Šå‚³ï¼Œä»£è¡¨æè¿°æª”è¢«ç§»é™¤
        if not uploaded_desc_file and st.session_state.get("desc_file_name") is not None:
            reprocess = True

        # å¦‚æœåªæœ‰ CSVï¼Œä¸”èˆ‡ä¹‹å‰ç›¸åŒï¼Œå‰‡ä¸éœ€è¦é‡æ–°è™•ç†
        if uploaded_desc_file is None and st.session_state.get("desc_file_name") is None and not reprocess:
            pass
        # å¦‚æœå…©å€‹æª”æ¡ˆéƒ½å­˜åœ¨ä¸”èˆ‡ä¹‹å‰ç›¸åŒï¼Œä¹Ÿä¸è™•ç†
        elif uploaded_desc_file is not None and \
                st.session_state.get("desc_file_name") == uploaded_desc_file.name and \
                st.session_state.get("data_source_name") == uploaded_csv_file.name:
            pass
        else:
            reprocess = True  # å…¶ä»–æƒ…æ³å‰‡éœ€è¦é‡æ–°è™•ç†

        # å¦‚æœéœ€è¦é‡æ–°è™•ç†è³‡æ–™
        if reprocess:
            with st.spinner("æ­£åœ¨è™•ç† CSV èˆ‡æè¿°æª”..."):
                # å‘¼å«å‡½å¼è™•ç†æª”æ¡ˆå…§å®¹èˆ‡æ‘˜è¦
                if load_csv_and_get_summary(uploaded_csv_file, uploaded_desc_file):
                    st.success(f"CSV æª”æ¡ˆ '{st.session_state.data_source_name}' å·²æˆåŠŸè™•ç†ã€‚")
                    if uploaded_desc_file:
                        st.success(f"æè¿°æª”æ¡ˆ '{st.session_state.desc_file_name}' å·²æˆåŠŸè™•ç†ã€‚")
                    else:
                        st.info("æœªæä¾›æè¿°æª”æ¡ˆï¼Œæˆ–æè¿°æª”å·²ç§»é™¤ã€‚")
                    
                    # ç³»çµ±è¨Šæ¯ï¼šè™•ç†å®Œæˆä¸¦åŠ å…¥å°è©±è¨˜éŒ„
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"å·²è™•ç† '{st.session_state.data_source_name}'" +
                                   (f" ä¸¦ä½¿ç”¨æè¿°æª” '{st.session_state.desc_file_name}'ã€‚" if st.session_state.desc_file_name else "ã€‚") +
                                   " è«‹æŸ¥çœ‹è³‡æ–™å“è³ªå„€è¡¨æ¿æˆ–å…¶ä»–åˆ†é ã€‚"
                    })

                    st.rerun()  # é‡æ–°è¼‰å…¥ä»‹é¢
                else:
                    st.error("è™•ç† CSV æˆ–æè¿°æª”å¤±æ•—ã€‚")

    # å¦‚æœç›®å‰å·²æœ‰è³‡æ–™è¼‰å…¥
    if st.session_state.current_dataframe is not None:
        st.subheader("ç›®å‰è¼‰å…¥çš„æª”æ¡ˆï¼š")
        st.write(
            f"**{st.session_state.data_source_name}**ï¼ˆ{len(st.session_state.current_dataframe)} ç­†è³‡æ–™ Ã— {len(st.session_state.current_dataframe.columns)} æ¬„ä½ï¼‰")
        if st.session_state.desc_file_name:
            st.write(f"é™„å¸¶æè¿°æª”ï¼š**{st.session_state.desc_file_name}**")

        # æŒ‰éˆ•ï¼šæ¸…é™¤ç›®å‰è¼‰å…¥è³‡æ–™èˆ‡å°è©±å…§å®¹
        if st.button("æ¸…é™¤è³‡æ–™èˆ‡å°è©±å…§å®¹", key="clear_data_btn"):
            # è¦æ¸…é™¤çš„ Session è®Šæ•¸
            keys_to_reset = [
                "current_dataframe", "data_summary", "data_source_name", "desc_file_name",
                "current_analysis_artifacts", "messages", "lc_memory",
                "cdo_initial_report_text", "other_perspectives_text", "strategy_text", "cdo_workflow_stage",
                "trigger_code_generation", "trigger_report_generation", "trigger_judging", "trigger_html_export",
                "report_target_data_path", "report_target_plot_path", "report_target_query"
                # "temp_data_storage_path" é€™è£¡ä¸å¾ keys_to_reset ä¸­ç§»é™¤ï¼Œè€Œæ˜¯å–®ç¨è™•ç†ï¼Œ
                # å› ç‚ºæˆ‘å€‘éœ€è¦å…ˆåˆªé™¤ç›®éŒ„å…§å®¹ï¼Œå†ç§»é™¤å…¶è·¯å¾‘ã€‚
            ]
            for key in keys_to_reset:
                if key in st.session_state:
                    del st.session_state[key]

            # é‡ç½®å°è©±è¨Šæ¯
            st.session_state.messages = [{"role": "assistant", "content": "è³‡æ–™èˆ‡å°è©±å·²é‡ç½®ã€‚è«‹é‡æ–°ä¸Šå‚³ CSVã€‚"}]
            # é‡ç½® LangChain è¨˜æ†¶é«”
            st.session_state.lc_memory = ConversationBufferMemory(memory_key="chat_history", return_messages=False,
                                                                  input_key="user_query")
            st.session_state.current_analysis_artifacts = {}

            # åˆªé™¤è‡¨æ™‚è³‡æ–™å¤¾åŠå…¶å…§å®¹
            if "temp_data_storage_path" in st.session_state and \
               os.path.exists(st.session_state.temp_data_storage_path):
                try:
                    shutil.rmtree(st.session_state.temp_data_storage_path)
                    st.success(f"æš«å­˜è³‡æ–™å¤¾ '{os.path.basename(st.session_state.temp_data_storage_path)}' å·²æ¸…é™¤ã€‚")
                except Exception as e:
                    st.warning(f"ç„¡æ³•åˆªé™¤æš«å­˜è³‡æ–™å¤¾ '{os.path.basename(st.session_state.temp_data_storage_path)}'ï¼š{e}")
                del st.session_state.temp_data_storage_path # æ¸…é™¤ session_state ä¸­çš„è·¯å¾‘ï¼Œä»¥ä¾¿ä¸‹æ¬¡é‡æ–°å‰µå»º

            else:
                st.info("æ²’æœ‰æ´»å‹•çš„æš«å­˜è³‡æ–™éœ€è¦æ¸…é™¤ã€‚") # æ²’æœ‰è‡¨æ™‚ç›®éŒ„æ™‚çš„æç¤º
            
            st.rerun()
    # ä¸‹æ–¹æç¤ºè¨Šæ¯å€
    st.markdown("---")  # æ°´å¹³ç·šåˆ†éš”

    # é¡¯ç¤ºç›®å‰é¸ç”¨çš„æ¨¡å‹
    st.info(f"å·¥ä½œæ¨¡å‹ï¼š**{st.session_state.selected_worker_model}**\n\nè©•åˆ¤æ¨¡å‹ï¼š**{st.session_state.selected_judge_model}**")
    
    # é¡¯ç¤ºæš«å­˜è³‡æ–™å¤¾è·¯å¾‘
    if "temp_data_storage_path" in st.session_state:
        st.info(f"æš«å­˜è³‡æ–™å¤¾ä½ç½®ï¼š`{os.path.abspath(st.session_state.temp_data_storage_path)}`")
    else:
        st.info("æš«å­˜è³‡æ–™å¤¾å°šæœªåˆå§‹åŒ–ã€‚")
    # å®‰å…¨æ€§è­¦å‘Š
    st.warning("âš ï¸ **å®‰å…¨æ€§æé†’ï¼š** æ­¤ç¨‹å¼ä½¿ç”¨ `exec()` åŸ·è¡Œ AI ç”¢ç”Ÿçš„ç¨‹å¼ç¢¼ï¼Œåƒ…ä¾›å±•ç¤ºç”¨é€”ï¼Œè«‹å‹¿ç”¨æ–¼ç”Ÿç”¢ç’°å¢ƒã€‚")




# --- ä¸»ä»‹é¢ï¼šæ ¹æ“šè³‡æ–™æ˜¯å¦ä¸Šå‚³æ±ºå®šæ˜¯å¦é¡¯ç¤ºåˆ†é  ---
if st.session_state.current_dataframe is not None:
    # åˆ†é æ¨™é¡Œ
    tab_titles = ["ğŸ“Š è³‡æ–™å“è³ªå„€è¡¨æ¿", "ğŸ” è³‡æ–™æ¢ç´¢å™¨", "ğŸ‘¨â€ğŸ’¼ CDO åˆ†ææµç¨‹", "ğŸ’¬ AI åˆ†æå°è©±", "ğŸ›ï¸ PyGWalker æ¢ç´¢å™¨", "ğŸ“‹ é‹ç‡Ÿè¨ºæ–·å ±å‘Š", "ğŸ’¼ è«®è©¢æœå‹™å›é¥‹"]
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(tab_titles)

    # --- åˆ†é ä¸€ï¼šè³‡æ–™å“è³ªå„€è¡¨æ¿ ---
    with tab1:
        generate_data_quality_dashboard(st.session_state.current_dataframe.copy())

    # --- åˆ†é äºŒï¼šè³‡æ–™æ¢ç´¢å™¨ ---
    with tab2:
        st.header("ğŸ” è³‡æ–™æ¢ç´¢å™¨")
        if st.session_state.data_summary:
            with st.expander("æŸ¥çœ‹å®Œæ•´è³‡æ–™æ‘˜è¦ï¼ˆJSON æ ¼å¼ï¼‰", expanded=False):
                st.json(st.session_state.data_summary)
            if st.session_state.data_summary.get("user_provided_column_descriptions"):
                with st.expander("ä½¿ç”¨è€…æä¾›çš„æ¬„ä½èªªæ˜", expanded=True):
                    st.markdown(st.session_state.data_summary["user_provided_column_descriptions"])
        else:
            st.write("ç›®å‰å°šç„¡è³‡æ–™æ‘˜è¦ã€‚")
        with st.expander(f"é¡¯ç¤ºå‰ 5 ç­†è³‡æ–™ï¼ˆ{st.session_state.data_source_name}ï¼‰"):
            st.dataframe(st.session_state.current_dataframe.head())
        with st.expander(f"é¡¯ç¤ºå¾Œ 5 ç­†è³‡æ–™ï¼ˆ{st.session_state.data_source_name}ï¼‰"):
            st.dataframe(st.session_state.current_dataframe.tail())

    # --- åˆ†é ä¸‰ï¼šCDO åˆ†ææµç¨‹ ---
    with tab3:
        st.header("ğŸ‘¨â€ğŸ’¼ CDO ä¸»å°åˆ†ææµç¨‹")
        st.markdown("å•Ÿå‹• AI åˆ†ææµç¨‹ï¼šç”± CDO æè¿°è³‡æ–™ï¼ˆå¯åŒ…å«ä½¿ç”¨è€…æ¬„ä½èªªæ˜ï¼‰ï¼Œå„éƒ¨é–€ VP æå‡ºè§€é»ï¼ŒCDO å½™æ•´ç­–ç•¥å»ºè­°ã€‚")

        # å•Ÿå‹•æŒ‰éˆ•
        if st.button("ğŸš€ å•Ÿå‹• CDO åˆ†ææµç¨‹", key="start_cdo_workflow_btn"):
            # åˆå§‹åŒ–æµç¨‹éšæ®µèˆ‡å…§å®¹
            st.session_state.cdo_workflow_stage = "initial_description"
            st.session_state.cdo_initial_report_text = None
            st.session_state.other_perspectives_text = None
            st.session_state.strategy_text = None
            # æ–°å¢è¨Šæ¯èˆ‡è¨˜æ†¶
            st.session_state.messages.append({"role": "assistant",
                                              "content": f"é–‹å§‹ä½¿ç”¨ **{st.session_state.selected_worker_model}** é€²è¡Œ CDO åˆå§‹è³‡æ–™æè¿°..."})
            st.session_state.lc_memory.save_context(
                {"user_query": f"ä½¿ç”¨è€…å•Ÿå‹•äº† CDO åˆ†ææµç¨‹ï¼š{st.session_state.data_source_name}"},
                {"output": "è«‹æ±‚é€²è¡Œ CDO åˆå§‹æè¿°ã€‚"})
            st.rerun()

        # æ ¹æ“šç›®å‰éšæ®µå‘¼å«ä¸åŒ prompt åŸ·è¡Œ
        worker_llm = get_llm_instance(st.session_state.selected_worker_model)
        current_stage = st.session_state.get("cdo_workflow_stage")

        # ç¬¬ä¸€æ­¥ï¼šCDO åˆå§‹è³‡æ–™æè¿°
        if current_stage == "initial_description":
            if worker_llm and st.session_state.data_summary:
                with st.spinner(f"CDOï¼ˆ{st.session_state.selected_worker_model}ï¼‰æ­£åœ¨æè¿°è³‡æ–™..."):
                    try:
                        memory_ctx = st.session_state.lc_memory.load_memory_variables({})
                        prompt_inputs = {
                            "data_summary": json.dumps(st.session_state.data_summary, indent=2),
                            "chat_history": memory_ctx.get("chat_history", "")
                        }
                        response = worker_llm.invoke(
                            cdo_initial_data_description_prompt_template.format_prompt(**prompt_inputs))
                        st.session_state.cdo_initial_report_text = response.content if hasattr(response, 'content') else response.get('text', "CDO å›å ±ç”¢ç”Ÿå¤±æ•—ã€‚")
                        st.session_state.messages.append({"role": "assistant",
                                                          "content": f"**CDO çš„åˆå§‹æè¿°ï¼ˆä½¿ç”¨ {st.session_state.selected_worker_model}ï¼‰:**\n\n{st.session_state.cdo_initial_report_text}"})
                        st.session_state.lc_memory.save_context({"user_query": "CDO åˆå§‹æè¿°è«‹æ±‚"},
                                                                {"output": f"CDO å›å ±å‰ 100 å­—ï¼š{st.session_state.cdo_initial_report_text[:100]}..."})
                        st.session_state.cdo_workflow_stage = "departmental_perspectives"
                        st.rerun()
                    except Exception as e:
                        st.error(f"CDO æè¿°éšæ®µç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                        st.session_state.cdo_workflow_stage = None
            else:
                st.error("ç¼ºå°‘æ¨¡å‹æˆ–è³‡æ–™æ‘˜è¦ï¼Œç„¡æ³•åŸ·è¡Œ CDO æµç¨‹ã€‚")

        # ç¬¬äºŒæ­¥ï¼šVP éƒ¨é–€è§€é»
        if current_stage == "departmental_perspectives" and st.session_state.cdo_initial_report_text:
            with st.spinner(f"éƒ¨é–€ä¸»ç®¡ï¼ˆ{st.session_state.selected_worker_model}ï¼‰æ­£åœ¨æå‡ºè§€é»..."):
                try:
                    memory_ctx = st.session_state.lc_memory.load_memory_variables({})
                    prompt_inputs = {
                        "data_summary": json.dumps(st.session_state.data_summary, indent=2),
                        "chat_history": memory_ctx.get("chat_history", ""),
                        "cdo_initial_report": st.session_state.cdo_initial_report_text
                    }
                    response = worker_llm.invoke(individual_perspectives_prompt_template.format_prompt(**prompt_inputs))
                    st.session_state.other_perspectives_text = response.content if hasattr(response, 'content') else response.get('text', "éƒ¨é–€è§€é»ç”¢ç”Ÿå¤±æ•—ã€‚")
                    st.session_state.messages.append({"role": "assistant",
                                                      "content": f"**éƒ¨é–€ä¸»ç®¡è§€é»ï¼ˆ{st.session_state.selected_worker_model}ï¼‰:**\n\n{st.session_state.other_perspectives_text}"})
                    st.session_state.lc_memory.save_context({"user_query": "éƒ¨é–€è§€é»è«‹æ±‚"},
                                                            {"output": f"éƒ¨é–€è§€é»å‰ 100 å­—ï¼š{st.session_state.other_perspectives_text[:100]}..."})
                    st.session_state.cdo_workflow_stage = "strategy_synthesis"
                    st.rerun()
                except Exception as e:
                    st.error(f"éƒ¨é–€è§€é»éšæ®µéŒ¯èª¤ï¼š{e}")
                    st.session_state.cdo_workflow_stage = None

        # ç¬¬ä¸‰æ­¥ï¼šCDO çµ±æ•´ç­–ç•¥å»ºè­°
        if current_stage == "strategy_synthesis" and st.session_state.other_perspectives_text:
            with st.spinner(f"CDOï¼ˆ{st.session_state.selected_worker_model}ï¼‰æ­£åœ¨çµ±æ•´ç­–ç•¥å»ºè­°..."):
                try:
                    memory_ctx = st.session_state.lc_memory.load_memory_variables({})
                    prompt_inputs = {
                        "data_summary": json.dumps(st.session_state.data_summary, indent=2),
                        "chat_history": memory_ctx.get("chat_history", ""),
                        "cdo_initial_report": st.session_state.cdo_initial_report_text,
                        "generated_perspectives_from_others": st.session_state.other_perspectives_text
                    }
                    response = worker_llm.invoke(synthesize_analysis_suggestions_prompt_template.format_prompt(**prompt_inputs))
                    st.session_state.strategy_text = response.content if hasattr(response, 'content') else response.get('text', "ç­–ç•¥å½™æ•´å¤±æ•—ã€‚")
                    st.session_state.messages.append({"role": "assistant",
                                                      "content": f"**CDO çš„æœ€çµ‚ç­–ç•¥å»ºè­°ï¼ˆ{st.session_state.selected_worker_model}ï¼‰:**\n\n{st.session_state.strategy_text}\n\nè«‹å‰å¾€ã€ŒAI åˆ†æå°è©±ã€åˆ†é é€²è¡Œå¾ŒçºŒåˆ†æã€‚"})
                    st.session_state.lc_memory.save_context({"user_query": "è«‹æ±‚ç­–ç•¥çµ±æ•´"},
                                                            {"output": f"ç­–ç•¥å‰ 100 å­—ï¼š{st.session_state.strategy_text[:100]}..."})
                    st.session_state.cdo_workflow_stage = "completed"
                    st.success("CDO åˆ†ææµç¨‹å·²å®Œæˆ âœ…")
                    st.rerun()
                except Exception as e:
                    st.error(f"ç­–ç•¥çµ±æ•´éšæ®µéŒ¯èª¤ï¼š{e}")
                    st.session_state.cdo_workflow_stage = None

        # é¡¯ç¤ºæ­·ç¨‹æ‘˜è¦ï¼ˆå±•é–‹æ¡†ï¼‰
        if st.session_state.cdo_initial_report_text:
            with st.expander("ğŸ“‹ CDO åˆå§‹è³‡æ–™æè¿°", expanded=(current_stage in ["initial_description", "departmental_perspectives", "strategy_synthesis", "completed"])):
                st.markdown(st.session_state.cdo_initial_report_text)
        if st.session_state.other_perspectives_text:
            with st.expander("ğŸ‘¥ éƒ¨é–€è§€é»", expanded=(current_stage in ["departmental_perspectives", "strategy_synthesis", "completed"])):
                st.markdown(st.session_state.other_perspectives_text)
        if st.session_state.strategy_text:
            with st.expander("ğŸ¯ CDO æœ€çµ‚ç­–ç•¥å»ºè­°", expanded=(current_stage in ["strategy_synthesis", "completed"])):
                st.markdown(st.session_state.strategy_text)

    # --- åˆ†é å››ï¼šAI åˆ†æå°è©± ---
    with tab4:
        st.header("ğŸ’¬ AI åˆ†æå°è©±")
        st.caption("èˆ‡ Worker AI å°è©±ç”¢ç”Ÿåˆ†æï¼Œäº¦å¯è«‹ Judge AI è©•ä¼°å…¶å“è³ªèˆ‡å»ºè­°ã€‚")

        chat_container = st.container()
        with chat_container:
            for i, message in enumerate(st.session_state.messages):
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
                    # é¡¯ç¤ºåŸ·è¡Œçµæœï¼ˆåœ–è¡¨ã€è¡¨æ ¼ã€æ–‡å­—ï¼‰
                    if message["role"] == "assistant" and "executed_result" in message:
                        exec_res = message["executed_result"]
                        res_type = exec_res.get("type")
                        orig_query = message.get("original_user_query", st.session_state.current_analysis_artifacts.get("original_user_query", "æœªçŸ¥æŸ¥è©¢"))

                        # é¡¯ç¤ºè¡¨æ ¼
                        if res_type == "table":
                            if exec_res.get("data_path") and os.path.exists(exec_res["data_path"]):
                                try:
                                    df_disp = pd.read_csv(exec_res["data_path"])
                                    st.dataframe(df_disp)
                                    if st.button(f"ğŸ“Š ç‚ºæ­¤è¡¨æ ¼ç”¢ç”Ÿå ±å‘Š##{i}", key=f"rep_tbl_btn_{i}_tab4"):
                                        st.session_state.trigger_report_generation = True
                                        st.session_state.report_target_data_path = exec_res["data_path"]
                                        st.session_state.report_target_plot_path = None
                                        st.session_state.report_target_query = orig_query
                                        st.rerun()
                                except Exception as e:
                                    st.error(f"è¡¨æ ¼é¡¯ç¤ºéŒ¯èª¤ï¼š{e}")
                            elif exec_res.get("dataframe_result") is not None:
                                st.dataframe(exec_res.get("dataframe_result"))
                                st.caption("æ­¤è¡¨æ ¼ä¾†è‡ª DataFrame çµæœ")

                        # é¡¯ç¤ºåœ–è¡¨
                        elif res_type == "plot":
                            if exec_res.get("plot_path") and os.path.exists(exec_res["plot_path"]):
                                st.image(exec_res["plot_path"])
                                report_button_label = "ğŸ“„ ç‚ºæ­¤åœ–è¡¨ç”¢ç”Ÿå ±å‘Š"
                                target_data_for_report = None
                                if exec_res.get("data_path") and os.path.exists(exec_res["data_path"]):
                                    report_button_label += "ï¼ˆå«è³‡æ–™ï¼‰"
                                    target_data_for_report = exec_res["data_path"]

                                if st.button(f"{report_button_label}##{i}", key=f"rep_plot_btn_{i}_tab4"):
                                    st.session_state.trigger_report_generation = True
                                    st.session_state.report_target_data_path = target_data_for_report
                                    st.session_state.report_target_plot_path = exec_res["plot_path"]
                                    st.session_state.report_target_query = orig_query
                                    st.rerun()
                            else:
                                st.warning(f"æ‰¾ä¸åˆ°åœ–è¡¨ï¼š{exec_res.get('plot_path', 'æœªæŒ‡å®šè·¯å¾‘')}")

                        # é¡¯ç¤ºæ–‡å­—è¼¸å‡º
                        elif res_type == "text":
                            st.markdown(f"**è¼¸å‡ºå…§å®¹ï¼š**\n```\n{exec_res.get('value', 'ç„¡æ–‡å­—è¼¸å‡º')}\n```")

                        # é¡¯ç¤ºå ±å‘Š
                        elif res_type == "report_generated":
                            if exec_res.get("report_path") and os.path.exists(exec_res["report_path"]):
                                st.markdown(f"_å ±å‘Šå·²ç”¢ç”Ÿï¼š`{os.path.abspath(exec_res['report_path'])}`_")

                        # é¡¯ç¤ºè©•å¯©æŒ‰éˆ•
                        artifacts_for_judge = st.session_state.get("current_analysis_artifacts", {})
                        can_judge = artifacts_for_judge.get("generated_code") and (
                            artifacts_for_judge.get("executed_data_path") or
                            artifacts_for_judge.get("executed_dataframe_result") is not None or
                            artifacts_for_judge.get("plot_image_path") or
                            artifacts_for_judge.get("executed_text_output") or
                            (res_type == "text" and exec_res.get("value"))
                        )
                        if can_judge:
                            if st.button(f"âš–ï¸ è©•ä¼°æ­¤åˆ†æ##{i}", key=f"judge_btn_{i}_tab4"):
                                st.session_state.trigger_judging = True
                                st.rerun()

                    # é¡¯ç¤º Judge AI çš„è©•è«–
                    if message["role"] == "assistant" and "critique_text" in message:
                        with st.expander(f"æŸ¥çœ‹ {st.session_state.selected_judge_model} çš„è©•è«–", expanded=True):
                            st.markdown(message["critique_text"])
                        if st.button(f"ğŸ“„ åŒ¯å‡º PDF å ±å‘Š##{i}", key=f"pdf_exp_btn_{i}_tab4"):
                            pdf_artifacts = st.session_state.current_analysis_artifacts.copy()
                            if not pdf_artifacts.get("executed_dataframe_result") and pdf_artifacts.get("executed_data_path"):
                                try:
                                    pdf_artifacts["executed_dataframe_result"] = pd.read_csv(pdf_artifacts["executed_data_path"])
                                except:
                                    pass
                            with st.spinner("æ­£åœ¨ç”¢ç”Ÿ PDF..."):
                                pdf_path = export_analysis_to_pdf(pdf_artifacts)
                                if pdf_path and os.path.exists(pdf_path):
                                    with open(pdf_path, "rb") as f_pdf:
                                        st.download_button("ä¸‹è¼‰ PDF å ±å‘Š", f_pdf, os.path.basename(pdf_path), "application/pdf", key=f"dl_pdf_{i}_tab4_{datetime.datetime.now().timestamp()}")
                                    st.success(f"PDF å ±å‘Šç”¢ç”ŸæˆåŠŸï¼š{os.path.basename(pdf_path)}")
                                else:
                                    st.error("PDF ç”¢ç”Ÿå¤±æ•—")

                        if st.button(f"ğŸ“„ åŒ¯å‡º Bento HTML å ±å‘Š##{i}", key=f"html_exp_btn_{i}_tab4"):
                            st.session_state.trigger_html_export = True
                            st.rerun()

        # æœ€ä¸‹æ–¹è¼¸å…¥æ¬„ä½
        if user_query := st.chat_input("è«‹è¼¸å…¥åˆ†æéœ€æ±‚ï¼ˆWorker æ¨¡å‹å°‡è‡ªå‹•ç”¢ç”Ÿä¸¦åŸ·è¡Œç¨‹å¼ç¢¼ï¼‰...", key="user_query_input_tab4"):
            st.session_state.messages.append({"role": "user", "content": user_query})
            if st.session_state.current_dataframe is None or st.session_state.data_summary is None:
                st.warning("è«‹å…ˆå¾å´é‚Šæ¬„ä¸Šå‚³ä¸¦è™•ç† CSV æª”æ¡ˆã€‚")
                st.session_state.messages.append({"role": "assistant", "content": "è«‹å…ˆæä¾› CSV è³‡æ–™æ‰èƒ½é€²è¡Œåˆ†æã€‚"})
            else:
                worker_llm_chat = get_llm_instance(st.session_state.selected_worker_model)
                if not worker_llm_chat:
                    st.error(f"Worker æ¨¡å‹ {st.session_state.selected_worker_model} æœªå•Ÿç”¨")
                    st.session_state.messages.append({"role": "assistant", "content": f"Worker æ¨¡å‹ {st.session_state.selected_worker_model} ç„¡æ³•ä½¿ç”¨"})
                else:
                    st.session_state.current_analysis_artifacts = {"original_user_query": user_query}
                    st.session_state.trigger_code_generation = True
                    st.rerun()

    # --- åˆ†é äº”ï¼šPyGWalker ---
    with tab5:
        import streamlit as st
        import pygwalker as pyg
        import streamlit.components.v1 as components

        st.title('ğŸ›ï¸ PygWalker äº’å‹•å¼æ¢ç´¢')
        st.subheader('æ‹–æ”¾æ¬„ä½å³å¯ç”Ÿæˆåœ–è¡¨ï¼Œä¸¦å¯é€é AI å•ç­”é€²è¡Œåˆ†æ')

        # 1. æª¢æŸ¥ session_state ä¸­æ˜¯å¦å·²æœ‰è¼‰å…¥çš„ DataFrame
        if 'current_dataframe' in st.session_state and st.session_state.current_dataframe is not None:
            
            # 2. é¡¯ç¤ºæŒ‰éˆ•ï¼Œç•¶æŒ‰éˆ•è¢«é»æ“Šæ™‚ï¼Œå°‡ session_state çš„ç‹€æ…‹è¨­ç‚º True
            st.info("é»æ“Šä¸‹æ–¹æŒ‰éˆ•ä»¥è¼‰å…¥äº’å‹•åˆ†æä»‹é¢ã€‚è«‹æ³¨æ„ï¼Œè¼‰å…¥éç¨‹å¯èƒ½éœ€è¦å¹¾ç§’é˜ã€‚")
            if st.button("ğŸš€ å•Ÿå‹•äº’å‹•å¼åˆ†æ (PyGWalker)"):
                st.session_state.show_pygwalker = True

            # 3. åªæœ‰åœ¨æŒ‰éˆ•è¢«é»æ“Šå¾Œ (ç‹€æ…‹ç‚º True)ï¼Œæ‰é¡¯ç¤º PyGWalker
            if st.session_state.get('show_pygwalker', False):
                # å¾ session_state ç²å– DataFrame
                df = st.session_state.current_dataframe
                
                source_name = st.session_state.get('data_source_name', 'å·²è¼‰å…¥çš„è³‡æ–™')
                st.success(f"âœ… æ­£åœ¨åˆ†æã€Œ{source_name}ã€ã€‚")

                # ä½¿ç”¨ pygwalker ç”Ÿæˆ HTML ä¸¦åµŒå…¥
                pyg_html = pyg.walk(df, env='Streamlit', return_html=True, dark='dark')
                
                # åµŒå…¥å…ƒä»¶ä¸¦æä¾›ä¸€å€‹æŒ‰éˆ•ä¾†éš±è—å®ƒ
                components.html(pyg_html, height=800, scrolling=True)
                
                if st.button("æ”¶èµ·åˆ†æè¦–çª—"):
                    st.session_state.show_pygwalker = False
                    st.rerun() # ç«‹å³é‡æ–°æ•´ç†é é¢ä»¥éš±è—å…ƒä»¶

        else:
            # å¦‚æœ session_state ä¸­æ²’æœ‰ DataFrameï¼Œæç¤ºä½¿ç”¨è€…å…ˆå»ä¸Šå‚³æª”æ¡ˆ
            st.warning("âš ï¸ è«‹å…ˆåœ¨ã€Œè³‡æ–™ä¸Šå‚³ã€é é¢è¼‰å…¥æ‚¨çš„ CSV æª”æ¡ˆï¼Œæ‰èƒ½åœ¨æ­¤é€²è¡Œæ¢ç´¢ã€‚")

    # --- åˆ†é å…­ï¼šé‹ç‡Ÿè¨ºæ–·å ±å‘Š ---
    with tab6:
        if st.session_state.current_dataframe is not None:
            df = st.session_state.current_dataframe

            # æ”¶é›†æ•¸æ“šæ¦‚æ³ (åŠ å…¥æ¬„ä½æª¢æŸ¥)
            data_summary_lines = [f"1. æ•¸æ“šè¦æ¨¡ï¼šå…± {len(df)} æ¢è¨˜éŒ„"]
            if 'Item_Type' in df.columns:
                data_summary_lines.append(f"2. å•†å“é¡å‹ï¼š{', '.join(df['Item_Type'].unique())}")
            else:
                data_summary_lines.append("2. å•†å“é¡å‹ï¼šæœªæ‰¾åˆ° 'Item_Type' æ¬„ä½ã€‚")

            if 'Outlet_Identifier' in df.columns:
                data_summary_lines.append(f"3. é–€åº—æ•¸é‡ï¼š{len(df['Outlet_Identifier'].unique())} å®¶")
            else:
                data_summary_lines.append("3. é–€åº—æ•¸é‡ï¼šæœªæ‰¾åˆ° 'Outlet_Identifier' æ¬„ä½ã€‚")
            
            # ä½¿ç”¨åˆ—è¡¨æ¨å°å¼è™•ç†å¤šå€‹æ•¸å€¼æ¬„ä½ï¼Œé¿å…ç›´æ¥ KeyError
            numeric_cols_for_summary = {}
            if 'Item_MRP' in df.columns and pd.api.types.is_numeric_dtype(df['Item_MRP']):
                data_summary_lines.append(f"4. å¹³å‡å•†å“åƒ¹æ ¼ï¼š{df['Item_MRP'].mean():.2f}")
                numeric_cols_for_summary['Item_MRP'] = df['Item_MRP']
            else:
                data_summary_lines.append("4. å¹³å‡å•†å“åƒ¹æ ¼ï¼šæœªæ‰¾åˆ°æˆ–éæ•¸å€¼ 'Item_MRP' æ¬„ä½ã€‚")

            if 'Item_Weight' in df.columns and pd.api.types.is_numeric_dtype(df['Item_Weight']):
                data_summary_lines.append(f"5. å•†å“é‡é‡ç¯„åœï¼š{df['Item_Weight'].min():.2f} - {df['Item_Weight'].max():.2f}")
                numeric_cols_for_summary['Item_Weight'] = df['Item_Weight']
            else:
                data_summary_lines.append("5. å•†å“é‡é‡ç¯„åœï¼šæœªæ‰¾åˆ°æˆ–éæ•¸å€¼ 'Item_Weight' æ¬„ä½ã€‚")
            
            data_summary = "\n".join(data_summary_lines)

            # æ”¶é›†é¢¨éšªåˆ†æçµæœ (åŠ å…¥æ¬„ä½æª¢æŸ¥)
            risk_metrics = {}
            if 'Item_MRP' in numeric_cols_for_summary and not numeric_cols_for_summary['Item_MRP'].empty:
                # ç¢ºä¿åˆ†æ¯ä¸ç‚ºé›¶
                mrp_mean = numeric_cols_for_summary['Item_MRP'].mean()
                if mrp_mean != 0:
                    risk_metrics['éŠ·å”®é æ¸¬æº–ç¢ºåº¦'] = f"{1 - numeric_cols_for_summary['Item_MRP'].std() / mrp_mean:.2%}"
                else:
                    risk_metrics['éŠ·å”®é æ¸¬æº–ç¢ºåº¦'] = "ç„¡æ³•è¨ˆç®—ï¼ˆå¹³å‡åƒ¹æ ¼ç‚ºé›¶ï¼‰"
            else:
                risk_metrics['éŠ·å”®é æ¸¬æº–ç¢ºåº¦'] = "ç„¡æ³•è¨ˆç®—ï¼ˆç¼ºå°‘ 'Item_MRP'ï¼‰"

            if 'Item_Type' in df.columns and not df['Item_Type'].empty:
                unique_item_types = df['Item_Type'].unique()
                if len(unique_item_types) > 0:
                    risk_metrics['åº«å­˜é€±è½‰ç‡'] = f"{len(df) / len(unique_item_types):.2f}"
                    risk_metrics['å•†å“å¤šæ¨£æ€§'] = f"{len(unique_item_types)}"
                else:
                    risk_metrics['åº«å­˜é€±è½‰ç‡'] = "ç„¡æ³•è¨ˆç®—ï¼ˆç„¡å”¯ä¸€å•†å“é¡å‹ï¼‰"
                    risk_metrics['å•†å“å¤šæ¨£æ€§'] = "ç„¡æ³•è¨ˆç®—ï¼ˆç„¡å”¯ä¸€å•†å“é¡å‹ï¼‰"
            else:
                risk_metrics['åº«å­˜é€±è½‰ç‡'] = "ç„¡æ³•è¨ˆç®—ï¼ˆç¼ºå°‘ 'Item_Type'ï¼‰"
                risk_metrics['å•†å“å¤šæ¨£æ€§'] = "ç„¡æ³•è¨ˆç®—ï¼ˆç¼ºå°‘ 'Item_Type'ï¼‰"

            if 'Item_MRP' in numeric_cols_for_summary and not numeric_cols_for_summary['Item_MRP'].empty:
                mrp_max = numeric_cols_for_summary['Item_MRP'].max()
                mrp_min = numeric_cols_for_summary['Item_MRP'].min()
                if mrp_max != 0:
                    risk_metrics['åƒ¹æ ¼å€é–“è¦†è“‹ç‡'] = f"{(mrp_max - mrp_min) / mrp_max:.2%}"
                else:
                    risk_metrics['åƒ¹æ ¼å€é–“è¦†è“‹ç‡'] = "ç„¡æ³•è¨ˆç®—ï¼ˆæœ€å¤§åƒ¹æ ¼ç‚ºé›¶ï¼‰"
            else:
                risk_metrics['åƒ¹æ ¼å€é–“è¦†è“‹ç‡'] = "ç„¡æ³•è¨ˆç®—ï¼ˆç¼ºå°‘ 'Item_MRP'ï¼‰"

            risk_summary = "\n".join([f"{k}ï¼š{v}" for k, v in risk_metrics.items()])

            # ç”Ÿæˆè¨ºæ–·å ±å‘Š
            with st.spinner('æ­£åœ¨ç”Ÿæˆé‹ç‡Ÿè¨ºæ–·å ±å‘Š...'):
                diagnosis = generate_operation_diagnosis(data_summary, risk_summary)
            # é¡¯ç¤ºè¨ºæ–·å ±å‘Š
            st.markdown("## ğŸ“Š æ•¸æ“šæ¦‚æ³")
            st.text(data_summary)
            st.markdown("## âš ï¸ é¢¨éšªæŒ‡æ¨™")
            for metric, value in risk_metrics.items():
                st.metric(metric, value)
            st.markdown("## ğŸ“‹ é‹ç‡Ÿè¨ºæ–·å ±å‘Š")
            st.markdown(diagnosis)
            # ç”ŸæˆPDFå ±å‘Š
            pdf_data = generate_operation_diagnosis_pdf(data_summary, risk_summary, diagnosis)
            # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
            if pdf_data:
                st.download_button(
                    label="ä¸‹è¼‰è¨ºæ–·å ±å‘Š (PDF)",
                    data=pdf_data,
                    file_name="é‹ç‡Ÿè¨ºæ–·å ±å‘Š.pdf",
                    mime="application/pdf"
                )
        else:
            st.info('è«‹å…ˆåœ¨é¦–é ä¸Šå‚³æ•¸æ“šæ–‡ä»¶')
                    # --- åˆ†é ä¸ƒï¼šå•†æ¥­è«®è©¢æœå‹™å›é¥‹ ---
        with tab7:
            st.header("ğŸ“ è«®è©¢æœå‹™å›é¥‹")
            st.markdown("è«‹ç•™ä¸‹æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘å€‘å°‡ç›¡å¿«èˆ‡æ‚¨è¯ç¹«ï¼")

            col1, col2 = st.columns(2)
            with col1:
                user_name = st.text_input("å§“å")
                user_email = st.text_input("é›»å­éƒµä»¶")
            with col2:
                phone = st.text_input("é€£çµ¡é›»è©±")
                subject = st.text_input("ä¸»æ—¨", value="è«®è©¢æœå‹™å›é¥‹")

            message = st.text_area("éœ€æ±‚æˆ–ç•™è¨€", height=150)

            if st.button("âœ‰ï¸ é€å‡º"):
                if not user_email or not message:
                    st.warning("è«‹å¡«å¯«é›»å­éƒµä»¶èˆ‡ç•™è¨€å…§å®¹")
                else:
                    try:
                        import smtplib, ssl
                        from email.mime.text import MIMEText
                        from email.mime.multipart import MIMEMultipart

                        receiver = EMAIL_SENDER  # æ”¶ä»¶è€…ï¼Œå¯è‡ªè¡Œèª¿æ•´
                        mime_msg = MIMEMultipart()
                        mime_msg["From"] = EMAIL_SENDER
                        mime_msg["To"] = receiver
                        mime_msg["Subject"] = f"[è«®è©¢å›é¥‹] {subject}"

                        body = f"""è¦ªæ„›çš„å®¢æˆ¶æ‚¨å¥½ï¼Œ

                        æ„Ÿè¬æ‚¨ä½¿ç”¨æˆ‘å€‘çš„è«®è©¢æœå‹™ã€‚ä»¥ä¸‹æ˜¯æ‚¨çš„è«®è©¢å…§å®¹ï¼š

                        è«®è©¢é¡å‹ï¼š{subject}

                        æ‚¨çš„è«®è©¢å…§å®¹ï¼š
                        {message}
 
                        å¦‚æœæ‚¨æœ‰ä»»ä½•å•é¡Œï¼Œæ­¡è¿éš¨æ™‚èˆ‡æˆ‘å€‘è¯ç¹«ã€‚

                        ç¥ å•†ç¥º
                        æ‚¨çš„åˆ†æåœ˜éšŠ
                        """
                        mime_msg.attach(MIMEText(body, "plain", "utf-8"))
                        

                        context = ssl.create_default_context()
                        with smtplib.SMTP_SSL("smtp.gmail.com", 465, context=context) as smtp:
                            smtp.login(EMAIL_SENDER, EMAIL_PASSWORD)
                            smtp.sendmail(EMAIL_SENDER, receiver, mime_msg.as_string())

                        st.success("å·²æˆåŠŸé€å‡ºï¼Œæ„Ÿè¬æ‚¨çš„å›é¥‹ï¼")

                        # --- ä½¿ç”¨ AI è‡ªå‹•å›è¦†ä½¿ç”¨è€…ç•™è¨€ ---
                        with st.spinner("AI å›è¦†ç”Ÿæˆä¸­..."):
                            try:
                                ai_prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å•†æ¥­é¡§å•ï¼Œè«‹é‡å°ä»¥ä¸‹å®¢æˆ¶éœ€æ±‚æä¾›å‹å–„ä¸”å…·é«”çš„ä¸­æ–‡å›è¦†èˆ‡å»ºè­°ï¼š\n\nå®¢æˆ¶éœ€æ±‚ï¼š\n{message}\n\nå›è¦†ï¼š"""
                                ai_response = client.chat.completions.create(
                                    model="gpt-4o-mini",
                                    messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å•†æ¥­é¡§å•ï¼Œæ“…é•·ä»¥æ¸…æ™°ã€å…·é«”çš„æ–¹å¼è§£æ±ºå®¢æˆ¶å•é¡Œã€‚"},
                                              {"role": "user", "content": ai_prompt}],
                                    temperature=0.7,
                                    max_tokens=500
                                )
                                ai_reply = ai_response.choices[0].message.content.strip()
                            except Exception as e:
                                ai_reply = f"ç”Ÿæˆå›è¦†æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
                        st.markdown("---")
                        st.subheader("ğŸ¤– AI å›è¦†")
                        st.markdown(ai_reply)
                    except Exception as e:
                        st.error(f"å¯„é€å¤±æ•—ï¼š{e}")




# --- è‹¥å°šæœªä¸Šå‚³è³‡æ–™ï¼Œé¡¯ç¤ºæç¤ºèˆ‡éå¾€å°è©± ---

elif st.session_state.active_page == 'operation_optimization':
    st.markdown("<h1 style='text-align: center;'>é‹ç‡Ÿå„ªåŒ–</h1>", unsafe_allow_html=True)
    
    if st.session_state.df is not None:
        # --- é€šç”¨æ•¸æ“šæ¦‚æ³èˆ‡é¢¨éšª ---
        data_summary, risk_metrics = summarize_dataframe_generic(st.session_state.df)
        risk_summary = "\n".join([f"{k}ï¼š{v}" for k, v in risk_metrics.items()])
        
        # ç”Ÿæˆè¨ºæ–·å ±å‘Š
        with st.spinner('æ­£åœ¨ç”Ÿæˆé‹ç‡Ÿè¨ºæ–·å ±å‘Š...'):
            diagnosis = generate_operation_diagnosis(data_summary, risk_summary)
            
        # é¡¯ç¤ºè¨ºæ–·å ±å‘Š
        st.markdown("## ğŸ“Š æ•¸æ“šæ¦‚æ³")
        st.text(data_summary)
        
        st.markdown("## âš ï¸ é¢¨éšªæŒ‡æ¨™")
        for metric, value in risk_metrics.items():
            st.metric(metric, value)
        
        st.markdown("## ğŸ“‹ é‹ç‡Ÿè¨ºæ–·å ±å‘Š")
        st.markdown(diagnosis)
        
        # ç”ŸæˆPDFå ±å‘Š
        pdf_data = generate_pdf_report(data_summary, risk_summary, diagnosis)
        
        # æ·»åŠ ä¸‹è¼‰æŒ‰éˆ•
        st.download_button(
            label="ä¸‹è¼‰è¨ºæ–·å ±å‘Š (PDF)",
            data=pdf_data,
            file_name="é‹ç‡Ÿè¨ºæ–·å ±å‘Š.pdf",
            mime="application/pdf"
        )
        
    else:
        st.info('è«‹å…ˆåœ¨é¦–é ä¸Šå‚³æ•¸æ“šæ–‡ä»¶')



else:
    st.info("ğŸ‘‹ æ­¡è¿ï¼è«‹ä½¿ç”¨å´é‚Šæ¬„ä¸Šå‚³ CSV æª”æ¡ˆï¼ˆä¹Ÿå¯ä»¥ä¸Šå‚³åŒ…å«æ¬„ä½æè¿°çš„ .txt æª”ï¼‰ä»¥é–‹å§‹åˆ†ææµç¨‹ã€‚")
    for i, message in enumerate(st.session_state.messages):
        if message["role"] == "assistant":
            with st.chat_message(message["role"]):
                st.markdown(message["content"])






# --- ç¨‹å¼ç¢¼ç”Ÿæˆé‚è¼¯ ---
if st.session_state.get("trigger_code_generation", False):
    st.session_state.trigger_code_generation = False  # é—œé–‰è§¸ç™¼å™¨ï¼Œé¿å…é‡è¤‡è§¸ç™¼
    user_query = st.session_state.messages[-1]["content"]  # å–å¾—ä½¿ç”¨è€…çš„æœ€å¾Œä¸€å‰‡è¼¸å…¥ä½œç‚ºæŸ¥è©¢

    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        gen_code_str = ""

        # é¡¯ç¤ºæ­£åœ¨ç”¢ç”Ÿç¨‹å¼ç¢¼çš„æç¤º
        msg_placeholder.markdown(
            f"â³ **{st.session_state.selected_worker_model}** æ­£åœ¨ç‚ºä¸‹åˆ—éœ€æ±‚ç”Ÿæˆç¨‹å¼ç¢¼ï¼š'{html.escape(user_query)}'...")
        with st.spinner(f"{st.session_state.selected_worker_model} æ­£åœ¨ç”¢ç”Ÿ Python ç¨‹å¼ç¢¼..."):
            try:
                worker_llm_code_gen = get_llm_instance(st.session_state.selected_worker_model)
                mem_ctx = st.session_state.lc_memory.load_memory_variables({})
                data_sum_prompt = json.dumps(st.session_state.data_summary, indent=2) if st.session_state.data_summary else "{}"

                # å°‡ä½¿ç”¨è€…éœ€æ±‚ã€è³‡æ–™æ‘˜è¦èˆ‡èŠå¤©è¨˜æ†¶å¸¶å…¥æç¤ºæ¨¡æ¿ä¸­
                formatted_code_gen_prompt = code_generation_prompt_template.format_prompt(
                    data_summary=data_sum_prompt,
                    user_query=user_query,
                    chat_history=mem_ctx.get("chat_history", "")
                )

                # å‘¼å« LLM æ¨¡å‹ç”Ÿæˆç¨‹å¼ç¢¼
                response = worker_llm_code_gen.invoke(formatted_code_gen_prompt)
                gen_code_str = response.content if hasattr(response, 'content') else response.get('text', "")

                # ç§»é™¤ markdown èªæ³•æ¨™è¨˜ï¼ˆ```pythonï¼‰
                for prefix in ["```python\n", "```python", "```\n", "```"]:
                    if gen_code_str.startswith(prefix):
                        gen_code_str = gen_code_str[len(prefix):]
                if gen_code_str.endswith("\n```"):
                    gen_code_str = gen_code_str[:-len("\n```")]
                elif gen_code_str.endswith("```"):
                    gen_code_str = gen_code_str[:-len("```")]
                gen_code_str = gen_code_str.strip()

                # å„²å­˜ç”Ÿæˆçš„ç¨‹å¼ç¢¼åˆ°åˆ†æç‰©ä»¶ä¸­
                st.session_state.current_analysis_artifacts["generated_code"] = gen_code_str

                # é¡¯ç¤ºç¨‹å¼ç¢¼å…§å®¹ï¼Œä¸¦æç¤ºå³å°‡åŸ·è¡Œ
                assist_base_content = f"ğŸ” **{st.session_state.selected_worker_model} é‡å° '{html.escape(user_query)}' æ‰€ç”¢ç”Ÿçš„ç¨‹å¼ç¢¼å¦‚ä¸‹ï¼š**\n```python\n{gen_code_str}\n```\n"
                msg_placeholder.markdown(assist_base_content + "\nâ³ æ­£åœ¨åŸ·è¡Œç¨‹å¼ç¢¼...")
            except Exception as e:
                # éŒ¯èª¤è™•ç†
                err_msg = f"ç”¢ç”Ÿç¨‹å¼ç¢¼æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{html.escape(str(e))}"
                st.error(err_msg)
                st.session_state.messages.append({"role": "assistant", "content": err_msg})
                st.session_state.lc_memory.save_context({"user_query": user_query},
                                                        {"output": f"ç¨‹å¼ç¢¼ç”¢ç”ŸéŒ¯èª¤ï¼š{e}"})
                if msg_placeholder:
                    msg_placeholder.empty()
                st.rerun()

        if gen_code_str:
            curr_assist_resp_msg = {"role": "assistant", "content": assist_base_content,
                                    "original_user_query": user_query}
            with st.spinner("æ­£åœ¨åŸ·è¡Œç”Ÿæˆçš„ Python ç¨‹å¼ç¢¼..."):
                # åŸ·è¡Œç”Ÿæˆçš„ç¨‹å¼ç¢¼
                exec_result = code_executor.execute_code(gen_code_str, st.session_state.current_dataframe.copy())

                # æ ¹æ“šåŸ·è¡Œçµæœæ›´æ–°åˆ†æç‰©ä»¶ä¸­çš„æ¬„ä½
                if exec_result.get("data_path"):
                    st.session_state.current_analysis_artifacts["executed_data_path"] = exec_result["data_path"]
                if exec_result.get("plot_path"):
                    st.session_state.current_analysis_artifacts["plot_image_path"] = exec_result["plot_path"]
                if exec_result.get("type") == "text" and exec_result.get("value"):
                    st.session_state.current_analysis_artifacts["executed_text_output"] = exec_result.get("value")
                if exec_result.get("dataframe_result") is not None:
                    st.session_state.current_analysis_artifacts["executed_dataframe_result"] = exec_result.get("dataframe_result")

                # ç‰¹æ®Šè™•ç† plot_specific_data_dfï¼šè‹¥åœ–è¡¨ç”¨è³‡æ–™å­˜åœ¨ï¼Œå°±è®€å…¥
                if exec_result.get("type") == "plot" and exec_result.get("data_path") and "plot_data_for" in os.path.basename(exec_result.get("data_path")):
                    try:
                        st.session_state.current_analysis_artifacts["plot_specific_data_df"] = pd.read_csv(exec_result.get("data_path"))
                    except Exception as e:
                        st.warning(f"ç„¡æ³•è®€å–åœ–è¡¨è³‡æ–™ plot_specific_data_dfï¼š{exec_result.get('data_path')}ï¼ŒéŒ¯èª¤ï¼š{e}")

                # æ ¹æ“šåŸ·è¡Œçµæœé¡¯ç¤ºè¨Šæ¯èˆ‡è¨˜æ†¶
                llm_mem_output = ""
                if exec_result["type"] == "error":
                    curr_assist_resp_msg["content"] += f"\nâš ï¸ **åŸ·è¡ŒéŒ¯èª¤ï¼š**\n```\n{html.escape(exec_result['message'])}\n```"
                    if str(st.session_state.current_analysis_artifacts.get("executed_text_output", "")).startswith("Code executed, but"):
                        st.session_state.current_analysis_artifacts["executed_text_output"] = f"åŸ·è¡ŒéŒ¯èª¤ï¼š{html.escape(str(exec_result.get('final_analysis_result_value', 'æœªçŸ¥éŒ¯èª¤')))}"
                    llm_mem_output = f"åŸ·è¡ŒéŒ¯èª¤ï¼š{html.escape(exec_result['message'][:100])}..."
                else:
                    curr_assist_resp_msg["content"] += "\nâœ… **ç¨‹å¼ç¢¼åŸ·è¡ŒæˆåŠŸï¼**"
                    curr_assist_resp_msg["executed_result"] = exec_result

                    # é¡å¤–è£œå……æª”æ¡ˆä½ç½®
                    if exec_result.get("data_path"):
                        curr_assist_resp_msg["content"] += f"\nğŸ’¾ è³‡æ–™å„²å­˜æ–¼ï¼š`{os.path.abspath(exec_result['data_path'])}`"
                    if exec_result.get("plot_path"):
                        curr_assist_resp_msg["content"] += f"\nğŸ–¼ï¸ åœ–è¡¨å„²å­˜æ–¼ï¼š`{os.path.abspath(exec_result['plot_path'])}`"
                    if exec_result.get("data_path") and "plot_data_for" in os.path.basename(exec_result.get("data_path", "")):
                        curr_assist_resp_msg["content"] += "ï¼ˆæ­¤ç‚ºåœ–è¡¨å°ˆç”¨è³‡æ–™ï¼‰"

                    # ç”¢å‡ºæ‘˜è¦ä¾›è¨˜æ†¶ä½¿ç”¨
                    if exec_result["type"] == "table":
                        llm_mem_output = f"å·²ç”¢ç”Ÿè³‡æ–™è¡¨ï¼š{os.path.basename(exec_result.get('data_path', 'N/A'))}"
                    elif exec_result["type"] == "plot":
                        llm_mem_output = f"å·²ç”¢ç”Ÿåœ–è¡¨ï¼š{os.path.basename(exec_result.get('plot_path', 'N/A'))}"
                        if exec_result.get("data_path"):
                            llm_mem_output += f"ï¼ˆåœ–è¡¨è³‡æ–™ï¼š{os.path.basename(exec_result.get('data_path'))}ï¼‰"
                    elif exec_result["type"] == "text":
                        llm_mem_output = f"å·²ç”¢ç”Ÿæ–‡å­—è¼¸å‡ºï¼š{str(exec_result.get('value', ''))[:50]}..."
                    else:
                        llm_mem_output = "ç¨‹å¼åŸ·è¡Œå®Œæˆï¼Œä½†ç„¡æ³•è¾¨è­˜çµæœé¡å‹ã€‚"

                # å„²å­˜è¨˜æ†¶èˆ‡æ›´æ–°è¨Šæ¯è¨˜éŒ„
                st.session_state.lc_memory.save_context(
                    {"user_query": f"{user_query}\n---ç”Ÿæˆçš„ç¨‹å¼ç¢¼---\n{gen_code_str}\n---çµæŸ---"},
                    {"output": llm_mem_output})
                st.session_state.messages.append(curr_assist_resp_msg)
                if msg_placeholder:
                    msg_placeholder.empty()
                st.rerun()




# --- å ±å‘Šç”Ÿæˆé‚è¼¯ ---
if st.session_state.get("trigger_report_generation", False):  # å¦‚æœè§¸ç™¼ç”Ÿæˆå ±å‘Š
    st.session_state.trigger_report_generation = False  # é—œé–‰è§¸ç™¼å™¨ï¼Œé¿å…é‡è¤‡åŸ·è¡Œ

    # å–å¾—å¿…è¦çš„è³‡è¨Š
    data_path_rep = st.session_state.get("report_target_data_path")  # å ±å‘Šæ‰€ä¾æ“šçš„è³‡æ–™è·¯å¾‘
    plot_path_rep = st.session_state.get("report_target_plot_path")  # åœ–è¡¨åœ–ç‰‡è·¯å¾‘
    query_led_to_data = st.session_state.report_target_query         # è§¸ç™¼é€™æ¬¡å ±å‘Šçš„ä½¿ç”¨è€…æŸ¥è©¢å…§å®¹
    worker_llm_rep = get_llm_instance(st.session_state.selected_worker_model)  # å–å¾— LLM å¯¦ä¾‹

    # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ è³‡è¨Šå¯ç”Ÿæˆå ±å‘Š
    if not worker_llm_rep or not st.session_state.data_summary or (
            not data_path_rep and not plot_path_rep):
        st.error("ç„¡æ³•ç”Ÿæˆå ±å‘Šï¼šç¼ºå°‘ LLM æ¨¡å‹ã€è³‡æ–™æ‘˜è¦æˆ–è³‡æ–™/åœ–è¡¨è·¯å¾‘ã€‚")
    else:
        # é è¨­ç‚ºæ–‡å­—æˆ–åœ–ç‰‡æè¿°å ±å‘Š
        csv_or_text_content_rep = "ç„¡å¯ç”¨è³‡æ–™ï¼Œåƒ…ç‚ºæè¿°åœ–è¡¨æˆ–ç¼ºä¹æ•¸æ“šã€‚"
        if data_path_rep and os.path.exists(data_path_rep):
            try:
                with open(data_path_rep, 'r', encoding='utf-8') as f_rep_data:
                    csv_or_text_content_rep = f_rep_data.read()
            except Exception as e_read:
                st.error(f"è®€å–å ±å‘Šç”¨è³‡æ–™æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{html.escape(str(e_read))}")
                st.rerun()
        elif not data_path_rep and plot_path_rep:
            st.info("åƒ…ç”Ÿæˆåœ–è¡¨çš„æè¿°æ€§å ±å‘Šã€‚")

        # å»ºç«‹åœ–è¡¨èªªæ˜å­—ä¸²
        plot_info_for_prompt = "ç„¡åœ–è¡¨å¯ç”¨"
        if plot_path_rep and os.path.exists(plot_path_rep):
            plot_info_for_prompt = f"åœ–è¡¨åœ–ç‰‡å¯æ–¼ '{os.path.basename(plot_path_rep)}' å–å¾—ã€‚"
            if data_path_rep and "plot_data_for" in os.path.basename(data_path_rep):
                plot_info_for_prompt += f" èˆ‡åœ–è¡¨ç›¸é—œçš„è³‡æ–™ä½æ–¼ '{os.path.basename(data_path_rep)}'ã€‚"

        # é¡¯ç¤º Spinner èˆ‡è¨Šæ¯æç¤º
        with st.chat_message("assistant"):
            rep_spinner_container = st.empty()
            rep_spinner_container.markdown(
                f"ğŸ“ **{st.session_state.selected_worker_model}** æ­£åœ¨æ’°å¯«å ±å‘Šï¼š'{html.escape(query_led_to_data)}'...")
            with st.spinner("æ­£åœ¨ç”Ÿæˆå ±å‘Š..."):
                try:
                    mem_ctx = st.session_state.lc_memory.load_memory_variables({})
                    data_sum_prompt = json.dumps(st.session_state.data_summary, indent=2) if st.session_state.data_summary else "{}"

                    # çµ„åˆæç¤ºå…§å®¹
                    prompt_inputs = {
                        "table_data_csv_or_text": csv_or_text_content_rep,
                        "original_data_summary": data_sum_prompt,
                        "user_query_that_led_to_data": query_led_to_data,
                        "chat_history": mem_ctx.get("chat_history", ""),
                        "plot_info_if_any": plot_info_for_prompt
                    }

                    # å‘¼å«æ¨¡å‹ç”Ÿæˆå ±å‘Š
                    response = worker_llm_rep.invoke(report_generation_prompt_template.format_prompt(**prompt_inputs))
                    rep_text = response.content if hasattr(response, 'content') else response.get('text', "å ±å‘Šç”Ÿæˆå¤±æ•—ã€‚")

                    # å„²å­˜å ±å‘Šæ–‡å­—
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    safe_query = "".join(c if c.isalnum() else "_" for c in query_led_to_data[:30])
                    filepath = os.path.join(TEMP_DATA_STORAGE, f"report_for_{safe_query}_{timestamp}.txt")
                    with open(filepath, "w", encoding='utf-8') as f_write_rep:
                        f_write_rep.write(rep_text)

                    # æ›´æ–° artifacts èˆ‡ç‹€æ…‹
                    st.session_state.current_analysis_artifacts["generated_report_path"] = filepath
                    st.session_state.current_analysis_artifacts["report_query"] = query_led_to_data

                    # åŠ å…¥èŠå¤©è¨Šæ¯èˆ‡ä¿å­˜è¨˜æ†¶
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"ğŸ“Š **{st.session_state.selected_worker_model} é‡å° '{html.escape(query_led_to_data)}' æ‰€ç”¢ç”Ÿçš„å ±å‘Šå…§å®¹ï¼š**\n\n{rep_text}",
                        "original_user_query": query_led_to_data,
                        "executed_result": {
                            "type": "report_generated",
                            "report_path": filepath,
                            "data_source_path": data_path_rep or "ç„¡",
                            "plot_source_path": plot_path_rep or "ç„¡"
                        }
                    })
                    st.session_state.lc_memory.save_context(
                        {"user_query": f"ä½¿ç”¨è€…è«‹æ±‚ç”¢ç”Ÿå ±å‘Šï¼š'{query_led_to_data}'"},
                        {"output": f"ç”¢å‡ºå ±å‘Šå…§å®¹ï¼ˆå‰100å­—ï¼‰ï¼š{rep_text[:100]}..."}
                    )
                    if rep_spinner_container:
                        rep_spinner_container.empty()
                    st.rerun()
                except Exception as e_rep_gen:
                    # éŒ¯èª¤è™•ç†
                    err_msg_rep = f"ç”¢ç”Ÿå ±å‘Šéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{html.escape(str(e_rep_gen))}"
                    st.error(err_msg_rep)
                    st.session_state.messages.append({"role": "assistant", "content": err_msg_rep})
                    if rep_spinner_container:
                        rep_spinner_container.empty()
                    st.rerun()

        # æœ€å¾Œæ¸…é™¤å ±å‘Šç”¨çš„æš«å­˜è³‡æ–™è·¯å¾‘
        for key in ["report_target_data_path", "report_target_plot_path", "report_target_query"]:
            if key in st.session_state:
                del st.session_state[key]


# --- è©•åˆ¤é‚è¼¯ ---
if st.session_state.get("trigger_judging", False):  # å¦‚æœè§¸ç™¼è©•åˆ¤
    st.session_state.trigger_judging = False  # é‡è¨­è§¸ç™¼å™¨

    # è®€å–ç›¸é—œè³‡è¨Š
    artifacts_judge = st.session_state.current_analysis_artifacts
    judge_llm_instance = get_llm_instance(st.session_state.selected_judge_model)
    orig_query_artifacts = artifacts_judge.get("original_user_query", "ï¼ˆç„¡åŸå§‹æŸ¥è©¢å…§å®¹ï¼‰")

    # è‹¥æ¨¡å‹ä¸å¯ç”¨æˆ–ç„¡ä»£ç¢¼å¯è©•ï¼Œé¡¯ç¤ºéŒ¯èª¤
    if not judge_llm_instance or not artifacts_judge.get("generated_code"):
        st.error("ç„¡æ³•å–å¾—è©•åˆ¤æ¨¡å‹æˆ–æœªæ‰¾åˆ°å¯ä¾›è©•ä¼°çš„ AI ç”Ÿæˆç¨‹å¼ç¢¼ã€‚")
    else:
        try:
            code_content = artifacts_judge.get("generated_code", "ï¼ˆæœªæä¾› Python ç¨‹å¼ç¢¼ï¼‰")

            # è³‡æ–™å…§å®¹ï¼Œé è¨­ç‚ºç„¡è³‡æ–™
            data_content_for_judge = "å°šæœªæä¾› AI å·¥ä½œè€…è¼¸å‡ºçš„è³‡æ–™æˆ–æª”æ¡ˆã€‚"
            if artifacts_judge.get("executed_dataframe_result") is not None:
                data_content_for_judge = f"è³‡æ–™æ¡†æ¶çµæœï¼ˆåƒ…é¡¯ç¤ºå‰ 5 ç­†ï¼‰ï¼š\n{artifacts_judge['executed_dataframe_result'].head().to_string()}"
            elif artifacts_judge.get("executed_data_path") and os.path.exists(artifacts_judge["executed_data_path"]):
                with open(artifacts_judge["executed_data_path"], 'r', encoding='utf-8') as f_data_judge:
                    data_content_for_judge = f_data_judge.read(500)
                    if len(data_content_for_judge) == 500:
                        data_content_for_judge += "\n...ï¼ˆå…§å®¹æˆªæ–·ï¼‰"
            elif artifacts_judge.get("executed_text_output"):
                data_content_for_judge = f"AI å·¥ä½œè€…è¼¸å‡ºæ–‡å­—ï¼š{artifacts_judge.get('executed_text_output')}"

            # åˆ†æå ±å‘Šå…§å®¹
            report_content_judge = "å°šæœªç”¢ç”Ÿæ–‡å­—å ±å‘Šã€‚"
            if artifacts_judge.get("generated_report_path") and os.path.exists(artifacts_judge["generated_report_path"]):
                with open(artifacts_judge["generated_report_path"], 'r', encoding='utf-8') as f_report_judge:
                    report_content_judge = f_report_judge.read()

            # åœ–åƒè³‡æ–™
            plot_img_path_judge = artifacts_judge.get("plot_image_path", "N/A")
            plot_info_for_judge_prompt = "æœªç”¢ç”Ÿåœ–è¡¨æˆ–æœªæåŠåœ–åƒã€‚"
            if plot_img_path_judge != "N/A":
                if os.path.exists(plot_img_path_judge):
                    plot_info_for_judge_prompt = f"å·²ç”¢ç”Ÿåœ–è¡¨ï¼Œä½ç½®ï¼š'{os.path.basename(plot_img_path_judge)}'ã€‚"
                    if artifacts_judge.get("plot_specific_data_df") is not None and not artifacts_judge.get("plot_specific_data_df").empty:
                        plot_info_for_judge_prompt += f" å°æ‡‰åœ–è¡¨è³‡æ–™ï¼ˆå‰ 5 ç­†ï¼‰ï¼š\n{artifacts_judge['plot_specific_data_df'].head().to_string()}"
                    elif artifacts_judge.get("executed_data_path") and "plot_data_for" in os.path.basename(artifacts_judge.get("executed_data_path", "")):
                        plot_info_for_judge_prompt += f" å°æ‡‰åœ–è¡¨è³‡æ–™æª”ï¼š'{os.path.basename(artifacts_judge.get('executed_data_path'))}'ã€‚"
                else:
                    plot_info_for_judge_prompt = f"é æœŸåœ–åƒæª” '{os.path.basename(plot_img_path_judge)}' ä¸¦æœªæ‰¾åˆ°ã€‚"

            # è©•åˆ¤éç¨‹ï¼šé¡¯ç¤º Spinner èˆ‡æç¤º
            with st.chat_message("assistant"):
                critique_spinner_container = st.empty()
                critique_spinner_container.markdown(
                    f"âš–ï¸ **{st.session_state.selected_judge_model}** æ­£åœ¨å°ä»¥ä¸‹æŸ¥è©¢é€²è¡Œè©•åƒ¹ï¼š'{html.escape(orig_query_artifacts)}'...")
                with st.spinner("æ­£åœ¨ç”Ÿæˆè©•åƒ¹..."):
                    data_sum_prompt = json.dumps(st.session_state.data_summary, indent=2) if st.session_state.data_summary else "{}"

                    # å»ºæ§‹æç¤ºå…§å®¹
                    judge_inputs = {
                        "python_code": code_content,
                        "data_content_for_judge": data_content_for_judge,
                        "report_text_content": report_content_judge,
                        "original_user_query": orig_query_artifacts,
                        "data_summary": data_sum_prompt,
                        "plot_image_path": plot_img_path_judge,
                        "plot_info_for_judge": plot_info_for_judge_prompt
                    }

                    # å‘¼å« LLM è©•åˆ¤æ¨¡å‹
                    response = judge_llm_instance.invoke(judging_prompt_template.format_prompt(**judge_inputs))
                    critique_text = response.content if hasattr(response, 'content') else response.get('text', "è©•åƒ¹ç”Ÿæˆå¤±æ•—ã€‚")

                    # å„²å­˜è©•åƒ¹çµæœ
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    safe_query = "".join(c if c.isalnum() else "_" for c in orig_query_artifacts[:30])
                    critique_filepath = os.path.join(TEMP_DATA_STORAGE, f"critique_on_{safe_query}_{timestamp}.txt")
                    with open(critique_filepath, "w", encoding='utf-8') as f_critique:
                        f_critique.write(critique_text)

                    # æ›´æ–°ç‹€æ…‹ä¸¦é¡¯ç¤ºè¨Šæ¯
                    st.session_state.current_analysis_artifacts["generated_critique_path"] = critique_filepath
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"âš–ï¸ **æ¨¡å‹ {st.session_state.selected_judge_model} å°æŸ¥è©¢ '{html.escape(orig_query_artifacts)}' çš„è©•åƒ¹å·²ç”¢ç”Ÿï¼ˆæª”æ¡ˆå„²å­˜æ–¼ `{os.path.abspath(critique_filepath)}`ï¼‰ï¼š**",
                        "critique_text": critique_text
                    })
                    st.session_state.lc_memory.save_context(
                        {"user_query": f"å·²è«‹æ±‚è©•åƒ¹ï¼š'{orig_query_artifacts}'"},
                        {"output": f"è©•åƒ¹å‰ 100 å­—ï¼š{critique_text[:100]}..."}
                    )
                    if critique_spinner_container:
                        critique_spinner_container.empty()
                    st.rerun()

        except Exception as e_judge:
            # æ•æ‰ä¾‹å¤–ä¸¦æç¤º
            err_msg_judge = f"è©•åƒ¹éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{html.escape(str(e_judge))}"
            st.error(err_msg_judge)
            st.session_state.messages.append({"role": "assistant", "content": err_msg_judge})
            if 'critique_spinner_container' in locals() and critique_spinner_container:
                critique_spinner_container.empty()
            st.rerun()




# --- HTML å ±å‘ŠåŒ¯å‡ºé‚è¼¯ ---
if st.session_state.get("trigger_html_export", False):
    # é‡è¨­è§¸ç™¼ç‹€æ…‹ï¼Œæº–å‚™ç›¸é—œè³‡æ–™
    st.session_state.trigger_html_export = False  # é‡è¨­åŒ¯å‡ºç‹€æ…‹
    artifacts_html = st.session_state.current_analysis_artifacts  # åŒ…å«ç¨‹å¼ç¢¼èˆ‡åˆ†æçµæœçš„è³‡è¨Š
    cdo_report_html = st.session_state.get("cdo_initial_report_text")  # CDO åˆæ­¥å ±å‘Šå…§å®¹ï¼ˆç´”æ–‡å­—ï¼‰
    data_summary_html_dict = st.session_state.get("data_summary")  # è³‡æ–™æ‘˜è¦ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    main_df_for_html = st.session_state.get("current_dataframe")  # ä¸»è³‡æ–™è¡¨ï¼ˆDataFrameï¼‰
    # é©—è­‰é—œéµè³‡æ–™æ˜¯å¦é½Šå…¨
    if not artifacts_html or not cdo_report_html or not data_summary_html_dict or main_df_for_html is None:
        st.error(
            "ç„¡æ³•ç”¢ç”Ÿ HTMLï¼šç¼ºå°‘é—œéµè³‡è¨Šï¼ˆåˆ†æçµæœã€CDO å ±å‘Šã€è³‡æ–™æ‘˜è¦æˆ–ä¸»è¦è³‡æ–™é›†ï¼‰ã€‚è«‹ç¢ºèªå·²ä¸Šå‚³ CSVï¼ŒåŸ·è¡Œé CDO æµç¨‹èˆ‡åˆ†ææŒ‡ä»¤ã€‚")
    # ç”Ÿæˆ HTML å ±å‘Šä¸¦æä¾›ä¸‹è¼‰
    else:
        with st.chat_message("assistant"):
            html_spinner_container = st.empty()  # å ä½é¡¯ç¤ºå€
            html_spinner_container.markdown(f"â³ æ­£åœ¨ç”¢ç”Ÿ Bento é¢¨æ ¼ HTML å ±å‘Š...")
            with st.spinner("ç”¢ç”Ÿ HTML å ±å‘Šä¸­..."):
                html_file_path = generate_bento_html_report_python(artifacts_html, cdo_report_html,
                                                                   data_summary_html_dict, main_df_for_html)

            # è‹¥ç”¢ç”ŸæˆåŠŸï¼Œé¡¯ç¤ºä¸‹è¼‰æŒ‰éˆ•èˆ‡æˆåŠŸè¨Šæ¯
                if html_file_path and os.path.exists(html_file_path):
                    st.session_state.current_analysis_artifacts["generated_html_report_path"] = html_file_path
                    with open(html_file_path, "rb") as fp_html:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è¼‰ Bento HTML å ±å‘Š",
                            data=fp_html,
                            file_name=os.path.basename(html_file_path),
                            mime="text/html",
                            key=f"download_html_{datetime.datetime.now().timestamp()}"
                        )

                    success_msg_html = f"Bento HTML å ±å‘Šå·²ç”¢ç”Ÿï¼š**{os.path.basename(html_file_path)}**ã€‚"
                    st.success(success_msg_html)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"ğŸ“„ {success_msg_html}ï¼ˆå®Œæ•´è·¯å¾‘ï¼š`{os.path.abspath(html_file_path)}`ï¼‰"
                    })

# è‹¥å¤±æ•—ï¼Œé¡¯ç¤ºéŒ¯èª¤è¨Šæ¯
                else:
                    error_msg_html = "ç”¢ç”Ÿ Bento HTML å ±å‘Šå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚"
                    st.error(error_msg_html)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": f"æŠ±æ­‰ï¼Œç”¢ç”Ÿ HTML å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚éŒ¯èª¤è¨Šæ¯ï¼š{error_msg_html}"
                    })

                if html_spinner_container:
                    html_spinner_container.empty()
