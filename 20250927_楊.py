import streamlit as st
import pandas as pd
import os
import io
import time
import dotenv
from PIL import Image
import numpy as np
import json
import re

# --- Plotly å’Œ Gemini/Langchain/OpenAI ç­‰æ ¸å¿ƒå¥—ä»¶ ---
import plotly.express as px
import google.generativeai as genai
from openai import OpenAI
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    """å„²å­˜ä¸Šå‚³çš„æª”æ¡ˆåˆ°æŒ‡å®šç›®éŒ„"""
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    """è™•ç†åœ–ç‰‡ä¸Šå‚³ï¼Œæº–å‚™éš¨ä¸‹ä¸€å‰‡è¨Šæ¯ç™¼é€"""
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- RAG æ ¸å¿ƒå‡½å¼ ---
@st.cache_resource
def create_lc_retriever(file_path: str, openai_api_key: str):
    """å»ºç«‹ LangChain çš„ RAG æª¢ç´¢å™¨"""
    with st.status("æ­£åœ¨å»ºç«‹ RAG çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šè¼‰å…¥èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            loader = CSVLoader(file_path=file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")
            
            status.update(label="æ­¥é©Ÿ 2/3ï¼šå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            vector_store = LangChainFAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")
            
            status.update(label="æ­¥é©Ÿ 3/3ï¼šæª¢ç´¢å™¨æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            return vector_store.as_retriever(search_kwargs={'k': 5})
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

# --- Gemini API ç›¸é—œå‡½å¼ ---
def get_gemini_client(api_key):
    """å–å¾— Gemini å®¢æˆ¶ç«¯"""
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-2.5-flash")

def get_gemini_response_with_history(client, history, user_prompt):
    """å¸¶æœ‰æ­·å²ç´€éŒ„çš„ Gemini å°è©±"""
    gemini_history = []
    if not isinstance(history, list): history = []
    for msg in history:
        role = "user" if msg.get("role") in ["human", "user"] else "model"
        content = msg.get("content", "")
        if not isinstance(content, str): content = str(content)
        gemini_history.append({"role": role, "parts": [content]})
    
    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text

def get_gemini_response_for_image(api_key, user_prompt, image_pil):
    """è™•ç†åœ–ç‰‡è¼¸å…¥çš„ Gemini è«‹æ±‚"""
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None
        return response.text
    except Exception as e: return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(api_key, executive_role_name, full_prompt, require_plot_suggestion: bool = False):
    """
    åŸ·è¡Œé«˜ç®¡åˆ†æçš„æ ¸å¿ƒ Gemini API å‘¼å«ã€‚
    é€é require_plot_suggestion åƒæ•¸æ§åˆ¶æ˜¯å¦å¼·åˆ¶è¦æ±‚åœ–è¡¨ã€‚
    """
    if not api_key: return f"éŒ¯èª¤ï¼šå°ˆæ¥­ç¶“ç†äºº ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    
    final_prompt = full_prompt
    if require_plot_suggestion:
        plotting_instruction = """
**[åœ–è¡¨å»ºè­°æ ¼å¼æŒ‡ä»¤]**:
åœ¨ä½ çš„åˆ†ææ–‡å­—çµæŸå¾Œï¼Œä½ **å¿…é ˆ**æ ¹æ“šä½ çš„åˆ†æï¼Œæä¾›ä¸€å€‹æœ€èƒ½ç¸½çµæ ¸å¿ƒè§€é»çš„åœ–è¡¨å»ºè­°ã€‚è«‹**å‹™å¿…**æä¾›ä¸€å€‹ JSON ç‰©ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼Œä¸å¾—çœç•¥ï¼š
```json
{"plotting_suggestion": {"plot_type": "é¡å‹", "x": "Xè»¸æ¬„ä½å", "y": "Yè»¸æ¬„ä½å", "title": "åœ–è¡¨æ¨™é¡Œ", "explanation": "ä¸€å¥è©±è§£é‡‹æ­¤åœ–è¡¨çš„æ ¸å¿ƒæ´è¦‹"}}
```
å…¶ä¸­ `plot_type` å¿…é ˆæ˜¯ `bar`, `scatter`, `line`, `histogram` ä¸­çš„ä¸€ç¨®ã€‚å°æ–¼ `histogram`ï¼Œ`y` æ¬„ä½å¯ä»¥çœç•¥æˆ–è¨­ç‚º `null`ã€‚ä½ **çµ•å°ä¸èƒ½**å›ç­” `{"plotting_suggestion": null}` æˆ–çœç•¥é€™å€‹ JSON ç‰©ä»¶ã€‚
"""
        final_prompt = f"{full_prompt}\n\n{plotting_instruction}"
    
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(final_prompt)
        return response.text
    except Exception as e: return f"éŒ¯èª¤: {e}"

def generate_data_profile(df, is_simple=False):
    """ç”Ÿæˆ DataFrame çš„æ–‡å­—æ‘˜è¦"""
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    if is_simple:
        preview_rows = min(5, df.shape[0])
        return f"è³‡æ–™å…±æœ‰ {df.shape[0]} è¡Œ, {df.shape[1]} å€‹æ¬„ä½ã€‚\nå‰ {preview_rows} ç­†è³‡æ–™é è¦½:\n{df.head(preview_rows).to_string()}"
    
    buffer = io.StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# --- åœ–è¡¨ç”Ÿæˆè¼”åŠ©å‡½å¼ ---
def parse_plotting_suggestion(response_text: str):
    """å¾ AI çš„å›æ‡‰ä¸­è§£æå‡ºåœ–è¡¨å»ºè­° JSON å’Œåˆ†ææ–‡å­—"""
    json_pattern = r"```json\s*(\{.*?\})\s*```|(\{.*plotting_suggestion.*?\})"
    match = re.search(json_pattern, response_text, re.DOTALL)
    
    if not match:
        return None, response_text.strip()
        
    json_str = match.group(1) if match.group(1) else match.group(2)
    
    try:
        analysis_text = response_text.replace(match.group(0), "").strip()
        suggestion_data = json.loads(json_str)
        plotting_info = suggestion_data.get("plotting_suggestion")
        return plotting_info, analysis_text
        
    except (json.JSONDecodeError, AttributeError):
        analysis_text = response_text.replace(match.group(0), "").strip()
        st.warning("AI æä¾›äº†æ ¼å¼ä¸æ­£ç¢ºçš„åœ–è¡¨å»ºè­°ï¼Œå·²å¿½ç•¥ã€‚")
        return None, analysis_text

def create_plot_from_suggestion(df: pd.DataFrame, suggestion: dict):
    """æ ¹æ“š AI æä¾›çš„çµæ§‹åŒ–å»ºè­°ä¾†ç”Ÿæˆ Plotly åœ–è¡¨"""
    if not suggestion: return None
    
    plot_type = suggestion.get("plot_type", "").lower()
    x_col = suggestion.get("x")
    y_col = suggestion.get("y")
    title = suggestion.get("title", f"AI å»ºè­°åœ–è¡¨")

    if not all([plot_type, x_col]):
        st.warning(f"AI å»ºè­°çš„è³‡è¨Šä¸å®Œæ•´ (ç¼ºå°‘åœ–è¡¨é¡å‹æˆ–Xè»¸)ï¼Œç„¡æ³•ç¹ªåœ–ã€‚")
        return None

    if x_col not in df.columns or (y_col and y_col not in df.columns):
        st.warning(f"AI å»ºè­°çš„æ¬„ä½ '{x_col}' æˆ– '{y_col}' ä¸å­˜åœ¨æ–¼è³‡æ–™ä¸­ï¼Œç„¡æ³•ç¹ªåœ–ã€‚")
        return None

    fig = None
    try:
        if plot_type == "bar":
            if y_col and pd.api.types.is_numeric_dtype(df[y_col]):
                grouped_df = df.groupby(x_col)[y_col].sum().reset_index()
                fig = px.bar(grouped_df, x=x_col, y=y_col, title=title)
            else:
                counts = df[x_col].value_counts().reset_index()
                counts.columns = [x_col, 'count']
                fig = px.bar(counts, x=x_col, y='count', title=title)
        elif plot_type == "scatter":
            if not y_col: st.warning("æ•£ä½ˆåœ–éœ€è¦ y è»¸æ¬„ä½ã€‚"); return None
            fig = px.scatter(df, x=x_col, y=y_col, title=title)
        elif plot_type == "line":
            if not y_col: st.warning("æŠ˜ç·šåœ–éœ€è¦ y è»¸æ¬„ä½ã€‚"); return None
            sorted_df = df.sort_values(by=x_col)
            fig = px.line(sorted_df, x=x_col, y=y_col, title=title)
        elif plot_type == "histogram":
            fig = px.histogram(df, x=x_col, title=title)
        else:
            st.warning(f"å°šä¸æ”¯æ´çš„åœ–è¡¨é¡å‹: '{plot_type}'"); return None
        return fig
    except Exception as e:
        st.error(f"æ ¹æ“š AI å»ºè­° '{title}' ç¹ªè£½åœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# --- åœ–è¡¨ç”Ÿæˆ Agent æ ¸å¿ƒå‡½å¼ ---
def get_df_context(df: pd.DataFrame) -> str:
    buffer = io.StringIO()
    df.info(buf=buffer)
    info_str = buffer.getvalue()
    head_str = df.head().to_string()
    context = f"""
ä»¥ä¸‹æ˜¯æ‚¨éœ€è¦åˆ†æçš„ Pandas DataFrame çš„è©³ç´°è³‡è¨Šã€‚DataFrame è®Šæ•¸åç¨±ç‚º `df`ã€‚
1. DataFrame çš„åŸºæœ¬è³‡è¨Š (df.info()):
{info_str}
2. DataFrame çš„å‰ 5 ç­†è³‡æ–™ (df.head()):
{head_str}"""
    return context

def run_pandas_analyst_agent(api_key: str, df: pd.DataFrame, user_query: str) -> str:
    try:
        llm = ChatOpenAI(api_key=api_key, model="gpt-4-turbo", temperature=0)
        pandas_agent = create_pandas_dataframe_agent(llm, df, verbose=True, allow_dangerous_code=True)
        agent_prompt = f"""
ä½œç‚ºä¸€åè³‡æ·±çš„æ•¸æ“šåˆ†æå¸«ï¼Œä½ çš„ä»»å‹™æ˜¯æ·±å…¥æ¢ç´¢æä¾›çš„ DataFrame (`df`)ã€‚
ä½¿ç”¨è€…çš„ç›®æ¨™æ˜¯ï¼š"{user_query}"
è«‹ä½ åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
1. å¾¹åº•åœ°æ¢ç´¢å’Œåˆ†æ `df`ï¼Œæ‰¾å‡ºå…¶ä¸­æœ€é‡è¦ã€æœ€æœ‰è¶£ã€æœ€å€¼å¾—é€éè¦–è¦ºåŒ–ä¾†å‘ˆç¾çš„ä¸€å€‹æ ¸å¿ƒæ´è¦‹ã€‚
2. ä¸è¦ç”Ÿæˆä»»ä½•ç¹ªåœ–ç¨‹å¼ç¢¼ã€‚
3. ä½ çš„æœ€çµ‚è¼¸å‡º**å¿…é ˆæ˜¯**ä¸€æ®µç°¡æ½”çš„æ–‡å­—æ‘˜è¦ã€‚é€™æ®µæ‘˜è¦éœ€è¦æ¸…æ¥šåœ°æè¿°ä½ ç™¼ç¾çš„æ´è¦‹ï¼Œä¸¦å»ºè­°æ‡‰è©²ç¹ªè£½ä»€éº¼æ¨£çš„åœ–è¡¨ä¾†å±•ç¤ºé€™å€‹æ´è¦‹ã€‚
ç¾åœ¨ï¼Œè«‹é–‹å§‹åˆ†æã€‚"""
        response = pandas_agent.invoke({"input": agent_prompt})
        return response['output']
    except Exception as e:
        return f"Pandas Agent åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"

def generate_plot_code(api_key: str, df_context: str, user_query: str, analyst_conclusion: str = None) -> str:
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-2.5-flash")
        if analyst_conclusion:
            prompt = f"""
ä½ æ˜¯ä¸€ä½é ‚å°–çš„ Python æ•¸æ“šè¦–è¦ºåŒ–å°ˆå®¶ï¼Œç²¾é€šä½¿ç”¨ Plotly Express å‡½å¼åº«ã€‚
ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæ•¸æ“šåˆ†æå¸«çš„çµè«–å’Œä½¿ç”¨è€…çš„åŸå§‹ç›®æ¨™ï¼Œç·¨å¯«ä¸€æ®µ Python ç¨‹å¼ç¢¼ä¾†ç”Ÿæˆæœ€åˆé©çš„åœ–è¡¨ã€‚
**æ•¸æ“šåˆ†æå¸«çš„çµè«–:**
{analyst_conclusion}
**åŸå§‹ä½¿ç”¨è€…ç›®æ¨™:**
"{user_query}"
**DataFrame çš„è³‡è¨Š:**
{df_context}
**åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡:**
1. ä½ åªèƒ½ç”Ÿæˆ Python ç¨‹å¼ç¢¼ï¼Œçµ•å°ä¸èƒ½åŒ…å«ä»»ä½•æ–‡å­—è§£é‡‹ã€è¨»è§£æˆ– ```python æ¨™ç±¤ã€‚
2. ç¨‹å¼ç¢¼å¿…é ˆåŸºæ–¼ä¸Šè¿°**æ•¸æ“šåˆ†æå¸«çš„çµè«–**ä¾†ç”Ÿæˆã€‚
3. ç”Ÿæˆçš„ç¨‹å¼ç¢¼å¿…é ˆä½¿ç”¨ `plotly.express` (åŒ¯å…¥ç‚º `px`)ã€‚
4. DataFrame çš„è®Šæ•¸åç¨±å›ºå®šç‚º `df`ã€‚
5. æœ€çµ‚ç”Ÿæˆçš„åœ–è¡¨ç‰©ä»¶å¿…é ˆè³¦å€¼çµ¦ä¸€å€‹åç‚º `fig` çš„è®Šæ•¸ã€‚
ç¾åœ¨ï¼Œè«‹ç”Ÿæˆç¨‹å¼ç¢¼ï¼š"""
        else:
            prompt = f"""
ä½ æ˜¯ä¸€ä½é ‚å°–çš„ Python æ•¸æ“šè¦–è¦ºåŒ–å°ˆå®¶ï¼Œç²¾é€šä½¿ç”¨ Plotly Express å‡½å¼åº«ã€‚
ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„ DataFrame è³‡è¨Šå’Œä½¿ç”¨è€…çš„è¦æ±‚ï¼Œç·¨å¯«ä¸€æ®µ Python ç¨‹å¼ç¢¼ä¾†ç”Ÿæˆä¸€å€‹åœ–è¡¨ã€‚
**åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡:**
1. ä½ åªèƒ½ç”Ÿæˆ Python ç¨‹å¼ç¢¼ï¼Œçµ•å°ä¸èƒ½åŒ…å«ä»»ä½•æ–‡å­—è§£é‡‹ã€è¨»è§£æˆ– ```python æ¨™ç±¤ã€‚
2. ç”Ÿæˆçš„ç¨‹å¼ç¢¼å¿…é ˆä½¿ç”¨ `plotly.express` (åŒ¯å…¥ç‚º `px`)ã€‚
3. DataFrame çš„è®Šæ•¸åç¨±å›ºå®šç‚º `df`ã€‚
4. æœ€çµ‚ç”Ÿæˆçš„åœ–è¡¨ç‰©ä»¶å¿…é ˆè³¦å€¼çµ¦ä¸€å€‹åç‚º `fig` çš„è®Šæ•¸ã€‚
**DataFrame çš„è³‡è¨Š:**
{df_context}
**ä½¿ç”¨è€…çš„ç¹ªåœ–è¦æ±‚:**
"{user_query}"
ç¾åœ¨ï¼Œè«‹ç”Ÿæˆç¨‹å¼ç¢¼ï¼š"""
        response = model.generate_content(prompt)
        code = response.text.strip().replace("```python", "").replace("```", "").strip()
        return code
    except Exception as e:
        return f"ç¹ªåœ–ç¨‹å¼ç¢¼ç”Ÿæˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"

# --- å¯é‡ç”¨çš„é«˜ç®¡å·¥ä½œæµå‡½å¼ ---
def run_executive_workflow(api_key: str, df: pd.DataFrame, user_query: str, rag_context: str, conversation_history: str = ""):
    """
    åŸ·è¡Œå®Œæ•´çš„é«˜ç®¡åˆ†æå·¥ä½œæµ (åƒ…é™æ•´åˆå¼)ã€‚
    - user_query: ä½¿ç”¨è€…ç•¶å‰çš„å•é¡Œæˆ–æŒ‡ä»¤ã€‚
    - conversation_history: æ ¼å¼åŒ–å¾Œçš„æ­·å²å°è©±ç´€éŒ„ã€‚
    """
    data_profile = generate_data_profile(df)
    
    history_prompt_injection = ""
    if conversation_history:
        history_prompt_injection = f"""
**[å…ˆå‰å°è©±çš„å®Œæ•´æ­·å²ç´€éŒ„]:**
---
{conversation_history}
---
è«‹å‹™å¿…å°‡ä¸Šè¿°æ­·å²ç´€éŒ„ç´å…¥è€ƒé‡ï¼Œä»¥ç¢ºä¿ä½ çš„åˆ†æå…·æœ‰é€£çºŒæ€§ï¼Œé¿å…é‡è¤‡å·²ç¶“è¨è«–éçš„è§€é»ï¼Œä¸¦æ ¹æ“šæœ€æ–°çš„æŒ‡ç¤ºé€²è¡Œèª¿æ•´ã€‚
"""

    final_report = ""
    plot_suggestion = None

    # æ­¤å‡½å¼ç¾åœ¨åªè™•ç†æ•´åˆå¼å·¥ä½œæµ
    with st.spinner("AI ç¶“ç†äººåœ˜éšŠæ­£åœ¨å”ä½œåˆ†æä¸­..."):
        single_stage_prompt = f"""
ä½ å°‡æ‰®æ¼”ä¸€å€‹ç”± CFOã€COO å’Œ CEO çµ„æˆçš„é«˜éšä¸»ç®¡åœ˜éšŠï¼Œå°ä¸€ä»½è³‡æ–™é€²è¡Œåˆ†æã€‚
{history_prompt_injection}
**ç•¶å‰ä½¿ç”¨è€…ç›®æ¨™/æŒ‡ä»¤:** {user_query}
**è³‡æ–™æ‘˜è¦:**\n{data_profile}
**ç›¸é—œçŸ¥è­˜åº«ä¸Šä¸‹æ–‡ (RAG):** {rag_context if rag_context else "ç„¡"}

**ä½ çš„ä»»å‹™:**
è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹é †åºå’Œæ ¼å¼ï¼Œç”Ÿæˆä¸€ä»½å…¨æ–°çš„ã€å®Œæ•´çš„åˆ†æå ±å‘Šä¾†å›æ‡‰**ç•¶å‰ä½¿ç”¨è€…ç›®æ¨™/æŒ‡ä»¤**ï¼š
1.  **CFO åˆ†æ:**
    - ä»¥ `### CFO (è²¡å‹™é•·) åˆ†æå ±å‘Š` ä½œç‚ºé–‹é ­ã€‚
    - å¾è²¡å‹™è§’åº¦åˆ†æã€‚
2.  **COO åˆ†æ:**
    - ä»¥ `### COO (ç‡Ÿé‹é•·) åˆ†æå ±å‘Š` ä½œç‚ºé–‹é ­ã€‚
    - å¾ç‡Ÿé‹æ•ˆç‡è§’åº¦åˆ†æã€‚
3.  **CEO ç¸½çµ:**
    - ä»¥ `### CEO (åŸ·è¡Œé•·) æˆ°ç•¥ç¸½çµ` ä½œç‚ºé–‹é ­ã€‚
    - **æ•´åˆ** CFO å’Œ COO çš„è§€é»ï¼Œä¸¦é‡å°**ç•¶å‰ä½¿ç”¨è€…ç›®æ¨™/æŒ‡ä»¤**æå‡ºæˆ°ç•¥ç¸½çµèˆ‡å»ºè­°ã€‚
**[æ¥µå…¶é‡è¦çš„ç¹ªåœ–æŒ‡ä»¤]**:
åœ¨ä½ æä¾›åœ–è¡¨å»ºè­°çš„ JSON ç‰©ä»¶æ™‚ï¼Œ**ä½ çµ•å°å¿…é ˆåªä½¿ç”¨ã€Œè³‡æ–™æ‘˜è¦ã€ä¸­ `Data columns` å€å¡Šåˆ—å‡ºçš„æ¬„ä½åç¨±**ã€‚
**çµ•å°ä¸å…è¨±**ç™¼æ˜ã€å‡è¨­æˆ–ä½¿ç”¨ä»»ä½•æœªåœ¨è³‡æ–™æ‘˜è¦ä¸­æ˜ç¢ºåˆ—å‡ºçš„æ¬„ä½åç¨±ã€‚
"""
        full_response = get_gemini_executive_analysis(api_key, "Executive Team", single_stage_prompt, require_plot_suggestion=True)
        plot_suggestion, final_report = parse_plotting_suggestion(full_response)
    
    return final_report, plot_suggestion

# --- ä¸»æ‡‰ç”¨å…¥å£ ---
def main():
    st.set_page_config(page_title="Gemini å¤šåŠŸèƒ½ AI åŠ©ç†", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç†")

    executive_session_id = "executive_chat"
    keys_to_init = {
        "use_rag": False, "use_multi_stage_workflow": False,
        "retriever_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "chat_histories": {},
        "executive_user_query": "",
        "plot_code": "",
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state: st.session_state[key] = default_value
    if executive_session_id not in st.session_state.chat_histories:
        st.session_state.chat_histories[executive_session_id] = []

    with st.sidebar:
        st.header("âš™ï¸ åŠŸèƒ½èˆ‡æ¨¡å¼è¨­å®š")
        st.session_state.use_rag = st.checkbox("å•Ÿç”¨ RAG çŸ¥è­˜åº«", value=st.session_state.use_rag)
        st.session_state.use_multi_stage_workflow = st.checkbox("å•Ÿç”¨éšæ®µå¼å·¥ä½œæµ (å¤šé‡è¨˜æ†¶)", value=st.session_state.use_multi_stage_workflow, help="é è¨­(ä¸å‹¾é¸): AI ä¸€æ¬¡å®Œæˆæ‰€æœ‰è§’è‰²åˆ†æ (å–®ä¸€è¨˜æ†¶)ã€‚å‹¾é¸: AI ä¾åºå®Œæˆå„è§’è‰²åˆ†æ (å¤šé‡è¨˜æ†¶)ï¼Œé–‹éŠ·è¼ƒå¤§ã€‚")
        st.divider()
        st.header("ğŸ”‘ API é‡‘é‘°")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key", type="password", key="openai_api_key_input", help="RAG åŠŸèƒ½èˆ‡åœ–è¡¨Agentçš„åˆ†ææ¨¡å¼æœƒä½¿ç”¨æ­¤é‡‘é‘°ã€‚")
        st.divider()
        st.header("ğŸ“ è³‡æ–™ä¸Šå‚³")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=["csv"])
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                st.session_state.last_uploaded_filename = uploaded_file.name
                file_path = save_uploaded_file(uploaded_file)
                st.session_state.uploaded_file_path = file_path
                st.success(f"æª”æ¡ˆ '{uploaded_file.name}' ä¸Šå‚³æˆåŠŸï¼")
                st.session_state.retriever_chain = None
                if st.session_state.use_rag:
                    openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                    if not openai_api_key: st.error("RAG åŠŸèƒ½å·²å•Ÿç”¨ï¼Œè«‹åœ¨ä¸Šæ–¹è¼¸å…¥æ‚¨çš„ OpenAI API Keyï¼")
                    else: st.session_state.retriever_chain = create_lc_retriever(file_path, openai_api_key)
        if st.session_state.retriever_chain: st.success("âœ… RAG çŸ¥è­˜åº«å·²å•Ÿç”¨ï¼")
        
        st.header("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            settings = {k: st.session_state.get(k) for k in ['gemini_api_key_input', 'openai_api_key_input', 'use_rag', 'use_multi_stage_workflow']}
            st.session_state.clear()
            for key, value in settings.items(): st.session_state[key] = value
            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ å°ˆæ¥­ç¶“ç†äºº"]
    # tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ å°ˆæ¥­ç¶“ç†äºº", "ğŸ“Š åœ–è¡¨ç”Ÿæˆ Agent"]
    tabs = st.tabs(tab_titles)

    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")

    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•ä¸»è¦åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)
    
    df = None
    if st.session_state.uploaded_file_path:
        try:
            df = pd.read_csv(st.session_state.uploaded_file_path)
        except Exception as e:
            st.error(f"è®€å– CSV æª”æ¡ˆå¤±æ•—: {e}")

    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œä¸€èˆ¬å°è©±ã€åœ–ç‰‡åˆ†æã€‚RAG å•ç­”åŠŸèƒ½å¯ç”±å´é‚Šæ¬„é–‹é—œå•Ÿç”¨ã€‚")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if user_input := st.chat_input("è«‹å°æ•¸æ“šã€åœ–ç‰‡æå•æˆ–é–‹å§‹å°è©±..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    prompt_context = ""
                    if st.session_state.use_rag and st.session_state.retriever_chain:
                        context = "\n---\n".join([doc.page_content for doc in st.session_state.retriever_chain.invoke(user_input)])
                        prompt_context = f"è«‹æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n\n[ä¸Šä¸‹æ–‡]:\n{context}\n\n"
                    elif not st.session_state.use_rag and df is not None:
                        prompt_context = f"è«‹åƒè€ƒä»¥ä¸‹è³‡æ–™æ‘˜è¦ä¾†å›ç­”å•é¡Œã€‚\n\n[è³‡æ–™æ‘˜è¦]:\n{generate_data_profile(df.head(), is_simple=True)}\n\n"
                        
                    if st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(gemini_api_key, f"{prompt_context} [å•é¡Œ]:\n{user_input}", st.session_state.pending_image_for_main_gemini)
                    else:
                        response = get_gemini_response_with_history(gemini_client, st.session_state.chat_histories[session_id][:-1], f"{prompt_context}[å•é¡Œ]:\n{user_input}")
                    
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    with tabs[1]:
        st.header("ğŸ’¼ å°ˆæ¥­ç¶“ç†äºº")
        st.caption(f"ç›®å‰æ¨¡å¼ï¼š{'éšæ®µå¼ (å¤šé‡è¨˜æ†¶)' if st.session_state.use_multi_stage_workflow else 'æ•´åˆå¼ (å–®ä¸€è¨˜æ†¶)'} | RAGï¼š{'å•Ÿç”¨' if st.session_state.use_rag else 'åœç”¨'}")

        if df is None:
            st.info("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³ CSV æª”æ¡ˆä»¥å•Ÿç”¨æ­¤åŠŸèƒ½ã€‚")
        else:
            # --- é¡¯ç¤ºæ­·å²å°è©± ---
            for msg in st.session_state.chat_histories[executive_session_id]:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
                    if msg["role"] == "ai":
                        plot_suggestion = msg.get("plot_suggestion")
                        if plot_suggestion:
                            st.markdown("---")
                            st.write(f"**å»ºè­°åœ–è¡¨:** {plot_suggestion.get('title', '')}")
                            st.caption(plot_suggestion.get("explanation", ""))
                            fig = create_plot_from_suggestion(df, plot_suggestion)
                            if fig: st.plotly_chart(fig, use_container_width=True)
                            else: st.warning("ç„¡æ³•ç”Ÿæˆå»ºè­°çš„åœ–è¡¨ã€‚")

            # --- è¼¸å…¥èˆ‡è™•ç†é‚è¼¯ ---
            user_query = st.text_input("è«‹è¼¸å…¥æ‚¨çš„åˆ†æç›®æ¨™æˆ–è¿½å•ï¼š", key="executive_query", placeholder="ä¾‹å¦‚ï¼šåˆ†æå„ç”¢å“ç·šçš„éŠ·å”®è¡¨ç¾")

            if st.button("æäº¤åˆ†æ / è¿½å•", key="start_executive_analysis"):
                if not user_query:
                    st.warning("è«‹å…ˆè¼¸å…¥æ‚¨çš„åˆ†æç›®æ¨™ï¼")
                else:
                    st.session_state.chat_histories[executive_session_id].append({"role": "user", "content": user_query})
                    st.rerun() 

            if st.session_state.chat_histories[executive_session_id] and st.session_state.chat_histories[executive_session_id][-1]["role"] == "user":
                last_user_query = st.session_state.chat_histories[executive_session_id][-1]["content"]
                
                history_list = []
                for msg in st.session_state.chat_histories[executive_session_id][:-1]:
                   role = "ä½¿ç”¨è€…" if msg['role'] == 'user' else "AIç¶“ç†äººåœ˜éšŠ"
                   history_list.append(f"{role}:\n{msg['content']}")
                history_str = "\n\n".join(history_list)

                rag_context = ""
                if st.session_state.use_rag and st.session_state.retriever_chain:
                    rag_context = "\n---\n".join([doc.page_content for doc in st.session_state.retriever_chain.invoke(last_user_query)])

                if not st.session_state.use_multi_stage_workflow:
                    new_report, new_plot_suggestion = run_executive_workflow(
                        api_key=gemini_api_key, df=df, user_query=last_user_query,
                        rag_context=rag_context, conversation_history=history_str
                    )
                    st.session_state.chat_histories[executive_session_id].append({
                        "role": "ai", "content": new_report, "plot_suggestion": new_plot_suggestion
                    })
                    st.rerun()
                else:
                    with st.chat_message("ai"):
                        data_profile = generate_data_profile(df)
                        history_prompt_injection = f"\n**[å…ˆå‰å°è©±çš„å®Œæ•´æ­·å²ç´€éŒ„]:**\n---\n{history_str}\n---\nè«‹å‹™å¿…å°‡ä¸Šè¿°æ­·å²ç´€éŒ„ç´å…¥è€ƒé‡ï¼Œä»¥ç¢ºä¿ä½ çš„åˆ†æå…·æœ‰é€£çºŒæ€§ï¼Œé¿å…é‡è¤‡å·²ç¶“è¨è«–éçš„è§€é»ï¼Œä¸¦æ ¹æ“šæœ€æ–°çš„æŒ‡ç¤ºé€²è¡Œèª¿æ•´ã€‚" if history_str else ""
                        
                        # --- CFO éšæ®µ ---
                        with st.spinner("CFO æ­£åœ¨åˆ†æä¸­..."):
                            cfo_prompt = f"ä½œç‚ºå°ˆæ¥­çš„è²¡å‹™é•·(CFO)ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šé€²è¡Œåˆ†æã€‚\n{history_prompt_injection}\n**ç•¶å‰ä½¿ç”¨è€…ç›®æ¨™/æŒ‡ä»¤:** {last_user_query}\n**è³‡æ–™æ‘˜è¦:**\n{data_profile}\n**ç›¸é—œçŸ¥è­˜åº«ä¸Šä¸‹æ–‡ (RAG):** {rag_context if rag_context else 'ç„¡'}\n**ä½ çš„ä»»å‹™:** å¾è²¡å‹™è§’åº¦åˆ†æï¼Œæä¾›æ•¸æ“šé©…å‹•çš„æ´è¦‹ã€‚**åœ¨æ­¤éšæ®µä¸éœ€æä¾›åœ–è¡¨å»ºè­°ã€‚**"
                            cfo_response = get_gemini_executive_analysis(gemini_api_key, "CFO", cfo_prompt, require_plot_suggestion=False)
                            _, cfo_analysis_text = parse_plotting_suggestion(cfo_response)
                        st.markdown("### CFO (è²¡å‹™é•·) åˆ†æå ±å‘Š")
                        st.markdown(cfo_analysis_text)
                        st.markdown("---")

                        # --- COO éšæ®µ ---
                        with st.spinner("COO æ­£åœ¨åˆ†æä¸­..."):
                            coo_prompt = f"ä½œç‚ºå°ˆæ¥­çš„ç‡Ÿé‹é•·(COO)ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šé€²è¡Œåˆ†æã€‚\n{history_prompt_injection}\n**CFO å·²å®Œæˆçš„åˆ†æ:**\n{cfo_analysis_text}\n**ç•¶å‰ä½¿ç”¨è€…ç›®æ¨™/æŒ‡ä»¤:** {last_user_query}\n**è³‡æ–™æ‘˜è¦:**\n{data_profile}\n**ç›¸é—œçŸ¥è­˜åº«ä¸Šä¸‹æ–‡ (RAG):** {rag_context if rag_context else 'ç„¡'}\n**ä½ çš„ä»»å‹™:** å¾ç‡Ÿé‹æ•ˆç‡è§’åº¦åˆ†æã€‚**åœ¨æ­¤éšæ®µä¸éœ€æä¾›åœ–è¡¨å»ºè­°ã€‚**"
                            coo_response = get_gemini_executive_analysis(gemini_api_key, "COO", coo_prompt, require_plot_suggestion=False)
                            _, coo_analysis_text = parse_plotting_suggestion(coo_response)
                        st.markdown("### COO (ç‡Ÿé‹é•·) åˆ†æå ±å‘Š")
                        st.markdown(coo_analysis_text)
                        st.markdown("---")

                        # --- CEO éšæ®µ ---
                        with st.spinner("CEO æ­£åœ¨ç¸½çµä¸­..."):
                            ceo_prompt = f"""ä½œç‚ºå…¬å¸çš„åŸ·è¡Œé•·(CEO)ï¼Œä½ çš„ä»»å‹™æ˜¯åŸºæ–¼ä½ çš„åœ˜éšŠåˆ†æï¼Œæä¾›å…¨é¢çš„æˆ°ç•¥ç¸½çµã€‚\n{history_prompt_injection}\n**è²¡å‹™é•· (CFO) çš„åˆ†æå ±å‘Š:**\n{cfo_analysis_text}\n**ç‡Ÿé‹é•· (COO) çš„åˆ†æå ±å‘Š:**\n{coo_analysis_text}\n**ç•¶å‰ä½¿ç”¨è€…ç›®æ¨™/æŒ‡ä»¤:** {last_user_query}\n**ä½ çš„ä»»å‹™:** æ•´åˆ CFO å’Œ COO çš„è§€é»ï¼Œé‡å°**ç•¶å‰ä½¿ç”¨è€…ç›®æ¨™/æŒ‡ä»¤**æä¾›é«˜å±¤æ¬¡çš„æˆ°ç•¥ç¸½çµå’Œå»ºè­°ã€‚
                            **[æ¥µå…¶é‡è¦çš„ç¹ªåœ–æŒ‡ä»¤]**:
åœ¨ä½ æä¾›åœ–è¡¨å»ºè­°çš„ JSON ç‰©ä»¶æ™‚ï¼Œ**ä½ çµ•å°å¿…é ˆåªä½¿ç”¨ä¸Šæ–¹ã€ŒåŸå§‹è³‡æ–™æ‘˜è¦ã€ä¸­ `Data columns` å€å¡Šåˆ—å‡ºçš„æ¬„ä½åç¨±**ã€‚
**çµ•å°ä¸å…è¨±**ç™¼æ˜ã€å‡è¨­æˆ–ä½¿ç”¨ä»»ä½•æœªåœ¨è³‡æ–™æ‘˜è¦ä¸­æ˜ç¢ºåˆ—å‡ºçš„æ¬„ä½åç¨±ã€‚"""
                            ceo_response = get_gemini_executive_analysis(gemini_api_key, "CEO", ceo_prompt, require_plot_suggestion=True)
                            plot_suggestion, ceo_summary_text = parse_plotting_suggestion(ceo_response)
                        st.markdown("### CEO (åŸ·è¡Œé•·) æˆ°ç•¥ç¸½çµ")
                        st.markdown(ceo_summary_text)

                        if plot_suggestion:
                            st.markdown("---")
                            st.write(f"**å»ºè­°åœ–è¡¨:** {plot_suggestion.get('title', '')}")
                            st.caption(plot_suggestion.get("explanation", ""))
                            fig = create_plot_from_suggestion(df, plot_suggestion)
                            if fig: st.plotly_chart(fig, use_container_width=True)
                            else: st.warning("ç„¡æ³•ç”Ÿæˆå»ºè­°çš„åœ–è¡¨ã€‚")

                        final_report = f"### CFO (è²¡å‹™é•·) åˆ†æå ±å‘Š\n{cfo_analysis_text}\n\n---\n\n### COO (ç‡Ÿé‹é•·) åˆ†æå ±å‘Š\n{coo_analysis_text}\n\n---\n\n### CEO (åŸ·è¡Œé•·) æˆ°ç•¥ç¸½çµ\n{ceo_summary_text}"
                        st.session_state.chat_histories[executive_session_id].append({
                            "role": "ai", "content": final_report, "plot_suggestion": plot_suggestion
                        })

    # with tabs[2]:
    #     st.header("ğŸ“Š åœ–è¡¨ç”Ÿæˆ Agent")
    #     st.caption("é€™æ˜¯ä¸€å€‹ä½¿ç”¨ Agent ä¾†ç”Ÿæˆåœ–è¡¨ç¨‹å¼ç¢¼çš„ç¯„ä¾‹ã€‚")
        
    #     if df is None:
    #         st.info("è«‹å…ˆåœ¨å´é‚Šæ¬„ä¸Šå‚³ CSV æª”æ¡ˆä»¥å•Ÿç”¨æ­¤åŠŸèƒ½ã€‚")
    #     else:
    #         st.write("#### DataFrame é è¦½")
    #         st.dataframe(df.head())
            
    #         mode = st.radio("é¸æ“‡æ¨¡å¼ï¼š", ("AI åˆ†æå¸«å»ºè­°", "ç›´æ¥ä¸‹æŒ‡ä»¤"), horizontal=True, key="agent_mode")

    #         user_plot_query = st.text_input("è«‹è¼¸å…¥æ‚¨çš„ç¹ªåœ–ç›®æ¨™ï¼š", key="agent_query", placeholder="ä¾‹å¦‚ï¼šæˆ‘æƒ³çœ‹å„å€‹åŸå¸‚çš„å¹³å‡æˆ¿åƒ¹")

    #         if st.button("ç”Ÿæˆåœ–è¡¨", key="agent_generate"):
    #             df_context = get_df_context(df)
    #             if mode == "AI åˆ†æå¸«å»ºè­°":
    #                 if not openai_api_key:
    #                     st.error("æ­¤æ¨¡å¼éœ€è¦ OpenAI API Keyï¼Œè«‹åœ¨å´é‚Šæ¬„è¼¸å…¥ã€‚")
    #                 else:
    #                     with st.spinner("AI åˆ†æå¸«æ­£åœ¨æ€è€ƒæœ€ä½³åœ–è¡¨..."):
    #                         analyst_conclusion = run_pandas_analyst_agent(openai_api_key, df, user_plot_query)
    #                         st.info(f"**åˆ†æå¸«çµè«–:** {analyst_conclusion}")
    #                     with st.spinner("è¦–è¦ºåŒ–å°ˆå®¶æ­£åœ¨ç”Ÿæˆç¨‹å¼ç¢¼..."):
    #                         code = generate_plot_code(gemini_api_key, df_context, user_plot_query, analyst_conclusion)
    #                         st.session_state.plot_code = code
    #             else: # ç›´æ¥ä¸‹æŒ‡ä»¤
    #                 with st.spinner("è¦–è¦ºåŒ–å°ˆå®¶æ­£åœ¨æ ¹æ“šæ‚¨çš„æŒ‡ä»¤ç”Ÿæˆç¨‹å¼ç¢¼..."):
    #                     code = generate_plot_code(gemini_api_key, df_context, user_plot_query)
    #                     st.session_state.plot_code = code
            
    #         if st.session_state.plot_code:
    #             st.write("#### æœ€çµ‚åœ–è¡¨")
    #             try:
    #                 exec_scope = {'df': df, 'px': px}
    #                 exec(st.session_state.plot_code, exec_scope)
    #                 fig = exec_scope.get('fig')
    #                 if fig:
    #                     st.plotly_chart(fig, use_container_width=True)
    #                 else:
    #                     st.error("ç¨‹å¼ç¢¼æœªæˆåŠŸç”Ÿæˆåç‚º 'fig' çš„åœ–è¡¨ç‰©ä»¶ã€‚")
    #             except Exception as e:
    #                 st.error(f"åŸ·è¡Œç”Ÿæˆçš„ç¨‹å¼ç¢¼æ™‚å‡ºéŒ¯: {e}")

    #             with st.expander("æŸ¥çœ‹/ç·¨è¼¯ç”Ÿæˆçš„ç¨‹å¼ç¢¼"):
    #                 st.code(st.session_state.plot_code, language='python')


if __name__ == "__main__":
    main()
