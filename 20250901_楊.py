import streamlit as st
import pandas as pd
import os
import io
import time
import dotenv
from PIL import Image
import numpy as np

# --- Plotly å’Œ Gemini/Langchain/OpenAI ç­‰æ ¸å¿ƒå¥—ä»¶ ---
import plotly.express as px
import google.generativeai as genai
from openai import OpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import CSVLoader
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- åˆå§‹åŒ–èˆ‡å¸¸æ•¸å®šç¾© ---
dotenv.load_dotenv()
UPLOAD_DIR = "uploaded_files"
if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)

# --- åŸºç¤è¼”åŠ©å‡½æ•¸ ---
def save_uploaded_file(uploaded_file):
    if not os.path.exists(UPLOAD_DIR): os.makedirs(UPLOAD_DIR)
    file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
    with open(file_path, "wb") as f: f.write(uploaded_file.getbuffer())
    return file_path

def add_user_image_to_main_chat(uploaded_file):
    try:
        file_path = save_uploaded_file(uploaded_file)
        st.session_state.pending_image_for_main_gemini = Image.open(file_path)
        st.image(st.session_state.pending_image_for_main_gemini, caption="åœ–ç‰‡å·²ä¸Šå‚³ï¼Œå°‡éš¨ä¸‹ä¸€æ¢æ–‡å­—è¨Šæ¯ç™¼é€ã€‚", use_container_width=True)
    except Exception as e: st.error(f"è™•ç†ä¸Šå‚³åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")

# --- RAG æ ¸å¿ƒå‡½å¼ ---
@st.cache_resource
def create_lc_retriever(file_path: str, openai_api_key: str):
    with st.status("æ­£åœ¨å»ºç«‹ RAG çŸ¥è­˜åº«...", expanded=True) as status:
        try:
            status.update(label="æ­¥é©Ÿ 1/3ï¼šè¼‰å…¥èˆ‡åˆ‡å‰²æ–‡ä»¶...")
            loader = CSVLoader(file_path=file_path, encoding='utf-8')
            documents = loader.load()
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
            docs = text_splitter.split_documents(documents)
            status.update(label=f"æ­¥é©Ÿ 1/3 å®Œæˆï¼å·²åˆ‡å‰²æˆ {len(docs)} å€‹å€å¡Šã€‚")
            status.update(label="æ­¥é©Ÿ 2/3ï¼šå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
            vector_store = LangChainFAISS.from_documents(docs, embeddings)
            status.update(label="æ­¥é©Ÿ 2/3 å®Œæˆï¼å‘é‡åµŒå…¥å·²ç”Ÿæˆã€‚")
            status.update(label="æ­¥é©Ÿ 3/3ï¼šæª¢ç´¢å™¨æº–å‚™å®Œæˆï¼", state="complete", expanded=False)
            return vector_store.as_retriever(search_kwargs={'k': 5})
        except Exception as e:
            st.error(f"å»ºç«‹çŸ¥è­˜åº«éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
            status.update(label="å»ºç«‹å¤±æ•—", state="error")
            return None

# --- Gemini API ç›¸é—œå‡½å¼ ---
def get_gemini_client(api_key):
    genai.configure(api_key=api_key)
    return genai.GenerativeModel("gemini-1.5-flash-latest")

def get_gemini_response_with_history(client, history, user_prompt):
    gemini_history = []
    if not isinstance(history, list):
        history = []
    for msg in history:
        role = "user" if msg["role"] == "human" else "model"
        content = msg.get("content", "")
        if not isinstance(content, str):
            content = str(content)
        gemini_history.append({"role": role, "parts": [content]})

    chat = client.start_chat(history=gemini_history)
    response = chat.send_message(user_prompt)
    return response.text
    
def get_gemini_response_for_image(api_key, user_prompt, image_pil):
    if not api_key: return "éŒ¯èª¤ï¼šæœªè¨­å®š Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash-latest")
        response = model.generate_content([user_prompt, image_pil])
        st.session_state.pending_image_for_main_gemini = None
        return response.text
    except Exception as e: return f"éŒ¯èª¤: {e}"

def get_gemini_executive_analysis(api_key, executive_role_name, full_prompt):
    if not api_key: return f"éŒ¯èª¤ï¼šå°ˆæ¥­ç¶“ç†äºº ({executive_role_name}) æœªèƒ½ç²å– Gemini API Keyã€‚"
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-pro-latest")
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e: return f"éŒ¯èª¤: {e}"
    
def generate_data_profile(df, is_simple=False):
    if df is None or df.empty: return "æ²’æœ‰è³‡æ–™å¯ä¾›åˆ†æã€‚"
    if is_simple:
        preview_rows = min(5, df.shape[0])
        return f"è³‡æ–™å…±æœ‰ {df.shape[0]} è¡Œ, {df.shape[1]} å€‹æ¬„ä½ã€‚\nå‰ {preview_rows} ç­†è³‡æ–™é è¦½:\n{df.head(preview_rows).to_string()}"
    buffer = io.StringIO()
    df.info(buf=buffer)
    profile_parts = [f"è³‡æ–™å½¢ç‹€: {df.shape}", f"æ¬„ä½è³‡è¨Š:\n{buffer.getvalue()}"]
    try: profile_parts.append(f"\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:\n{df.describe(include='number').to_string()}")
    except: pass
    try: profile_parts.append(f"\né¡åˆ¥æ¬„ä½çµ±è¨ˆ:\n{df.describe(include=['object', 'category']).to_string()}")
    except: pass
    profile_parts.append(f"\nå‰ 5 ç­†è³‡æ–™:\n{df.head().to_string()}")
    return "\n".join(profile_parts)

# --- è³‡æ–™æ¢ç´¢å™¨æ ¸å¿ƒå‡½æ•¸ ---
@st.cache_data
def get_overview_metrics(df):
    if df is None or df.empty: return 0, 0, 0, 0, 0
    num_rows, num_cols = df.shape
    missing_percentage = (df.isnull().sum().sum() / (num_rows * num_cols)) * 100 if (num_rows * num_cols) > 0 else 0
    numeric_cols_count = len(df.select_dtypes(include=np.number).columns)
    duplicate_rows = df.duplicated().sum()
    return num_rows, num_cols, missing_percentage, numeric_cols_count, duplicate_rows

@st.cache_data
def get_column_quality_assessment(df):
    if df is None or df.empty: return pd.DataFrame()
    quality_data = [{"æ¬„ä½": col, "è³‡æ–™é¡å‹": str(df[col].dtype), "ç¼ºå¤±å€¼æ¯”ä¾‹ (%)": (df[col].isnull().sum() / len(df)) * 100 if len(df) > 0 else 0, "å”¯ä¸€å€¼æ•¸é‡": df[col].nunique()} for col in df.columns]
    return pd.DataFrame(quality_data)

def display_simple_data_explorer(df):
    st.subheader("äº’å‹•å¼è³‡æ–™æ¢ç´¢")
    st.markdown("---")
    st.markdown("##### é—œéµæŒ‡æ¨™")
    num_rows, num_cols, missing_percentage, numeric_cols_count, duplicate_rows = get_overview_metrics(df)
    kpi_cols = st.columns(5)
    kpi_cols[0].metric("ç¸½è¡Œæ•¸", f"{num_rows:,}")
    kpi_cols[1].metric("ç¸½åˆ—æ•¸", f"{num_cols:,}")
    kpi_cols[2].metric("ç¼ºå¤±å€¼æ¯”ä¾‹", f"{missing_percentage:.2f}%")
    kpi_cols[3].metric("æ•¸å€¼å‹æ¬„ä½", f"{numeric_cols_count}")
    kpi_cols[4].metric("é‡è¤‡è¡Œæ•¸", f"{duplicate_rows:,}")
    st.markdown("##### æ¬„ä½å“è³ªè©•ä¼°")
    st.dataframe(get_column_quality_assessment(df), use_container_width=True)
    st.markdown("---")
    st.markdown("##### æ¬„ä½è³‡æ–™åˆ†ä½ˆ")
    plot_col1, plot_col2 = st.columns(2)
    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    with plot_col1:
        if numeric_cols:
            selected_numeric = st.selectbox("é¸æ“‡ä¸€å€‹æ•¸å€¼å‹æ¬„ä½æŸ¥çœ‹åˆ†ä½ˆ:", numeric_cols, key="explorer_numeric")
            if selected_numeric:
                st.plotly_chart(px.histogram(df, x=selected_numeric, title=f"'{selected_numeric}' çš„åˆ†ä½ˆ", marginal="box"), use_container_width=True)
        else: st.info("ç„¡æ•¸å€¼å‹æ¬„ä½å¯ä¾›åˆ†æã€‚")
    with plot_col2:
        if categorical_cols:
            selected_categorical = st.selectbox("é¸æ“‡ä¸€å€‹é¡åˆ¥å‹æ¬„ä½æŸ¥çœ‹åˆ†ä½ˆ:", categorical_cols, key="explorer_categorical")
            if selected_categorical:
                top_n = st.slider("é¡¯ç¤ºå‰ N å€‹é¡åˆ¥", 5, 20, 10, key="explorer_top_n")
                counts = df[selected_categorical].value_counts().nlargest(top_n)
                st.plotly_chart(px.bar(counts, x=counts.index, y=counts.values, title=f"'{selected_categorical}' çš„å‰ {top_n} å€‹é¡åˆ¥åˆ†ä½ˆ", labels={'index':selected_categorical, 'y':'æ•¸é‡'}), use_container_width=True)
        else: st.info("ç„¡é¡åˆ¥å‹æ¬„ä½å¯ä¾›åˆ†æã€‚")
    st.markdown("##### æ•¸å€¼æ¬„ä½ç›¸é—œæ€§ç†±åŠ›åœ–")
    if len(numeric_cols) > 1:
        st.plotly_chart(px.imshow(df[numeric_cols].corr(numeric_only=True), text_auto=True, aspect="auto", title="æ•¸å€¼æ¬„ä½ç›¸é—œæ€§ç†±åŠ›åœ–", color_continuous_scale='RdBu_r'), use_container_width=True)
    else: st.info("éœ€è¦è‡³å°‘å…©å€‹æ•¸å€¼å‹æ¬„ä½æ‰èƒ½è¨ˆç®—ç›¸é—œæ€§ã€‚")
    
# --- ä¸»æ‡‰ç”¨å…¥å£ ---
def main():
    st.set_page_config(page_title="Gemini Multi-Function Bot", page_icon="âœ¨", layout="wide")
    st.title("âœ¨ Gemini å¤šåŠŸèƒ½ AI åŠ©ç† ")

    executive_session_id = "executive_chat"
    keys_to_init = {
        "use_rag": False, "use_multi_stage_workflow": False, "use_simple_explorer": False,
        "retriever_chain": None, "uploaded_file_path": None, "last_uploaded_filename": None,
        "pending_image_for_main_gemini": None, "chat_histories": {},
        "executive_workflow_stage": "idle", "executive_user_query": "",
        "executive_data_profile_str": "", "executive_rag_context": "", "cfo_analysis_text": "",
        "coo_analysis_text": "", "ceo_summary_text": "",
        "sp_workflow_stage": "idle", "sp_user_query": "", "sp_final_report": "",
        "follow_up_query": "", "follow_up_stage": "idle", 
        "follow_up_cfo_analysis": "", "follow_up_coo_analysis": "", "follow_up_ceo_analysis": "",
    }
    for key, default_value in keys_to_init.items():
        if key not in st.session_state: st.session_state[key] = default_value
    if executive_session_id not in st.session_state.chat_histories:
        st.session_state.chat_histories[executive_session_id] = []

    with st.sidebar:
        st.header("âš™ï¸ åŠŸèƒ½èˆ‡æ¨¡å¼è¨­å®š")
        st.session_state.use_rag = st.checkbox("å•Ÿç”¨ RAG çŸ¥è­˜åº« (éœ€ OpenAI Key)", value=st.session_state.use_rag)
        st.session_state.use_multi_stage_workflow = st.checkbox("å•Ÿç”¨éšæ®µå¼å·¥ä½œæµ (å¤šé‡è¨˜æ†¶)", value=st.session_state.use_multi_stage_workflow, help="é è¨­(ä¸å‹¾é¸): AI ä¸€æ¬¡å®Œæˆæ‰€æœ‰è§’è‰²åˆ†æ (å–®ä¸€è¨˜æ†¶)ã€‚å‹¾é¸: AI ä¾åºå®Œæˆå„è§’è‰²åˆ†æ (å¤šé‡è¨˜æ†¶)ï¼Œé–‹éŠ·è¼ƒå¤§ã€‚")
        st.session_state.use_simple_explorer = st.checkbox("å•Ÿç”¨ç°¡æ˜“è³‡æ–™æ¢ç´¢å™¨", value=st.session_state.use_simple_explorer, help="å‹¾é¸å¾Œï¼Œå°‡åœ¨å·¥ä½œæµçš„çµ±è¨ˆæ‘˜è¦å€å¡Šé¡¯ç¤ºäº’å‹•å¼åœ–è¡¨ã€‚")
        st.divider()
        st.header("ğŸ”‘ API é‡‘é‘°")
        st.text_input("è«‹è¼¸å…¥æ‚¨çš„ Google Gemini API Key", type="password", key="gemini_api_key_input")
        if st.session_state.use_rag:
            st.text_input("è«‹è¼¸å…¥æ‚¨çš„ OpenAI API Key (RAG åŠŸèƒ½éœ€è¦)", type="password", key="openai_api_key_input")
        st.divider()
        st.header("ğŸ“ è³‡æ–™ä¸Šå‚³")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=["csv"])
        if uploaded_file:
            if uploaded_file.name != st.session_state.get("last_uploaded_filename"):
                st.session_state.last_uploaded_filename = uploaded_file.name
                file_path = save_uploaded_file(uploaded_file)
                st.session_state.uploaded_file_path = file_path
                st.success(f"æª”æ¡ˆ '{uploaded_file.name}' ä¸Šå‚³æˆåŠŸï¼")
                st.session_state.retriever_chain = None 
                if st.session_state.use_rag:
                    openai_api_key = st.session_state.get("openai_api_key_input") or os.environ.get("OPENAI_API_KEY")
                    if not openai_api_key: st.error("RAG åŠŸèƒ½å·²å•Ÿç”¨ï¼Œè«‹åœ¨ä¸Šæ–¹è¼¸å…¥æ‚¨çš„ OpenAI API Keyï¼")
                    else: st.session_state.retriever_chain = create_lc_retriever(file_path, openai_api_key)
        if st.session_state.retriever_chain: st.success("âœ… RAG çŸ¥è­˜åº«å·²å•Ÿç”¨ï¼")
        st.header("ğŸ–¼ï¸ åœ–ç‰‡åˆ†æ")
        uploaded_image = st.file_uploader("ä¸Šå‚³åœ–ç‰‡", type=["png", "jpg", "jpeg"])
        if uploaded_image: add_user_image_to_main_chat(uploaded_image)
        st.divider()
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å°è©±èˆ‡è³‡æ–™"):
            settings = {k: st.session_state.get(k) for k in ['gemini_api_key_input', 'openai_api_key_input', 'use_rag', 'use_multi_stage_workflow', 'use_simple_explorer']}
            st.session_state.clear()
            for key, value in settings.items(): st.session_state[key] = value
            st.cache_resource.clear()
            st.success("æ‰€æœ‰å°è©±ã€Session è¨˜æ†¶å’Œå¿«å–å·²æ¸…é™¤ï¼")
            st.rerun()

    tab_titles = ["ğŸ’¬ ä¸»è¦èŠå¤©å®¤", "ğŸ’¼ å°ˆæ¥­ç¶“ç†äºº"]
    tabs = st.tabs(tab_titles)
    gemini_api_key = st.session_state.get("gemini_api_key_input") or os.environ.get("GOOGLE_API_KEY")
    if not gemini_api_key:
        st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ Google Gemini API Key ä»¥å•Ÿå‹•èŠå¤©åŠŸèƒ½ã€‚")
        st.stop()
    gemini_client = get_gemini_client(gemini_api_key)

    with tabs[0]:
        st.header("ğŸ’¬ ä¸»è¦èŠå¤©å®¤")
        st.caption("å¯é€²è¡Œä¸€èˆ¬å°è©±ã€åœ–ç‰‡åˆ†æã€‚RAG å•ç­”åŠŸèƒ½å¯ç”±å´é‚Šæ¬„é–‹é—œå•Ÿç”¨ã€‚")
        session_id = "main_chat"
        if session_id not in st.session_state.chat_histories: st.session_state.chat_histories[session_id] = []
        for msg in st.session_state.chat_histories[session_id]:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        if user_input := st.chat_input("è«‹å°æ•¸æ“šã€åœ–ç‰‡æå•æˆ–é–‹å§‹å°è©±..."):
            st.session_state.chat_histories[session_id].append({"role": "human", "content": user_input})
            with st.chat_message("human"): st.markdown(user_input)
            with st.chat_message("ai"):
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    prompt_context = ""
                    if st.session_state.use_rag and st.session_state.retriever_chain:
                        context = "\n---\n".join([doc.page_content for doc in st.session_state.retriever_chain.invoke(user_input)])
                        prompt_context = f"è«‹æ ¹æ“šä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚\n\n[ä¸Šä¸‹æ–‡]:\n{context}\n\n"
                    elif not st.session_state.use_rag and st.session_state.get("uploaded_file_path"):
                        try:
                            df = pd.read_csv(st.session_state.uploaded_file_path)
                            prompt_context = f"è«‹åƒè€ƒä»¥ä¸‹è³‡æ–™æ‘˜è¦ä¾†å›ç­”å•é¡Œã€‚\n\n[è³‡æ–™æ‘˜è¦]:\n{generate_data_profile(df.head(), is_simple=True)}\n\n"
                        except Exception as e: st.warning(f"è®€å– CSV æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    if st.session_state.pending_image_for_main_gemini:
                        response = get_gemini_response_for_image(gemini_api_key, f"{prompt_context} [å•é¡Œ]:\n{user_input}", st.session_state.pending_image_for_main_gemini)
                    else:
                        response = get_gemini_response_with_history(gemini_client, st.session_state.chat_histories[session_id][:-1], f"{prompt_context}[å•é¡Œ]:\n{user_input}")
                    st.markdown(response)
                    st.session_state.chat_histories[session_id].append({"role": "ai", "content": response})

    with tabs[1]:
        st.header("ğŸ’¼ å°ˆæ¥­ç¶“ç†äºº")
        st.caption(f"ç›®å‰æ¨¡å¼ï¼š{'éšæ®µå¼ (å¤šé‡è¨˜æ†¶)' if st.session_state.use_multi_stage_workflow else 'æ•´åˆå¼ (å–®ä¸€è¨˜æ†¶)'} | RAGï¼š{'å•Ÿç”¨' if st.session_state.use_rag else 'åœç”¨'} | ç°¡æ˜“æ¢ç´¢å™¨ï¼š{'å•Ÿç”¨' if st.session_state.use_simple_explorer else 'åœç”¨'}")

        # --- â–¼â–¼â–¼ å…¨æ–°ã€ä¿®æ­£å¾Œçš„æ§åˆ¶æµç¨‹ â–¼â–¼â–¼ ---
        # æ¨¡å¼ä¸€ï¼šéšæ®µå¼å·¥ä½œæµ
        if st.session_state.use_multi_stage_workflow:
            # Step 1: è™•ç†ç”¨æˆ¶è¼¸å…¥èˆ‡å•Ÿå‹•
            st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤æµç¨‹å°‡ä¾åºï¼ˆCFO->COO->CEOï¼‰é€²è¡Œåˆ†æï¼Œæ¯ä¸€æ­¥å®Œæˆå¾Œæœƒç«‹åˆ»é¡¯ç¤ºçµæœï¼Œä¸¦è‡ªå‹•è§¸ç™¼ä¸‹ä¸€æ­¥ã€‚")
            st.session_state.executive_user_query = st.text_area("è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("executive_user_query", ""), height=100, key="original_workflow_query")
            can_start = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("executive_user_query"))
            if st.button("ğŸš€ å•Ÿå‹•éšæ®µå¼åˆ†æ", disabled=not can_start or st.session_state.executive_workflow_stage != "idle", key="exec_flow_button"):
                st.session_state.chat_histories[executive_session_id] = []
                st.session_state.executive_workflow_stage = "cfo_analysis_pending"
                st.session_state.cfo_analysis_text, st.session_state.coo_analysis_text, st.session_state.ceo_summary_text, st.session_state.executive_rag_context = "", "", "", ""
                st.rerun()

            # Step 2: æ ¹æ“šç•¶å‰ç‹€æ…‹ï¼ŒåŸ·è¡Œå°æ‡‰çš„åˆ†æ (æ ¸å¿ƒä¿®æ­£)
            # é€™å€‹å€å¡Šçš„ç¨‹å¼ç¢¼åªåšè¨ˆç®—å’Œç‹€æ…‹è½‰æ›ï¼Œä¸åšé¡¯ç¤º
            stage = st.session_state.executive_workflow_stage
            if stage == "cfo_analysis_pending":
                with st.spinner("CFO æ­£åœ¨åˆ†æä¸­..."):
                    df = pd.read_csv(st.session_state.uploaded_file_path)
                    st.session_state.executive_data_profile_str = generate_data_profile(df)
                    query = st.session_state.executive_user_query
                    rag_context_for_prompt = ""
                    if st.session_state.use_rag and st.session_state.retriever_chain:
                        rag_context = "\n---\n".join([doc.page_content for doc in st.session_state.retriever_chain.invoke(query)])
                        st.session_state.executive_rag_context = rag_context
                        rag_context_for_prompt = f"\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{rag_context}"
                    cfo_prompt = f"ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹åŸºæ–¼ä½ çš„å°ˆæ¥­çŸ¥è­˜ï¼Œä¸¦åš´æ ¼åƒè€ƒä»¥ä¸‹æä¾›çš„è³‡æ–™ï¼Œç‚ºå•†æ¥­å•é¡Œæä¾›è²¡å‹™è§’åº¦çš„ç°¡æ½”åˆ†æã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{query}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{st.session_state.executive_data_profile_str}{rag_context_for_prompt}"
                    st.session_state.cfo_analysis_text = get_gemini_executive_analysis(gemini_api_key, "CFO", cfo_prompt)
                    st.session_state.executive_workflow_stage = "coo_analysis_pending" # è¨­å®šä¸‹ä¸€éšæ®µ
            
            elif stage == "coo_analysis_pending":
                with st.spinner("COO æ­£åœ¨åˆ†æä¸­..."):
                    rag_context_for_prompt = f"\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}" if st.session_state.executive_rag_context else ""
                    coo_prompt = f"ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹åŸºæ–¼å•†æ¥­å•é¡Œã€çµ±è¨ˆæ‘˜è¦ã€CFO çš„è²¡å‹™åˆ†æä»¥åŠç›¸é—œæ•¸æ“šï¼Œæä¾›ç‡Ÿé‹å±¤é¢çš„ç­–ç•¥èˆ‡æ½›åœ¨é¢¨éšªã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[çµ±è¨ˆæ‘˜è¦]:\n{st.session_state.executive_data_profile_str}{rag_context_for_prompt}"
                    st.session_state.coo_analysis_text = get_gemini_executive_analysis(gemini_api_key, "COO", coo_prompt)
                    st.session_state.executive_workflow_stage = "ceo_summary_pending" # è¨­å®šä¸‹ä¸€éšæ®µ
            
            elif stage == "ceo_summary_pending":
                with st.spinner("CEO æ­£åœ¨ç¸½çµä¸­..."):
                    rag_context_for_prompt = f"\n\n[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:\n{st.session_state.executive_rag_context}" if st.session_state.executive_rag_context else ""
                    ceo_prompt = f"ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆæ‰€æœ‰è³‡è¨Šï¼Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æ±ºç­–ç¸½çµã€‚\n\n[å•†æ¥­å•é¡Œ]:\n{st.session_state.executive_user_query}\n\n[CFO çš„è²¡å‹™åˆ†æ]:\n{st.session_state.cfo_analysis_text}\n\n[COO çš„ç‡Ÿé‹åˆ†æ]:\n{st.session_state.coo_analysis_text}{rag_context_for_prompt}"
                    st.session_state.ceo_summary_text = get_gemini_executive_analysis(gemini_api_key, "CEO", ceo_prompt)
                    st.session_state.executive_workflow_stage = "completed" # æ¨™è¨˜å®Œæˆ
                    full_report = f"### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ\n{st.session_state.cfo_analysis_text}\n\n### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ\n{st.session_state.coo_analysis_text}\n\n### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–\n{st.session_state.ceo_summary_text}"
                    st.session_state.chat_histories[executive_session_id].append({"role": "ai", "content": full_report})

        # æ¨¡å¼äºŒï¼šæ•´åˆå¼å·¥ä½œæµ
        else:
            st.info("**æ–¹æ³•èªªæ˜**ï¼šæ­¤ç‚ºé è¨­æµç¨‹ã€‚æ¨¡æ“¬ä¸€å€‹å…¨èƒ½çš„ AI å°ˆæ¥­ç¶“ç†äººåœ˜éšŠï¼Œåªç™¼é€**ä¸€æ¬¡**è«‹æ±‚ï¼ŒAI åœ¨ä¸€æ¬¡ç”Ÿæˆä¸­å®Œæˆæ‰€æœ‰è§’è‰²æ€è€ƒã€‚")
            st.session_state.sp_user_query = st.text_area("è«‹è¼¸å…¥å•†æ¥­å•é¡Œä»¥å•Ÿå‹•åˆ†æ:", value=st.session_state.get("sp_user_query", ""), height=100, key="sp_workflow_query")
            can_start_sp = bool(st.session_state.get("uploaded_file_path") and st.session_state.get("sp_user_query"))
            if st.button("ğŸš€ å•Ÿå‹•æ•´åˆåˆ†æ", disabled=not can_start_sp, key="sp_flow_button"):
                st.session_state.chat_histories[executive_session_id] = []
                st.session_state.sp_workflow_stage = "running"
                with st.spinner("AI å°ˆæ¥­ç¶“ç†äººåœ˜éšŠæ­£åœ¨é€²è¡Œå…¨é¢åˆ†æ..."):
                    df = pd.read_csv(st.session_state.uploaded_file_path)
                    data_profile = generate_data_profile(df)
                    st.session_state.executive_data_profile_str = data_profile
                    query = st.session_state.sp_user_query
                    rag_context_str = ""
                    if st.session_state.use_rag and st.session_state.retriever_chain:
                        rag_context = "\n---\n".join([doc.page_content for doc in st.session_state.retriever_chain.invoke(query)])
                        st.session_state.executive_rag_context = rag_context
                        rag_context_str = f"\n\n**[RAG æª¢ç´¢å‡ºçš„ç›¸é—œæ•¸æ“š]:**\n{rag_context}"
                    
                    mega_prompt = f"""ä½ æ˜¯ä¸€å€‹é ‚å°–çš„ AI å•†æ¥­åˆ†æåœ˜éšŠï¼Œèƒ½å¤ åœ¨ä¸€æ¬¡æ€è€ƒä¸­æ‰®æ¼”å¤šå€‹å°ˆæ¥­ç¶“ç†äººè§’è‰²ã€‚ä½ çš„ä»»å‹™æ˜¯é‡å°çµ¦å®šçš„å•†æ¥­å•é¡Œå’Œæ•¸æ“šï¼Œç”Ÿæˆä¸€ä»½åŒ…å«ä¸‰å€‹éƒ¨åˆ†çš„å®Œæ•´åˆ†æå ±å‘Šã€‚

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹çµæ§‹å’Œè¦æ±‚é€²è¡Œè¼¸å‡ºï¼Œä½¿ç”¨ Markdown æ¨™é¡Œä¾†å€åˆ†æ¯å€‹éƒ¨åˆ†ï¼š
---
### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè«‹å®Œå…¨ä»¥è²¡å‹™é•·çš„è§’åº¦æ€è€ƒã€‚å°ˆæ³¨æ–¼è²¡å‹™æŒ‡æ¨™ã€æˆæœ¬æ•ˆç›Šã€æŠ•è³‡å›å ±ç‡ã€æ¯›åˆ©ç‡ã€æ½›åœ¨çš„è²¡å‹™é¢¨éšªç­‰ã€‚ä½ çš„åˆ†æå¿…é ˆå®Œå…¨åŸºæ–¼æä¾›çš„æ•¸æ“šã€‚

### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ
åœ¨æ­¤éƒ¨åˆ†ï¼Œè½‰æ›ç‚ºç‡Ÿé‹é•·çš„è§’è‰²ã€‚ä½ éœ€è¦æ€è€ƒï¼Œåœ¨CFOæœƒæå‡ºçš„è²¡å‹™è€ƒé‡ä¸‹ï¼Œç‡Ÿé‹ä¸Šæ˜¯å¦å¯è¡Œï¼Ÿåˆ†ææ½›åœ¨çš„æµç¨‹ã€ä¾›æ‡‰éˆã€äººåŠ›è³‡æºæˆ–åŸ·è¡Œé¢¨éšªã€‚ä½ çš„åˆ†æéœ€è¦å‹™å¯¦ä¸”è‘—é‡æ–¼å¯åŸ·è¡Œæ€§ã€‚

### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–
åœ¨æ­¤éƒ¨åˆ†ï¼Œä½œç‚ºCEOï¼Œè«‹ç¶œåˆä¸Šè¿°çš„è²¡å‹™(CFO)å’Œç‡Ÿé‹(COO)åˆ†æã€‚ä¸è¦é‡è¤‡ç´°ç¯€ï¼Œè€Œæ˜¯æä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æˆ°ç•¥ç¸½çµã€‚æœ€çµ‚ï¼Œçµ¦å‡ºä¸€å€‹æ˜ç¢ºã€æœæ–·çš„**æ±ºç­–**ï¼ˆä¾‹å¦‚ï¼šæ‰¹å‡†ã€é§å›ã€éœ€è¦æ›´å¤šè³‡æ–™ï¼‰ï¼Œä¸¦åˆ—å‡º 2-3 å€‹æœ€é‡è¦çš„**å¾ŒçºŒè¡Œå‹•å»ºè­°**ã€‚
---
ç¾åœ¨ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šé–‹å§‹åˆ†æï¼š

**[å•†æ¥­å•é¡Œ]:**
{query}

**[è³‡æ–™çµ±è¨ˆæ‘˜è¦]:**
{data_profile}{rag_context_str}
"""
                    response = get_gemini_executive_analysis(gemini_api_key, "IntegratedTeam", mega_prompt)
                    st.session_state.sp_final_report = response
                    st.session_state.sp_workflow_stage = "completed"
                    st.session_state.chat_histories[executive_session_id].append({"role": "ai", "content": response})
                    st.rerun()

        # Step 3: çµ±ä¸€çš„é¡¯ç¤ºé‚è¼¯
        workflow_has_started = (st.session_state.executive_workflow_stage != "idle" or st.session_state.sp_workflow_stage != 'idle')
        if workflow_has_started:
            if st.session_state.get('executive_data_profile_str'):
                with st.expander("æŸ¥çœ‹çµ±è¨ˆæ‘˜è¦èˆ‡è³‡æ–™æ¢ç´¢" if st.session_state.use_simple_explorer else "æŸ¥çœ‹çµ±è¨ˆæ‘˜è¦", expanded=False):
                    st.subheader("ç´”æ–‡å­—çµ±è¨ˆæ‘˜è¦")
                    st.text(st.session_state.executive_data_profile_str)
                    if st.session_state.use_simple_explorer and st.session_state.get("uploaded_file_path"):
                        st.divider(); display_simple_data_explorer(pd.read_csv(st.session_state.uploaded_file_path))
            if st.session_state.use_rag and st.session_state.get('executive_rag_context'):
                with st.expander("æŸ¥çœ‹ RAG æª¢ç´¢å‡ºçš„ç›¸é—œè³‡æ–™"):
                    st.markdown(st.session_state.executive_rag_context)

            st.divider()
            st.subheader("åˆ†æå ±å‘Šèˆ‡å¾ŒçºŒå°è©±")
            user_query = st.session_state.get("executive_user_query") or st.session_state.get("sp_user_query")
            if user_query:
                with st.chat_message("human"): st.markdown(user_query)

            if st.session_state.cfo_analysis_text:
                with st.chat_message("ai"): st.markdown(f"### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ\n{st.session_state.cfo_analysis_text}")
            if st.session_state.coo_analysis_text:
                with st.chat_message("ai"): st.markdown(f"### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ\n{st.session_state.coo_analysis_text}")
            if st.session_state.ceo_summary_text:
                with st.chat_message("ai"): st.markdown(f"### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–\n{st.session_state.ceo_summary_text}")
            elif st.session_state.sp_final_report:
                with st.chat_message("ai"): st.markdown(st.session_state.sp_final_report)
            
            # Step 4: è‡ªå‹•åˆ·æ–°æ©Ÿåˆ¶
            if st.session_state.executive_workflow_stage in ["coo_analysis_pending", "ceo_summary_pending"]:
                time.sleep(1) 
                st.rerun()

            # Step 5: å¾ŒçºŒè¿½å•çš„å°è©±é‚è¼¯ (æµç¨‹å®Œæˆå¾Œæ‰å•Ÿç”¨)
            workflow_completed = (st.session_state.executive_workflow_stage == "completed" or st.session_state.sp_workflow_stage == 'completed')
            if workflow_completed:
                history = st.session_state.chat_histories[executive_session_id]
                if len(history) > 1:
                    for msg in history[1:]:
                        with st.chat_message(msg["role"]): st.markdown(msg["content"])

                if st.session_state.follow_up_stage != "idle":
                    with st.chat_message("ai"):
                        if st.session_state.follow_up_cfo_analysis: st.markdown(f"#### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ\n{st.session_state.follow_up_cfo_analysis}")
                        if st.session_state.follow_up_coo_analysis: st.markdown(f"#### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ\n{st.session_state.follow_up_coo_analysis}")
                        if st.session_state.follow_up_ceo_analysis: st.markdown(f"#### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–\n{st.session_state.follow_up_ceo_analysis}")
                        if st.session_state.follow_up_stage in ["cfo_pending", "coo_pending", "ceo_pending"]: st.spinner("å°ˆæ¥­åœ˜éšŠæ­£åœ¨åˆ†ææ‚¨çš„è¿½å•...")
                
                if st.session_state.follow_up_stage == "idle":
                    if user_input := st.chat_input("é‡å°ä»¥ä¸Šå ±å‘Šé€²è¡Œæå•..."):
                        st.session_state.chat_histories[executive_session_id].append({"role": "human", "content": user_input})
                        st.session_state.follow_up_query = user_input
                        st.session_state.follow_up_stage = "cfo_pending" 
                        st.session_state.follow_up_cfo_analysis = ""
                        st.session_state.follow_up_coo_analysis = ""
                        st.session_state.follow_up_ceo_analysis = ""
                        st.rerun()

        # è™•ç†è¿½å•çš„å¾Œç«¯é‚è¼¯
        if st.session_state.follow_up_stage != "idle":
            history_context = "\n\n".join([f"**{msg['role']}:**\n{msg['content']}" for msg in st.session_state.chat_histories[executive_session_id]])
            
            if st.session_state.follow_up_stage == "cfo_pending":
                cfo_prompt = f"""ä½œç‚ºè²¡å‹™é•·(CFO)ï¼Œè«‹é‡å°ä½¿ç”¨è€…æå‡ºçš„æœ€æ–°å•é¡Œï¼Œä¸¦æ ¹æ“šå®Œæ•´çš„å°è©±æ­·å²ï¼Œæä¾›è²¡å‹™è§’åº¦çš„å°ˆæ¥­åˆ†æã€‚\n\n[å®Œæ•´çš„å°è©±æ­·å²]:\n{history_context}\n\n[ä½¿ç”¨è€…æœ€æ–°æå‡ºçš„å•é¡Œ]:\n{st.session_state.follow_up_query}\n\nè«‹åƒ…æä¾›ä½ ä½œç‚º CFO çš„åˆ†æå…§å®¹ï¼ŒåŠ›æ±‚ç°¡æ½”ç²¾ç¢ºã€‚"""
                st.session_state.follow_up_cfo_analysis = get_gemini_executive_analysis(gemini_api_key, "CFO-FollowUp", cfo_prompt)
                st.session_state.follow_up_stage = "coo_pending"
                st.rerun()

            elif st.session_state.follow_up_stage == "coo_pending":
                coo_prompt = f"""ä½œç‚ºç‡Ÿé‹é•·(COO)ï¼Œè«‹é‡å°ä½¿ç”¨è€…æå‡ºçš„æœ€æ–°å•é¡Œï¼Œä¸¦æ ¹æ“šå®Œæ•´çš„å°è©±æ­·å²ï¼Œä»¥åŠå‰›å‰›CFOæä¾›çš„æœ€æ–°è²¡å‹™åˆ†æï¼Œæä¾›ç‡Ÿé‹è§’åº¦çš„ç­–ç•¥èˆ‡é¢¨éšªè©•ä¼°ã€‚\n\n[å®Œæ•´çš„å°è©±æ­·å²]:\n{history_context}\n\n[CFO å°æ­¤å•é¡Œçš„æœ€æ–°åˆ†æ]:\n{st.session_state.follow_up_cfo_analysis}\n\n[ä½¿ç”¨è€…æœ€æ–°æå‡ºçš„å•é¡Œ]:\n{st.session_state.follow_up_query}\n\nè«‹åƒ…æä¾›ä½ ä½œç‚º COO çš„åˆ†æå…§å®¹ï¼Œé‡é»åœ¨æ–¼å¯è¡Œæ€§èˆ‡åŸ·è¡Œå±¤é¢ã€‚"""
                st.session_state.follow_up_coo_analysis = get_gemini_executive_analysis(gemini_api_key, "COO-FollowUp", coo_prompt)
                st.session_state.follow_up_stage = "ceo_pending"
                st.rerun()

            elif st.session_state.follow_up_stage == "ceo_pending":
                ceo_prompt = f"""ä½œç‚ºåŸ·è¡Œé•·(CEO)ï¼Œè«‹æ•´åˆæ‰€æœ‰è³‡è¨Šï¼ŒåŒ…æ‹¬å®Œæ•´çš„å°è©±æ­·å²ã€CFOå’ŒCOOå°æœ€æ–°å•é¡Œçš„åˆ†æï¼Œç‚ºä½¿ç”¨è€…çš„å•é¡Œæä¾›ä¸€å€‹é«˜å±¤æ¬¡çš„æˆ°ç•¥ç¸½çµèˆ‡æœ€çµ‚æ±ºç­–ã€‚\n\n[å®Œæ•´çš„å°è©±æ­·å²]:\n{history_context}\n\n[CFO å°æ­¤å•é¡Œçš„æœ€æ–°åˆ†æ]:\n{st.session_state.follow_up_cfo_analysis}\n\n[COO å°æ­¤å•é¡Œçš„æœ€æ–°åˆ†æ]:\n{st.session_state.follow_up_coo_analysis}\n\n[ä½¿ç”¨è€…æœ€æ–°æå‡ºçš„å•é¡Œ]:\n{st.session_state.follow_up_query}\n\nè«‹æä¾›ä¸€å€‹ç°¡æ½”ã€é«˜å±¤æ¬¡çš„ç¸½çµï¼Œä¸¦çµ¦å‡ºæ˜ç¢ºçš„å¾ŒçºŒè¡Œå‹•å»ºè­°ã€‚"""
                st.session_state.follow_up_ceo_analysis = get_gemini_executive_analysis(gemini_api_key, "CEO-FollowUp", ceo_prompt)
                full_follow_up_response = f"""### ğŸ“Š è²¡å‹™é•· (CFO) åˆ†æ\n{st.session_state.follow_up_cfo_analysis}\n\n### ğŸ­ ç‡Ÿé‹é•· (COO) åˆ†æ\n{st.session_state.follow_up_coo_analysis}\n\n### ğŸ‘‘ åŸ·è¡Œé•· (CEO) æœ€çµ‚æ±ºç­–\n{st.session_state.follow_up_ceo_analysis}"""
                st.session_state.chat_histories[executive_session_id].append({"role": "ai", "content": full_follow_up_response})
                st.session_state.follow_up_stage = "idle" 
                st.rerun()

if __name__ == "__main__":
    main()
