
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from langchain_openai import ChatOpenAI
from langchain.agents.agent_types import AgentType
from langchain_experimental.agents import create_pandas_dataframe_agent
import matplotlib.colors as mcolors
from openai import OpenAI
import os
import streamlit as st
from PIL import Image
from io import BytesIO
import base64
from datetime import datetime
import json
import seaborn as sns 
import tempfile
import io
from pandas.api.types import is_numeric_dtype
from matplotlib.font_manager import FontProperties
# Set the font to a font that supports Chinese characters
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK']
plt.rcParams['axes.unicode_minus'] = False  # ç¢ºä¿è² è™Ÿé¡¯ç¤ºæ­£å¸¸

file_ids = {}
def execute_code_and_generate_image(code: str) -> tuple[bool, str, BytesIO]:
    """
    åŸ·è¡ŒPythonç¨‹å¼ç¢¼ä¸¦ç”Ÿæˆåœ–è¡¨åœ–ç‰‡
    è¿”å›: (æ˜¯å¦æˆåŠŸ, éŒ¯èª¤è¨Šæ¯, åœ–ç‰‡ç·©è¡å€)
    """
    try:
        # å‰µå»ºä¸€å€‹æ–°çš„åœ–è¡¨
        plt.figure(figsize=(10, 6))
        
        # åŸ·è¡Œä»£ç¢¼
        exec(code, globals())
        
        # ç²å–ç•¶å‰åœ–è¡¨
        buffer = BytesIO()
        plt.savefig(buffer, format='png', bbox_inches='tight')
        plt.close()
        
        # é‡ç½®ç·©è¡å€ä½ç½®
        buffer.seek(0)
        
        return True, None, buffer
    except Exception as e:
        plt.close()  # ç¢ºä¿é—œé–‰ä»»ä½•æ‰“é–‹çš„åœ–è¡¨
        return False, str(e), None
def get_output_path(filename, folder="visualizations"):
    """
    è¿”å›æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼ŒåŒæ—¶ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨ã€‚
    
    :param filename: str, æ–‡ä»¶å
    :param folder: str, æ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹
    :return: str, æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    """
    os.makedirs(folder, exist_ok=True)  # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
    return os.path.join(folder, filename)  # è¿”å›å®Œæ•´è·¯å¾„

def clean_csv(file_path, output_path=None):
    """
    è‡ªå‹•æ¸…ç† CSV æ–‡ä»¶æ•¸æ“šä¸¦è™•ç†å¸¸è¦‹å•é¡Œã€‚
    :param file_path: str, åŸå§‹ CSV æ–‡ä»¶çš„è·¯å¾‘ã€‚
    :param output_path: str, æ¸…ç†å¾Œæ•¸æ“šä¿å­˜çš„è·¯å¾‘ï¼ˆå¯é¸ï¼‰ã€‚
    :return: pandas.DataFrame, æ¸…ç†å¾Œçš„æ•¸æ“šã€‚
    """
    
    try:
        # è®€å– CSV æ–‡ä»¶
        df = pd.read_csv(file_path)  # åŠ è¼‰åŸå§‹ CSV æ–‡ä»¶åˆ° DataFrame
        print(f"åŸå§‹æ•¸æ“š:\n{df.head()}\n")
        
        # 1. åˆªé™¤å…¨ç©ºçš„åˆ—èˆ‡è¡Œ
        df.dropna(how="all", inplace=True)  # åˆªé™¤å…¨ä¸ºç©ºå€¼çš„è¡Œ
        df.dropna(axis=1, how="all", inplace=True)  # åˆªé™¤å…¨ä¸ºç©ºå€¼çš„åˆ—
        
        # 2. å¡«è£œç©ºå€¼ï¼ˆå°æ•¸å€¼å‹å’Œå­—ä¸²å‹åˆ†åˆ«è™•ç†ï¼‰
        for col in df.columns:
            if is_numeric_dtype(df[col]):
                df[col] = df[col].fillna(df[col].mean())
            else:
                df[col] = df[col].fillna("Unknown")       # 3. è‡ªå‹•æª¢æŸ¥ä¸¦è™•ç†ç•°å¸¸å€¼ï¼ˆä»¥æ•¸å­—åˆ—ç‚ºä¸»ï¼‰
        for col in df.select_dtypes(include=[np.number]).columns:  # é¸æ“‡æ‰€æœ‰æ•¸å€¼åˆ—
            mean, std = df[col].mean(), df[col].std()  # è¨ˆç®—å‡å€¼å’Œæ¨™æº–å·®
            lower_bound, upper_bound = mean - 3 * std, mean + 3 * std  # å®šç¾©åˆç†ç¯„åœ
            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]  # éæ¿¾æ‰è¶…å‡ºç¯„åœçš„ç•°å¸¸å€¼
        
        # 4. å»é™¤é‡è¤‡æ•¸æ“š
        df.drop_duplicates(inplace=True)  # åˆªé™¤é‡è¤‡è¡Œ
        
        # 5. æ¸…ç†åˆ—åèˆ‡å­—ä¸²ç©ºæ ¼
        df.columns = df.columns.str.strip()  # å»æ‰åˆ—åçš„å¤šé¤˜ç©ºæ ¼
        for col in df.select_dtypes(include=["object"]).columns:  # å°å­—ç¬¦ä¸²åˆ—å»é™¤ç©ºæ ¼
            df[col] = df[col].str.strip()
        
        print(f"æ¸…ç†å¾Œæ•¸æ“š:\n{df.head()}\n")
        
        # å¦‚æœæŒ‡å®šäº†è¼¸å‡ºè·¯å¾‘ï¼Œä¿å­˜æ¸…ç†å¾Œçš„æ•¸æ“š
        if output_path:
            df.to_csv(output_path, index=False)  # å°‡æ¸…ç†å¾Œçš„æ•¸æ“šä¿å­˜åˆ°æ–‡ä»¶
            print(f"æ¸…ç†å¾Œçš„æ•¸æ“šå·²ä¿å­˜åˆ° {output_path}")
        
        return df  # è¿”å›æ¸…ç†å¾Œçš„ DataFrame
    
    except Exception as e:
        print(f"æ¸…ç†éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")  # æ‰“å°éŒ¯èª¤ä¿¡æ¯
        return None

def convert_csv_to_jsonl(csv_file_path, jsonl_file_path):
    """
    å°‡ CSV æ–‡ä»¶è½‰æ›ç‚º JSONL æ–‡ä»¶
    :param csv_file_path: str, CSV æ–‡ä»¶çš„è·¯å¾‘
    :param jsonl_file_path: str, JSONL æ–‡ä»¶çš„ä¿å­˜è·¯å¾‘
    """
    try:
        # è®€å– CSV æ–‡ä»¶
        df = pd.read_csv(csv_file_path, encoding='utf-8')
        # å°‡æ¯è¡Œè½‰æ›ç‚º JSON æ ¼å¼ï¼Œä¸¦å¯«å…¥ JSONL æ–‡ä»¶
        with open(jsonl_file_path, 'w', encoding='utf-8') as file:
            for record in df.to_dict(orient='records'):
                file.write(json.dumps(record, ensure_ascii=False) + '\n')
        print(f"JSONL æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {jsonl_file_path}")
    except Exception as e:
        print(f"è½‰æ›éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
# å°‡åœ–åƒè½‰æ›ç‚ºBase64ç·¨ç¢¼çš„å‡½æ•¸
def get_image_base64(image_raw):
    """å°‡PIL Imageç‰©ä»¶è½‰æ›ç‚ºbase64ç·¨ç¢¼
    Args:
        image_raw: PIL Imageç‰©ä»¶
    Returns:
        str: base64ç·¨ç¢¼çš„åœ–ç‰‡å­—ä¸²
    """
    buffered = BytesIO()
    image_raw.save(buffered, format=image_raw.format or 'PNG')
    img_byte = buffered.getvalue()
    return base64.b64encode(img_byte).decode('utf-8')

def save_and_encode_chart(fig, chart_number):
    """å„²å­˜åœ–è¡¨ä¸¦è½‰æ›ç‚ºbase64ç·¨ç¢¼
    Args:
        fig: matplotlib figureç‰©ä»¶
        chart_number: åœ–è¡¨ç·¨è™Ÿ
    Returns:
        tuple: (base64ç·¨ç¢¼çš„åœ–ç‰‡å­—ä¸², åœ–ç‰‡è·¯å¾‘)
    """
    # å„²å­˜åœ–è¡¨
    chart_path = f"chart_{chart_number}.png"
    fig.savefig(chart_path)
    
    # è®€å–åœ–ç‰‡ä¸¦è½‰æ›ç‚ºbase64
    with Image.open(chart_path) as img:
        base64_str = get_image_base64(img)
    
    return base64_str, chart_path

def add_chart_to_messages(chart_path):
    """
    å°‡åœ–è¡¨åŠ å…¥åˆ°å°è©±è¨Šæ¯ä¸­
    Args:
        chart_path: åœ–ç‰‡è·¯å¾‘
    """
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # åŠ å…¥åœ–è¡¨åˆ°è¨Šæ¯
    chart_message = f"""ç”Ÿæˆçš„åœ–è¡¨ï¼š
                        åœ–è¡¨ #{len(st.session_state.generated_charts) + 1}
                        ç”Ÿæˆæ™‚é–“ï¼š{pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")}
                        """
    st.session_state.messages.append({
        "role": "assistant",
        "content": chart_message
    })
    
    # å„²å­˜åœ–è¡¨è³‡è¨Š
    if "generated_charts" not in st.session_state:
        st.session_state.generated_charts = []
    
    st.session_state.generated_charts.append({
        "number": len(st.session_state.generated_charts) + 1,
        "path": chart_path,
        "timestamp": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
    })

# ç”Ÿæˆ EDA æŠ¥å‘Š
def generate_eda_report(data):
    columns = data.columns.tolist()
    shape = data.shape
    missing_values = data.isnull().sum()
    numeric_cols = data.select_dtypes(include=['float64']).columns.tolist()
    integer_cols = data.select_dtypes(include=['int64']).columns.tolist()

    report = (
        f"Your dataset contains the following information:\n\n"
        f"Column names: A total of {len(columns)} columns, including {', '.join(columns)}.\n"
        f"Data dimensions: {shape[0]} rows and {shape[1]} columns.\n"
        f"Missing values:\n" +
        "\n".join([f"Column '{col}' has {missing_values[col]} missing values." for col in columns if missing_values[col] > 0]) +
        ("\nNo missing values.\n" if missing_values.sum() == 0 else "") +
        "\nData types:\n"
        f"Numeric columns: {', '.join(numeric_cols)}\n"
        f"Integer columns: {', '.join(integer_cols)} (possibly categorical variables)\n\n"
        "Next steps:\n\n"
        "1. Handle missing values using appropriate methods.\n"
        "2. Plot a correlation matrix to understand feature relationships.\n"
        "3. Perform basic feature engineering.\n"
    )
    return report

# ä¿å­˜ EDA å ±å‘Šç‚º JSONL æ–‡ä»¶
def save_eda_report_as_jsonl(report, output_path):
    with open(output_path, 'w', encoding='utf-8') as jsonl_file:
        jsonl_file.write(json.dumps({"text": report}, ensure_ascii=False) + '\n')

# ç¼ºå¤±å€¼å¤„ç†
def handle_missing_values(data):
    for col in data.columns:
        if data[col].dtype in ['float64', 'int64']:
            data[col] = data[col].fillna(data[col].median())
        elif data[col].dtype == 'object':
            data[col] = data[col].fillna(data[col].mode()[0])
    return data
# ç»˜åˆ¶å¹¶ä¿å­˜ç›¸å…³æ€§çŸ©é˜µ
def plot_correlation_matrix(data, output_path):
    plt.figure(figsize=(12, 8))
    correlation_matrix = data.corr()
    print(f"ç›¸é—œæ€§çŸ©é™£:\n{correlation_matrix}\n")  # Print ç›¸é—œæ€§çŸ©é™£
    sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
    plt.title("Correlation Matrix of Features")
    st.pyplot(plt)
    plt.savefig(output_path)
    print(f"ç›¸é—œæ€§çŸ©é™£åœ–è¡¨å·²ä¿å­˜åˆ° {output_path}")  # Print åœ–è¡¨ä¿å­˜è·¯å¾‘
    plt.close()


# åœ–åƒè½‰æ›ç‚º Base64
def convert_image_to_base64(image_path):
    with open(image_path, "rb") as img_file:
        encoded = base64.b64encode(img_file.read()).decode('utf-8')
    return encoded

# ä¸Šå‚³ JSONL æ–‡ä»¶åˆ° OpenAI
def upload_jsonl_to_openai(client, jsonl_path):
    with open(jsonl_path, "rb") as file:
        response = client.files.create(file=file, purpose="fine-tune")
    return response.id

# ä¸Šå‚³ Base64 åœ–åƒåˆ° OpenAI
def upload_base64_image_to_openai(client, base64_image, file_name):
    temp_dir = tempfile.gettempdir()
    temp_file_path = os.path.join(temp_dir, file_name)
    with open(temp_file_path, "wb") as temp_file:
        temp_file.write(base64.b64decode(base64_image))

    with open(temp_file_path, "rb") as file:
        response = client.files.create(file=file, purpose="user_data")
        file_id = response.id

    os.remove(temp_file_path)
    return file_id

# ä¿å­˜ç‰¹å¾µå·¥ç¨‹è™•ç†å¾Œçš„æ•¸æ“š
def save_processed_data(data, output_path):
    data.to_csv(output_path, index=False)

# ä¸Šå‚³è™•ç†å¾Œçš„æ•¸æ“š
def upload_processed_data(client, processed_data_path):
    with open(processed_data_path, "rb") as file:
        response = client.files.create(file=file, purpose="user_data")
    return response.id

# è¯·æ±‚ OpenAI ç”Ÿæˆç‰¹å¾å·¥ç¨‹ä»£ç 
def Feature_Engineering_report(client, eda_report, file_ids):

    prompt = f"""
    You are tasked with creating a detailed data analysis report based on the following uploaded files:

    1. EDA Report (JSONL file): File ID {file_ids['eda_jsonl']}
    2. Correlation Matrix: File ID {file_ids['correlation_matrix']}
    3. Processed Data (CSV): File ID {file_ids['processed_data']}

    The report should include:
    - Key insights from the EDA report.
    - Interpretation of the correlation matrix.
    - Detailed description of feature engineering steps taken.
    - Recommendations for further analysis or model building.

    EDA Report:
    {eda_report}
    """

    try:
        # ä½¿ç”¨ chat.completions.create ç”Ÿæˆå ±å‘Š
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert data analyst."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1500
        )

        # æå–ç”Ÿæˆçš„å ±å‘Š
        report = response.choices[0].message.content
        print(f"ç”Ÿæˆçš„å ±å‘Š:\n{report}\n")
        return report

    except Exception as e:
        print(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        raise RuntimeError(f"Error during report generation: {e}")

# æå–ä»£ç å—
def extract_code_blocks(content):
    clean_code_lines = []
    in_code_block = False
    for line in content.splitlines():
        if "```" in line:
            in_code_block = not in_code_block
            continue
        if in_code_block:
            clean_code_lines.append(line)
    return "\n".join(clean_code_lines).strip()

def validate_font_code(code):
    if "plt.rcParams['font.sans-serif']" not in code:
        return False, "ç”Ÿæˆçš„ä»£ç¢¼ç¼ºå°‘å­—å‹è¨­ç½®ã€‚"
    return True, "ä»£ç¢¼é©—è­‰æˆåŠŸã€‚"
def generate_final_report(client, eda_report, file_ids):
    """
    Generates a comprehensive final report by utilizing the OpenAI Chat API with pre-uploaded file IDs.

    Args:
        client (OpenAI): The initialized OpenAI client.
        eda_report (str): The exploratory data analysis report.
        file_ids (dict): A dictionary containing file IDs for different parts of the report.

    Returns:
        str: The generated final report summarizing the EDA, correlations, and feature engineering.
    """
    required_keys = ["eda_jsonl", "correlation_matrix", "feature_engineering_report", "processed_data"]
    missing_keys = [key for key in required_keys if key not in file_ids]
    if missing_keys:
        raise ValueError(f"Missing required file IDs: {', '.join(missing_keys)}")

    # æ„å»ºæœ€ç»ˆæŠ¥å‘Šçš„æç¤º
    prompt = f"""
    You are tasked with creating a detailed data analysis report based on the following uploaded files:

    1. EDA Report (JSONL file): File ID {file_ids['eda_jsonl']}
    2. Correlation Matrix: File ID {file_ids['correlation_matrix']}
    3. Feature Engineering Report: File ID {file_ids['feature_engineering_report']}
    4. Processed Data (CSV): File ID {file_ids['processed_data']}

    The report should include:
    - Key insights from the EDA report.
    - Interpretation of the correlation matrices (both original and processed).
    - Detailed description of feature engineering steps taken.
    - Recommendations for further analysis or model building.

    EDA Report:
    {eda_report}
    """

    # è°ƒç”¨ Chat API ç”ŸæˆæŠ¥å‘Š
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a highly skilled data analyst."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1500
        )

        # æå–ç”Ÿæˆçš„æŠ¥å‘Šå†…å®¹
        full_report = response.choices[0].message.content
        print(f"ç”Ÿæˆçš„æœ€çµ‚å ±å‘Š:\n{full_report}")
        return full_report

    except Exception as e:
        raise RuntimeError(f"Error during final report generation: {e}")

# ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
def final_report(df):
    output_dir = "visualizations"
    os.makedirs(output_dir, exist_ok=True)

    # 1. ç”Ÿæˆ EDA å ±å‘Š
    eda_report = generate_eda_report(df)
    st.write("### EDA Report")
    st.write(eda_report)

    # ä¿å­˜ä¸¦ä¸Šå‚³ EDA å ±å‘Š
    eda_jsonl_path = os.path.join(output_dir, "eda_report.jsonl")
    with open(eda_jsonl_path, "w", encoding="utf-8") as file:
        file.write(json.dumps({"text": eda_report}, ensure_ascii=False) + "\n")
    file_ids['eda_jsonl'] = upload_jsonl_to_openai(client, eda_jsonl_path)

    # 2. ç¼ºå¤±å€¼è™•ç†
    df = handle_missing_values(df)

    # 3. ç¹ªè£½ç›¸é—œæ€§çŸ©é™£
    correlation_path = os.path.join(output_dir, "correlation_matrix.png")
    plot_correlation_matrix(df, correlation_path)
    file_ids['correlation_matrix'] = upload_processed_data(client, correlation_path)

    # 4. ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“šä¸¦ä¸Šå‚³
    processed_jsonl_path = os.path.join(output_dir, "processed_data.jsonl")
    with open(processed_jsonl_path, "w", encoding="utf-8") as file:
        for record in df.to_dict(orient="records"):
            file.write(json.dumps(record, ensure_ascii=False) + "\n")
    file_ids['processed_data'] = upload_jsonl_to_openai(client, processed_jsonl_path)

    # 5. ç”Ÿæˆç‰¹å¾µå·¥ç¨‹å ±å‘Š
    feature_engineering_report = Feature_Engineering_report(client, eda_report, file_ids)

    # ä¿å­˜ä¸¦ä¸Šå‚³ç‰¹å¾µå·¥ç¨‹å ±å‘Š
    feature_report_path = os.path.join(output_dir, "feature_engineering_report.jsonl")
    with open(feature_report_path, "w", encoding="utf-8") as file:
        file.write(json.dumps({"text": feature_engineering_report}, ensure_ascii=False) + "\n")
    file_ids['feature_engineering_report'] = upload_jsonl_to_openai(client, feature_report_path)

    # 6. ç”Ÿæˆæœ€çµ‚å ±å‘Š
    return generate_final_report(client, eda_report, file_ids)


# è¨­ç½®é é¢é…ç½®
st.set_page_config(page_title="CSV æ•¸æ“šåˆ†æèˆ‡è¦–è¦ºåŒ–èŠå¤©æ©Ÿå™¨äºº", page_icon="ğŸ“Š")

# å´é‚Šæ¬„ API Key è¼¸å…¥
st.sidebar.title("OpenAI API Key")
user_api_key = st.sidebar.text_input(
    label="#### Your OpenAI API key",
    placeholder="Paste your OpenAI API key, sk-",
    type="password"
)


# ä¸»é é¢æ¨™é¡Œ
st.title("CSVæ•¸æ“šåˆ†æèŠå¤©æ©Ÿå™¨äºº")

# æª¢æŸ¥ API Key
if not user_api_key:
    st.warning("è«‹åœ¨å´é‚Šæ¬„è¼¸å…¥æ‚¨çš„ OpenAI API Key")
    st.stop()
else:
    client = OpenAI(api_key=user_api_key)
    llm = ChatOpenAI(
        api_key=user_api_key,
        model="gpt-4",
        temperature=0.3,
        streaming=True,
        MAX_TOKENS=600
    )

# åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
if "messages" not in st.session_state:
    st.session_state.messages = []
if "df" not in st.session_state:
    st.session_state.df = None
if "chart_counter" not in st.session_state:
    st.session_state.chart_counter = 0
if "generated_charts" not in st.session_state:
    st.session_state.generated_charts = []
if "analysis_done" not in st.session_state:
    st.session_state.analysis_done = False


# ä¸Šå‚³ CSV æ–‡ä»¶
uploaded_file = st.sidebar.file_uploader("é¸æ“‡ CSV æ–‡ä»¶", type="csv")
# åˆå§‹åŒ– ChatOpenAI æ¨¡å‹

if uploaded_file is not None and not st.session_state.analysis_done:

    # å‰µå»º Pandas DataFrame Agent
    
    try:
        # æ¸…ç† CSV æ–‡ä»¶
        df = clean_csv(uploaded_file)  # æ¸…ç†ä¸Šå‚³çš„ CSV æ–‡ä»¶
        agent = create_pandas_dataframe_agent(
                llm,
                df,
                verbose=True,
                agent_type=AgentType.OPENAI_FUNCTIONS,
                allow_dangerous_code=True
                )
        if df is not None and not df.empty:
            st.session_state.df = df  # å°‡æ¸…ç†å¾Œçš„æ•¸æ“šä¿å­˜åˆ° session state
            st.sidebar.write("è³‡æ–™é›†å…§å®¹ï¼š")
            st.sidebar.dataframe(df)
            
            st.write("æ•¸æ“šé è¦½:")
            st.dataframe(df.head())
            st.write("æ­£åœ¨ç”Ÿæˆåˆ†æå ±å‘Šï¼Œè«‹ç¨å€™...")
            csv_report = final_report(df)  # ç”Ÿæˆåˆ†æå ±å‘Š
            st.session_state.messages.append({
                "role": "assistant",
                "content": csv_report
            })
            st.session_state.analysis_done = True
            st.success("åˆ†æå ±å‘Šå·²å®Œæˆï¼Œç¾åœ¨å¯ä»¥é€²è¡Œå°è©±ï¼")
    
    except Exception as e:
        st.error(f"ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

if st.session_state.analysis_done:
    df = st.session_state.df
    # å»ºç«‹å°è©±è¼¸å…¥æ¡†
    user_input = st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...")

    # é¡¯ç¤ºæ­·å²è¨Šæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # è™•ç†ç”¨æˆ¶è¼¸å…¥
    if user_input:
        # å„²å­˜ç”¨æˆ¶è¨Šæ¯
        st.session_state.messages.append({
            "role": "user",
            "content": user_input
        })

        # é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
        with st.chat_message("user"):
            st.markdown(user_input)


        # å®šç¾©ç³»çµ±æç¤ºè©
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ•¸æ“šåˆ†æåŠ©æ‰‹ï¼Œå°ˆé–€å”åŠ©ç”¨æˆ¶åˆ†æCSVæ•¸æ“šã€ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨ï¼Œä¸¦é€²è¡Œæ•¸æ“šé æ¸¬ã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æ•¸æ“šåˆ†æèˆ‡è¦–è¦ºåŒ–ï¼ˆæŠ˜ç·šåœ–ã€æŸ±ç‹€åœ–ã€æ•£é»åœ–ã€ç†±åŠ›åœ–ç­‰ï¼‰
- æ™‚é–“åºåˆ—é æ¸¬èˆ‡è¶¨å‹¢åˆ†æ
- æ•¸æ“šç›¸é—œæ€§åˆ†æ
- ç•°å¸¸å€¼æª¢æ¸¬
- æ•¸æ“šåˆ†ä½ˆåˆ†æ
- åœ–è¡¨äº¤å‰æ¯”å°åˆ†æ

é‡è¦è¦å‰‡ï¼š
1. æ‰€æœ‰å›æ‡‰å¿…é ˆæä¾›å®Œæ•´çš„Pythonç¨‹å¼ç¢¼ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„importèªå¥
2. ä¸è¦å°‡ç¨‹å¼ç¢¼åˆ†æ®µè§£é‡‹ï¼Œç›´æ¥çµ¦å‡ºå¯ä»¥åŸ·è¡Œçš„å®Œæ•´ä»£ç¢¼
3. é æ¸¬åˆ†æå¿…é ˆåŒæ™‚ç”Ÿæˆé æ¸¬åœ–è¡¨ï¼Œå±•ç¤ºé æ¸¬çµæœ
4. åœ–è¡¨å¿…é ˆåŒ…å«å®Œæ•´çš„æ¨™é¡Œã€è»¸æ¨™ç±¤å’Œåœ–ä¾‹
5. ç¢ºä¿æ‰€æœ‰æ–‡å­—ä½¿ç”¨è‹±æ–‡
6. ç¨‹å¼ç¢¼ä¸­å¿…é ˆåŒ…å«é©ç•¶çš„éŒ¯èª¤è™•ç†
7. ä½¿ç”¨ 'st.session_state.df' å­˜å–æ•¸æ“šæ¡†æ¶
8. ä½¿ç”¨ st.session_state.generated_charts è¨ªå•å·²ç”Ÿæˆçš„åœ–è¡¨ä¿¡æ¯
- æ¯å€‹åœ–è¡¨åŒ…å«: {'number': ç·¨è™Ÿ, 'path': åœ–è¡¨è·¯å¾‘, 'timestamp': ç”Ÿæˆæ™‚é–“}
9. æ”¯æŒåœ–è¡¨çš„äº¤å‰æ¯”å°å’Œåˆ†æ

åœ–è¡¨è¦æ±‚ï¼š
- æ™‚é–“åºåˆ—æ•¸æ“šï¼šåŒæ™‚é¡¯ç¤ºæ­·å²æ•¸æ“šå’Œé æ¸¬æ•¸æ“šçš„æŠ˜ç·šåœ–
- åˆ†é¡æ•¸æ“šï¼šä½¿ç”¨æŸ±ç‹€åœ–æˆ–åœ“é¤…åœ–
- ç›¸é—œæ€§ï¼šä½¿ç”¨æ•£é»åœ–æˆ–ç†±åŠ›åœ–
- åˆ†ä½ˆåˆ†æï¼šä½¿ç”¨ç›´æ–¹åœ–æˆ–ç®±å‹åœ–
- åœ–è¡¨æ¯”å°ï¼šæ”¯æŒå¤šåœ–è¡¨ä¸¦æ’é¡¯ç¤ºå’Œæ¯”è¼ƒåˆ†æ

é æ¸¬åˆ†æè¦æ±‚ï¼š
- ä½¿ç”¨scikit-learnæˆ–statsmodelsç­‰åº«é€²è¡Œé æ¸¬
- åœ¨åœ–è¡¨ä¸­ç”¨ä¸åŒé¡è‰²å€åˆ†å¯¦éš›æ•¸æ“šå’Œé æ¸¬æ•¸æ“š
- é¡¯ç¤ºé æ¸¬çš„ç½®ä¿¡å€é–“
- åŠ å…¥é æ¸¬æº–ç¢ºåº¦æŒ‡æ¨™ï¼ˆå¦‚RMSEã€MAEç­‰ï¼‰

åœ–è¡¨äº¤å‰æ¯”å°åŠŸèƒ½ï¼š
- æ”¯æŒè®€å–å·²ä¿å­˜çš„åœ–è¡¨é€²è¡Œæ¯”è¼ƒ
- å¯ä»¥é€²è¡Œå¤šåœ–è¡¨ä¸¦æ’åˆ†æ
- æ”¯æŒä¸åŒæ™‚é–“æ®µçš„æ•¸æ“šæ¯”è¼ƒ
- å¯ä»¥æ¯”è¼ƒä¸åŒé æ¸¬æ¨¡å‹çš„çµæœ

ç¨‹å¼ç¢¼æ ¼å¼ï¼š
```python
# å¿…è¦çš„import
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from matplotlib.font_manager import FontProperties
import matplotlib.image as mpimg
# ... å…¶ä»–éœ€è¦çš„åº«
# ä½¿ç”¨matplotlibè¨­ç½®å­—å‹
font_path = "C:/Users/pear2/Desktop/pythonç·´ç¿’æª”/projet/NotoSansCJKtc-VF.ttf"
font_prop = FontProperties(fname=font_path)
plt.rcParams['axes.unicode_minus'] = False  # ç¢ºä¿è² è™Ÿé¡¯ç¤ºæ­£å¸¸

# ä½¿ç”¨session stateä¸­çš„DataFrameå’Œåœ–è¡¨ä¿¡æ¯
df = st.session_state.df

è«‹æ³¨æ„ï¼š
- ç¢ºä¿ä»£ç¢¼å¯ä»¥ç›´æ¥åŸ·è¡Œ
- åŒ…å«æ‰€æœ‰å¿…è¦çš„éŒ¯èª¤è™•ç†
- ä½¿ç”¨ç¹é«”ä¸­æ–‡è¨»è§£
- åœ–è¡¨è¦ç¾è§€ä¸”ä¿¡æ¯å®Œæ•´
- ä½¿ç”¨ st.session_state.df è¨ªå•æ•¸æ“šæ¡†æ¶
- ä½¿ç”¨ st.session_state.generated_charts è¨ªå•å·²ç”Ÿæˆçš„åœ–è¡¨
- æ”¯æŒåœ–è¡¨çš„äº¤å‰æ¯”å°å’Œåˆ†æ
"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Based on this DataFrame: {df.head().to_string()}\n\nQuestion: {user_input}"}
        ]

        # è™•ç†å®Œæ•´å›æ‡‰
        full_response = ""
        
        # å‰µå»ºåŠ©æ‰‹çš„èŠå¤©æ¶ˆæ¯å®¹å™¨
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            
            # ä½¿ç”¨ä¸²æµæ¨¡å¼ç²å–å›æ‡‰
            for chunk in client.chat.completions.create(
                model="gpt-4",
                messages=messages,
                temperature=0.3,
                stream=True,
            ):
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
                    # åªåœ¨å®Œæ•´å¥å­çµæŸæ™‚æ›´æ–°é¡¯ç¤º
                    if full_response.endswith(("ã€‚", "ï¼Ÿ", "ï¼", "\n")):
                        message_placeholder.markdown(full_response + "â–Œ")
            
            # æœ€å¾Œä¸€æ¬¡æ›´æ–°ï¼Œç¢ºä¿é¡¯ç¤ºå®Œæ•´å…§å®¹
            message_placeholder.markdown(full_response)
        
        # ä¿å­˜åŠ©æ‰‹å›æ‡‰åˆ°æ­·å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å« Python ä»£ç¢¼
        if "```python" in full_response:
            code_start = full_response.find("```python") + len("```python")
            code_end = full_response.find("```", code_start)
            code = full_response[code_start:code_end].strip()
            
            # åŸ·è¡Œä»£ç¢¼ä¸¦ç”Ÿæˆåœ–è¡¨
            success, error_msg, image_buffer = execute_code_and_generate_image(code)
            
            if success and image_buffer:
                # å¢åŠ è¨ˆæ•¸å™¨
                if 'chart_counter' not in st.session_state:
                    st.session_state.chart_counter = 0
                st.session_state.chart_counter += 1
                
                # ä¿å­˜åœ–è¡¨åˆ°æ–‡ä»¶
                filepath = save_chart_to_file(image_buffer, st.session_state.chart_counter)
                
                # å°‡åœ–è¡¨åŠ å…¥åˆ°å°è©±ä¸­
                add_chart_to_messages(filepath)
                
                # é¡¯ç¤ºåœ–è¡¨
                st.image(filepath, caption=f"åœ–è¡¨ #{st.session_state.chart_counter}", use_column_width=True)
            else:
                st.error(f"ç”Ÿæˆåœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error_msg}")
else:
    st.info("è«‹ä¸Šå‚³ä¸€å€‹ CSV æ–‡ä»¶ä»¥é–‹å§‹åˆ†æ")

# åœ¨å´é‚Šæ¬„é¡¯ç¤ºæ­·å²åœ–è¡¨
with st.sidebar:
    if st.session_state.generated_charts:
        st.subheader("æ­·å²ç”Ÿæˆçš„åœ–è¡¨")
        for chart in st.session_state.generated_charts:
            if os.path.exists(chart['path']):
                with st.expander(f"åœ–è¡¨ #{chart['number']} - {chart['timestamp']}"):
                    st.image(chart['path'], use_column_width=True)
